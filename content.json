{"pages":[{"title":"About Me","text":"目前在 ASUS OCIS 擔任研發替代役，在學習技術過程中經常吸取前輩們的經驗來提昇自己的專業知識，也因如此，希望自己也可以將研究過程中的問題與解決方法提供給更多的技術愛好者作為參考。 Skill JavaScript, ECMAScript 6/7 Vue2.0, Vuex Node.js, Express, Koa2 PHP, Laravel Docker, docker-compose, dockerfile OpenStack Kubernetes Tensorflow, Keras Ethereum Ceph Slides SDN x Cloud Native Meetup #18 基於 Ceph RGW 之上層 API Service 開發與效能分析 SDN x Cloud Native Meetup #4 Build the blockchain as a Service using Ethereum on Kubernetes 台中自由軟體愛好者社群(TFC) Blockchain &amp; Docker &amp; Kubernetes Kubernetes「容企新航向」巡迴論壇－台中場 Build the blockchain as a Service using Ethereum on Kubernetes","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"動態配置 HAProxy Configuration ","text":"在現在任何的雲端服務開發或部署時，都會面臨如何能處理大量使用者同時存取服務所產生大量請求的問題。然而大多數的處理方式免不了需要透過負載平衡服務，來將使用者請求分派至不同的實體或虛擬機器進行處理，因此負載平衡服務就扮演極為重要的角色。 本篇就以 HAProxy 為例，將使用者的請求以 Round Robin 的方式平均分派至不同機器進行處理，並結合 Data Plane API 提供一個 RESTful API 介面以提供動態修改 HAProxy 配置的功能。 軟體版本 Software Name Version HAProxy 2.2.4 HAProxy Data Plane API 2.1.0 環境 OS Host Name Private Network CentOS 7 haproxy 192.168.5.2 CentOS 7 httpd1 192.168.5.3 CentOS 7 httpd2 192.168.5.4 安裝 HAProxy更新 CentOS 上的 packags 與安裝相依套件 12$ yum update$ yum install wget systemd-devel 至 HAProxy Conmmunity Edition 下載 HAProxy 2.2.4(LTS) 並解壓縮 12$ wget http://www.haproxy.org/download/2.2/src/haproxy-2.2.4.tar.gz$ tar -zxvf haproxy-2.2.4.tar.gz 進入 haproxy-2.2.4/ 目錄，並進行編譯 12$ cd haproxy-2.2.4$ make TARGET=linux-glibc USE_SYSTEMD=1 安裝需要 make 與 gcc 兩個套件，若是使用 CentOS 作業系統可透過 yum install 指令進行安裝。 安裝 HAProxy 2.2.4 1$ make install 進入 haproxy-2.2.4/contrib/systemd/ 目錄編譯 haproxy.service 檔案，並將產生出來的 haproxy.service 移動至 /lib/systemd/system/ 目錄底下 12$ make$ mv haproxy.service /lib/systemd/system/ 建立 HAProxy 配置檔放置目錄，並寫入基本配置 1234567891011121314151617181920212223242526272829$ mkdir -p /etc/haproxy$ mkdir -p /var/lib/haproxy$ vim /etc/haproxy/haproxy.cfgglobal daemon chroot /var/lib/haproxy user haproxy group haproxy stats timeout 30sdefaults mode http log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000frontend http_front bind *:80 stats uri /haproxy?stats default_backend http_backbackend http_back balance roundrobin server server_name1 192.168.5.3:80 check server server_name2 192.168.5.4:80 check 建立 haproxy use 與 group 1$ useradd -r haproxy 啟動 HAProxy 服務 12345678910$ systemctl start haproxy$ systemctl status haproxy● haproxy.service - SYSV: HA-Proxy is a TCP/HTTP reverse proxy which is particularly suited for high availability environments. Loaded: loaded (/etc/rc.d/init.d/haproxy; bad; vendor preset: disabled) Active: active (running) since Sun 2020-10-25 09:01:42 UTC; 1s ago Docs: man:systemd-sysv-generator(8) Process: 13370 ExecStart=/etc/rc.d/init.d/haproxy start (code=exited, status=0/SUCCESS) Main PID: 13381 (haproxy) CGroup: /system.slice/haproxy.service └─13381 /usr/sbin/haproxy -D -f /etc/haproxy/haproxy.cfg -p /var/run/haproxy.pid 到這裡，基本的 HAProxy 已經安裝完成，接下來進行 HAProxy Data Plane API 安裝 安裝 HAProxy Data Plane API下載 HAProxy Data Plane API 1$ wget https://github.com/haproxytech/dataplaneapi/releases/download/v2.1.0/dataplaneapi_2.1.0_Linux_x86_64.tar.gz 解壓縮 dataplaneapi_2.1.0_Linux_x86_64.tar.gz 檔案並將 build/dataplaneapi 檔案複製至 /usr/local/bin 目錄 12$ tar -zxvf dataplaneapi_2.1.0_Linux_x86_64.tar.gz$ cp build/dataplaneapi /usr/sbin/ 在 /etc/haproxy/haproxy.cfg 中加入 userlist api 來設定呼叫 HAPorxy Data Plane API 驗證的使用者名稱與密碼。以本例 james 為帳號， mypassword 為密碼。 1234$ vim /etc/haproxy/haproxy.cfg userlist api user james insecure-password mypassword 修改完成後，重新啟動 haproxy 1$ systemctl restart haproxy 最後啟動 HAProxy Data Plane API 1$ dataplaneapi --host 0.0.0.0 --port 5555 --haproxy-bin /usr/sbin/haproxy --config-file /etc/haproxy/haproxy.cfg --reload-cmd &quot;systemctl reload haproxy&quot; --reload-delay 5 --userlist api HAProxy 也很貼心的提供一個 API 文件，只要啟動 HAProxy Data Plane API 後透過 http://127.0.0.1:5555/v2/docs 即可看到如下文件內容 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2020/10/25/HAProxy-Data-Plane-API/"},{"title":"透過 Elasticsearch 提供 Ceph RGW Metadata 搜尋","text":"使用 Object Storage 一陣子以後，或許你會發現儲存的檔案愈多要找檔案愈麻煩。因此 Ceph 在 K(Kraken) 版以後有提供一個 sync module 可以將資料同步到 Elasticsearch，方便使用者或管理人員利用 query 語法快速找到你想要搜尋檔案的 metadata。 ElasticsearchElasticsearch 是基於 Lucene 的分散式全文搜尋引擎，並在 Apache 許可證下作為開源軟體發佈。現今廣泛用於資料探勘領域，與 Logstash (數據收集與日誌解析引擎) 和 Kibana (數據可視覺化平台) 合稱 ELK。 環境 參數 數值 Operating System Ubuntu 16.04 LTS Ceph Cluster IP 192.168.1.226 RGW1 192.168.1.226:8001 RGW2 192.168.1.226:8002 Realm Name test-realm Zonegroup Name test-zonegroup Zone1 test-zone-1 Zone2 test-zone-2 Elasticsearch Version 5.6+, &lt; 6.0 Elasticsearch IP 192.168.1.226:9200 架構實作時需要建立至少兩個 Zone，一個是提供使用者利用 s3 potocal 透過 Radosgw 對 Ceph Object Storage 進行操作，另一個 Zone 是同步數據使用，並將 Metadata 儲存至 Elasticsearch 上。這邊的例子採用的是同一座 Ceph Cluster 並且以不同的 port 分配兩個 RGW(Radosgw) 並個別建立兩個 Zone 在同一個 Zonegroup 與 Realm 上。 安裝 Elasticsearch更新套件集 1$ apt-get update 安裝 java 1$ apt-get install openjdk-8-jdk -y 確認 java 環境是否正常 12345$ java -versionopenjdk version &quot;1.8.0_191&quot;OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12)OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) 下載並安裝 Elasticsearch 套件 12$ curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.16.deb$ dpkg -i elasticsearch-5.6.16.deb 啟動 Elasticsearch 服務 1$ systemctl start elasticsearch 編輯 Elasticsearch Configuration，將原先 network.host 註解移除並填入 0.0.0.0，以提供所有 IP Address 皆可進行存取。 1234$ vim /etc/elasticsearch/elasticsearch.yml…network.host 0.0.0.0… 重新啟動 Elasticsearch 服務1$ systemctl restart elasticsearch 確認環境是否正常提供服務1234567891011121314$ curl 192.168.1.226:9200{ &quot;name&quot; : &quot;QNbZkhV&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;MU9qYysiSLSDdpi-TXnWIw&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;5.6.16&quot;, &quot;build_hash&quot; : &quot;3a740d1&quot;, &quot;build_date&quot; : &quot;2019-03-13T15:33:36.565Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 注意：IP Address 記得更換為自己的 IP Address 建立 test-realm 1234567$ radosgw-admin realm create --rgw-realm=test-realm --default{ &quot;id&quot;: &quot;96cf396e-796b-47e5-9ae2-2d59fb9e43df&quot;, &quot;name&quot;: &quot;test-realm&quot;, &quot;current_period&quot;: &quot;e1376cfd-1270-4e0f-bead-56a9cfbb1f8a&quot;, &quot;epoch&quot;: 1} 建立 test-zonegroup123456789101112131415$ radosgw-admin zonegroup create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --endpoints=http://192.168.1.226:8001 --master --default{ &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, &quot;api_name&quot;: &quot;test-zonegroup&quot;, &quot;is_master&quot;: &quot;true&quot;, &quot;endpoints&quot;: [ &quot;http://192.168.1.226:8001&quot; ], &quot;hostnames&quot;: [], &quot;hostnames_s3website&quot;: [], &quot;master_zone&quot;: &quot;&quot;, &quot;zones&quot;: [], ...} 建立 test-zone-11234567891011121314$ radosgw-admin zone create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone --endpoints=http://192.168.1.226:8001 --access-key=test --secret=test --master --default{ ... &quot;placement_pools&quot;: [ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;index_pool&quot;: &quot;test-zone.rgw.buckets.index&quot;, ... } } ], ...} 建立 test 使用者123456789101112131415$ radosgw-admin user create --uid=test --display-name=&quot;test&quot; --access-key=test --secret=test --system{ &quot;user_id&quot;: &quot;test&quot;, &quot;display_name&quot;: &quot;test&quot;, &quot;email&quot;: &quot;&quot;, ... &quot;keys&quot;: [ { &quot;user&quot;: &quot;test&quot;, &quot;access_key&quot;: &quot;test&quot;, &quot;secret_key&quot;: &quot;test&quot; } ], ...} 更新當前 Realm 底下數據123456789101112131415161718$ radosgw-admin period update --commit{ &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;epoch&quot;: 1, ... &quot;period_map&quot;: { &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;zonegroups&quot;: [ { &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, ... } ], ... }, ...} 編輯 Radosgw Configuration12345678910111213141516$ vim /etc/ceph/radosgw.conf[client.radosgw.gateway]mon_host = 192.168.1.226:6789keyring = /etc/ceph/ceph.client.radosgw.keyringlog file = /var/log/ceph/client.radosgw.test-zone-1.logrgw enable usage log = truergw_frontends = civetweb port=8001rgw_zone = test-zone[client.radosgw.gateway2]mon_host = 192.168.1.226:6789keyring = /etc/ceph/ceph.client.radosgw.keyringlog file = /var/log/ceph/client.radosgw.test-zone-2.logrgw enable usage log = truergw frontends = civetweb port=8002rgw_zone = test-zone-2 注意：需在 /etc/ceph/ceph.client.radosgw.keyring 中加入 client.radosgw.gateway2 的 key 啟動 client.radosgw.gateway1$ radosgw -n client.radosgw.gateway -c /etc/ceph/radosgw.conf 建立 test-zone-21234567891011$ radosgw-admin zone create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone-2 --access-key=test --secret=test --endpoints=http://192.168.1.226:8002{ &quot;id&quot;: &quot;d0a13b78-251b-4314-b026-722dfbe79ff1&quot;, &quot;name&quot;: &quot;test-zone-2&quot;, &quot;domain_root&quot;: &quot;test-zone-2.rgw.meta:root&quot;, &quot;control_pool&quot;: &quot;test-zone-2.rgw.control&quot;, &quot;gc_pool&quot;: &quot;test-zone-2.rgw.log:gc&quot;, &quot;lc_pool&quot;: &quot;test-zone-2.rgw.log:lc&quot;, &quot;log_pool&quot;: &quot;test-zone-2.rgw.log&quot;, ...} 修改 test-zone-2 的配置，更改 tier-type 與 tier-config 並指向 Elasticsearch 的 Port。12345678910111213$ radosgw-admin zone modify --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone-2 --tier-type=elasticsearch --tier-config=endpoint=http://192.168.1.226:9200{ &quot;id&quot;: &quot;d0a13b78-251b-4314-b026-722dfbe79ff1&quot;, &quot;name&quot;: &quot;test-zone-2&quot;, &quot;domain_root&quot;: &quot;test-zone-2.rgw.meta:root&quot;, &quot;control_pool&quot;: &quot;test-zone-2.rgw.control&quot;, &quot;gc_pool&quot;: &quot;test-zone-2.rgw.log:gc&quot;, &quot;lc_pool&quot;: &quot;test-zone-2.rgw.log:lc&quot;, &quot;log_pool&quot;: &quot;test-zone-2.rgw.log&quot;, &quot;intent_log_pool&quot;: &quot;test-zone-2.rgw.log:intent&quot;, &quot;usage_log_pool&quot;: &quot;test-zone-2.rgw.log:usage&quot;, ...} 更新當前 Realm 底下數據123456789101112131415161718$ radosgw-admin period update --commit{ &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;epoch&quot;: 2, ... &quot;period_map&quot;: { &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;zonegroups&quot;: [ { &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, ... } ], ... }, ...} 啟動 client.radosgw.gateway21$ radosgw -n client.radosgw.gateway2 -c /etc/ceph/radosgw.conf 開啟瀏覽器輸入 192.168.1.226:8002 確認 Radosgw 服務正常 注意：Radosgw2 無法提供服務是正常的，因為若宣告 type 為 Elasticsearch 系統不會建立 bucket.data pool。 結果建立 Bucket 與上傳檔案，可利用 s3 api 或 s3 browser 進行測試。檢視 Pool 狀態可以發現 Radosgw2 系統沒有建立 bucket.data 1234567891011121314151617$ rados dfPOOL_NAME USED ….rgw.root 6.05KiB test-zone-2.rgw.buckets.index 0B test-zone-2.rgw.control 0B test-zone-2.rgw.log 5.07KiB test-zone-2.rgw.meta 0B test-zone.rgw.buckets.data 3B test-zone.rgw.buckets.index 0B test-zone.rgw.control 0B test-zone.rgw.log 0B test-zone.rgw.meta 663B total_objects 766total_used 4.22GiBtotal_avail 35.8GiBtotal_space 40.0GiB 可以看到 test-zone-2 因為宣告的 type 為 elasticsearch 因此不會建立 buckets.data。換句話說，使用者無法透過 RGW2 對 Ceph Object Storage 進行請求存取。 使用 Elasticsearch 查詢語法查詢 metadata12$ curl http://192.168.1.226:9200/_search?q=name:testTest1{&quot;took&quot;:10,&quot;timed_out&quot;:false,&quot;_shards&quot;:{&quot;total&quot;:26,&quot;successful&quot;:26,&quot;skipped&quot;:0,&quot;failed&quot;:0},&quot;hits&quot;:{&quot;total&quot;:1,&quot;max_score&quot;:0.6931472,&quot;hits&quot;:[{&quot;_index&quot;:&quot;rgw-test-realm-313b4db1&quot;,&quot;_type&quot;:&quot;object&quot;,&quot;_id&quot;:&quot;cf4bd9d8-eb23-4773-bd29-b5d640040f36.14828.2:testTest1:null&quot;,&quot;_score&quot;:0.6931472,&quot;_source&quot;:{&quot;bucket&quot;:&quot;bucket1&quot;,&quot;name&quot;:&quot;testTest1&quot;,&quot;instance&quot;:&quot;null&quot;,&quot;versioned_epoch&quot;:0,&quot;owner&quot;:{&quot;id&quot;:&quot;test&quot;,&quot;display_name&quot;:&quot;test&quot;},&quot;permissions&quot;:[&quot;test&quot;],&quot;meta&quot;:{&quot;size&quot;:550474,&quot;mtime&quot;:&quot;2019-03-20T09:07:47.272Z&quot;,&quot;etag&quot;:&quot;bf4fef418d8f1bb996200529e60bed00&quot;,&quot;tail_tag&quot;:&quot;cf4bd9d8-eb23-4773-bd29-b5d640040f36.14828.62968&quot;,&quot;x-amz-content-sha256&quot;:&quot;a4bb4653d3480dd8d5d58f76bca04dcc08e607bdb72b616b44feb980b3a387c6&quot;,&quot;x-amz-date&quot;:&quot;20190320T090745Z&quot;}}}]}} 注意：檔案名稱記得更換為自己所上傳的檔案名稱 參考 http://blog.umcloud.com/metasearch/ http://docs.ceph.com/docs/mimic/radosgw/elastic-sync-module/ https://ceph.com/rgw/new-luminous-rgw-metadata-search/","link":"/2019/03/25/Ceph-Object-Storage-Elasticsearch-實作快速搜尋-metadata/"},{"title":"Ceph RGW + OpenStack Barbican 配置教學(上篇)","text":"本篇要介紹如何將 Ceph RGW 與 OpenStack Barbican 進行整合，將使用者透過 Ceph Radosgw 上傳的檔案在 Server Site 進行 Server Side Encription - SSE(伺服器端加密)，可以有效提升資料安全性，讓儲存在伺服器端的資料不再是明碼。但相對也存在一定的風險，若儲存在 OpenStack Barbican 的 Secret 刪除，則可能導致資料無法正常被解碼，因此讀者在使用上需特別注意。 前置條件 在虛擬機器或實體機器上已安裝 Ceph RGW, OpenStack Keystone 與 MariaDB 看到這裡讀者或許會想問，為什麼需要 OpenStack Keystone，因為 OpenStack 每個 component 在使用都需要透過 OpenStack Keystone 進行驗證，因此若僅需要使用 Ceph Object SSE 也需要安裝 OpenStack Keystone。 安裝首先安裝 OpenStack Barbican 相關套件，本篇採用 OpenStack stein Version。 123# yum install https://repos.fedorapeople.org/repos/openstack/openstack-queens/rdo-release-queens-1.noarch.rpm# yum -y update# yum install -y openstack-barbican-api python2-barbicanclient 配置安裝完成後，進入資料庫建立 barbican 資料庫與使用者。 12345# mysql -u root -p$ CREATE DATABASE barbican;$ GRANT ALL PRIVILEGES ON barbican.* TO &apos;barbican&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;barbican&apos;;$ GRANT ALL PRIVILEGES ON barbican.* TO &apos;barbican&apos;@&apos;%&apos; IDENTIFIED BY &apos;barbican&apos;;$ exit; 建立一個 barbican 使用者。 123456789101112# openstack user create --domain default --password-prompt barbican+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | c33e4a4492944c278cd0f53791c98231 || name | barbican || options | {} || password_expires_at | None |+---------------------+----------------------------------+ 建立一個新的 project 並命名為 service。 1234567891011121314# openstack project create service+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | || domain_id | default || enabled | True || id | 31a3464dcf244931883c317fef836bbc || is_domain | False || name | service || parent_id | default || tags | [] |+-------------+----------------------------------+ 將 barbican User 指派為 service Project 的 admin Role。 1# openstack role add --project service --user barbican admin 建立 barbican Service。 1234567891011# openstack service create --name barbican --description &quot;Key Manager&quot; key-manager+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Key Manager || enabled | True || id | 018639d5972545b4b12406c23f9af2d9 || name | barbican || type | key-manager |+-------------+----------------------------------+ 建立 barbican endpoints。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# openstack endpoint create --region RegionOne key-manager public http://172.17.1.100:9311+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | e08911cc96584d9c8b8cdba8d98288e9 || interface | public || region | RegionOne || region_id | RegionOne || service_id | 018639d5972545b4b12406c23f9af2d9 || service_name | barbican || service_type | key-manager || url | http://172.17.1.100:9311 |+--------------+----------------------------------+# openstack endpoint create --region RegionOne key-manager internal http://172.17.1.100:9311+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | ab71ed47d2b241afa771551521b1341f || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 018639d5972545b4b12406c23f9af2d9 || service_name | barbican || service_type | key-manager || url | http://172.17.1.100:9311 |+--------------+----------------------------------+# openstack endpoint create --region RegionOne key-manager admin http://172.17.1.100:9311+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | bca4f788e5a6497aa314a810c637f545 || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 018639d5972545b4b12406c23f9af2d9 || service_name | barbican || service_type | key-manager || url | http://172.17.1.100:9311 |+--------------+----------------------------------+ 接著，修改 /etc/barbican/barbican.conf。 1234567891011121314151617# vim /etc/barbican/barbican.conf[DEFAULT]...sql_connection = mysql+pymysql://barbican:barbican@172.17.1.100/barbican...[keystone_authtoken]...www_authenticate_uri = http://172.17.1.100:5000auth_url = http://172.17.1.100:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = barbicanpassword = password 建立 OpenStack Barbican 相關資料表。 1# su -s /bin/sh -c &quot;barbican-manage db upgrade&quot; barbican 建立 httpd 啟動 OpenStack Barbican 配置檔。 1234567891011121314151617181920212223242526# vim /etc/httpd/conf.d/wsgi-barbican.confListen 9311&lt;VirtualHost *:9311&gt; ## Logging ErrorLog &quot;/var/log/httpd/barbican_wsgi_main_error_ssl.log&quot; LogLevel debug ServerSignature Off CustomLog &quot;/var/log/httpd/barbican_wsgi_main_access_ssl.log&quot; combined WSGIApplicationGroup %{GLOBAL} WSGIDaemonProcess barbican-api display-name=barbican-api group=barbican processes=2 threads=8 user=barbican WSGIProcessGroup barbican-api WSGIScriptAlias / &quot;/usr/lib/python2.7/site-packages/barbican/api/app.wsgi&quot; WSGIPassAuthorization On &lt;Directory /usr/lib&gt; &lt;IfVersion &gt;= 2.4&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.4&gt; Order allow,deny Allow from all &lt;/IfVersion&gt; &lt;/Directory&gt;&lt;/VirtualHost&gt; 啟動 OpenStack Barbican。 123# systemctl enable httpd.service# systemctl restart httpd.service# systemctl status httpd.service 確認 Barbican Port 已啟動並佔用。 123# netstat -ntlp | grep 9311tcp6 0 0 :::9311 :::* LISTEN 5924/httpd 建立 OpenStack Barbican 環境變數檔。 123456789# vim barbicanrcexport OS_USERNAME=barbicanexport OS_PASSWORD=passwordexport OS_PROJECT_NAME=serviceexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_AUTH_URL=http://172.17.1.100:5000/v3export OS_IDENTITY_API_VERSION=3 利用 source 指令更新 OpenStack 環境變數 1# source barbicanrc 結果建立第一把 KEM Key。 12345678910111213141516# openstack secret store --name mysecret --payload YXN1c2FzdXNhc3VzYXN1c2FzdXNhc3VzYXN1c2FzdXM= --payload-content-type application/octet-stream --payload-content-encoding base64 --secret-type symmetric+---------------+-----------------------------------------------------------------------+| Field | Value |+---------------+-----------------------------------------------------------------------+| Secret href | http://localhost:9311/v1/secrets/309092d6-6def-4e8c-aad4-637fda3722ca || Name | mysecret || Created | None || Status | None || Content types | {u&apos;default&apos;: u&apos;application/octet-stream&apos;} || Algorithm | aes || Bit length | 256 || Secret type | symmetric || Mode | cbc || Expiration | None |+---------------+-----------------------------------------------------------------------+ 確認當前已註冊的 Key。 1234567# openstack secret list+-----------------------------------------------------------------------+----------+---------------------------+--------+-------------------------------------------+-----------+------------+-------------+------+------------+| Secret href | Name | Created | Status | Content types | Algorithm | Bit length | Secret type | Mode | Expiration |+-----------------------------------------------------------------------+----------+---------------------------+--------+-------------------------------------------+-----------+------------+-------------+------+------------+| http://localhost:9311/v1/secrets/309092d6-6def-4e8c-aad4-637fda3722ca | mysecret | 2020-06-20T14:16:58+00:00 | ACTIVE | {u&apos;default&apos;: u&apos;application/octet-stream&apos;} | aes | 256 | symmetric | cbc | None |+-----------------------------------------------------------------------+----------+---------------------------+--------+-------------------------------------------+-----------+------------+-------------+------+------------+ 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2020/06/20/Ceph-RGW-OpenStack-Barbican-配置教學-上篇/"},{"title":"Ceph Object Storage Placement 介紹","text":"在使用儲存服務的時候，時常會根據使用者的權限(付的錢？XD)來決定這個使用者可以使用哪些硬體支援，常見的像是 SSD, HDD 或者 Tape 等等。 本篇就是專門介紹在 Ceph 中透過 Crush Rule 將硬體支援歸納出多種不同的 Partitions，並且配置每個 RGW 使用者可以儲存的 Placement tags 與 default Placement，以達到如上述限制使用者可儲存的硬體支援。 前置條件 需已建置一個 Ceph 叢集 Ceph 叢集需提供 RGW 服務 環境 參數 數值 Operating System CentOS 7 Host Name ceph Private Network 172.17.1.100 Ceph Version 14.2.2 配置確認當前 Crush Map 包含兩個 root1234567891011121314151617$ ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -9 3.00000 root slow-10 1.00000 host OSD04 3 hdd 1.00000 osd.3 up 1.00000 1.00000-11 1.00000 host OSD05 4 hdd 1.00000 osd.4 up 1.00000 1.00000-12 1.00000 host OSD06 5 hdd 1.00000 osd.5 up 1.00000 1.00000 -1 3.00000 root default -3 1.00000 host OSD01 0 hdd 1.00000 osd.0 up 1.00000 1.00000 -4 1.00000 host OSD02 1 hdd 1.00000 osd.1 up 1.00000 1.00000 -5 1.00000 host OSD03 2 hdd 1.00000 osd.2 up 1.00000 1.00000 建立一個新的 Crush Rule 名為 slow 並且 Type 為 replicated 1$ ceph osd crush rule create-replicated slow slow host 確認當前已存在的 Crush Rule 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ ceph osd crush rule dump[ { &quot;rule_id&quot;: 0, &quot;rule_name&quot;: &quot;replicated_rule&quot;, &quot;ruleset&quot;: 0, &quot;type&quot;: 1, &quot;min_size&quot;: 1, &quot;max_size&quot;: 10, &quot;steps&quot;: [ { &quot;op&quot;: &quot;take&quot;, &quot;item&quot;: -1, &quot;item_name&quot;: &quot;default&quot; }, { &quot;op&quot;: &quot;chooseleaf_firstn&quot;, &quot;num&quot;: 0, &quot;type&quot;: &quot;host&quot; }, { &quot;op&quot;: &quot;emit&quot; } ] }, { &quot;rule_id&quot;: 1, &quot;rule_name&quot;: &quot;slow&quot;, &quot;ruleset&quot;: 1, &quot;type&quot;: 1, &quot;min_size&quot;: 1, &quot;max_size&quot;: 10, &quot;steps&quot;: [ { &quot;op&quot;: &quot;take&quot;, &quot;item&quot;: -9, &quot;item_name&quot;: &quot;slow&quot; }, { &quot;op&quot;: &quot;chooseleaf_firstn&quot;, &quot;num&quot;: 0, &quot;type&quot;: &quot;host&quot; }, { &quot;op&quot;: &quot;emit&quot; } ] }] 在 default Zonegroup 增加新的 Placement target 名為 slow 並且賦予 tag 為 slow 1234567891011121314151617181920212223242526$ radosgw-admin zonegroup placement add --rgw-zonegroup default --placement-id slow --tags slow[ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;name&quot;: &quot;default-placement&quot;, &quot;tags&quot;: [], &quot;storage_classes&quot;: [ &quot;STANDARD&quot; ] } }, { &quot;key&quot;: &quot;slow&quot;, &quot;val&quot;: { &quot;name&quot;: &quot;slow&quot;, &quot;tags&quot;: [ &quot;slow&quot; ], &quot;storage_classes&quot;: [ &quot;STANDARD&quot; ] } }] 替 default-placement Placement target 新增名為 default-placement tag 12345678910111213141516171819202122232425262728$ radosgw-admin zonegroup placement modify --rgw-zonegroup default --placement-id default-placement --tags default-placement[ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;name&quot;: &quot;default-placement&quot;, &quot;tags&quot;: [ &quot;default-placement&quot; ], &quot;storage_classes&quot;: [ &quot;STANDARD&quot; ] } }, { &quot;key&quot;: &quot;slow&quot;, &quot;val&quot;: { &quot;name&quot;: &quot;slow&quot;, &quot;tags&quot;: [ &quot;slow&quot; ], &quot;storage_classes&quot;: [ &quot;STANDARD&quot; ] } }] 在 default Zone 增加新的 placement pool，並且命名為 slow。建置過程中需配置 index-pool, data-pool 與 –data-extra-pool 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ radosgw-admin zone placement add --rgw-zone default --placement-id slow --index-pool slow.rgw.buckets.index --data-pool slow.rgw.buckets.data --data-extra-pool slow.rgw.buckets.non-ec{ &quot;id&quot;: &quot;67798670-9929-4675-a595-a27b10576b5e&quot;, &quot;name&quot;: &quot;default&quot;, &quot;domain_root&quot;: &quot;default.rgw.meta:root&quot;, &quot;control_pool&quot;: &quot;default.rgw.control&quot;, &quot;gc_pool&quot;: &quot;default.rgw.log:gc&quot;, &quot;lc_pool&quot;: &quot;default.rgw.log:lc&quot;, &quot;log_pool&quot;: &quot;default.rgw.log&quot;, &quot;intent_log_pool&quot;: &quot;default.rgw.log:intent&quot;, &quot;usage_log_pool&quot;: &quot;default.rgw.log:usage&quot;, &quot;reshard_pool&quot;: &quot;default.rgw.log:reshard&quot;, &quot;user_keys_pool&quot;: &quot;default.rgw.meta:users.keys&quot;, &quot;user_email_pool&quot;: &quot;default.rgw.meta:users.email&quot;, &quot;user_swift_pool&quot;: &quot;default.rgw.meta:users.swift&quot;, &quot;user_uid_pool&quot;: &quot;default.rgw.meta:users.uid&quot;, &quot;otp_pool&quot;: &quot;default.rgw.otp&quot;, &quot;system_key&quot;: { &quot;access_key&quot;: &quot;&quot;, &quot;secret_key&quot;: &quot;&quot; }, &quot;placement_pools&quot;: [ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;index_pool&quot;: &quot;default.rgw.buckets.index&quot;, &quot;storage_classes&quot;: { &quot;STANDARD&quot;: { &quot;data_pool&quot;: &quot;default.rgw.buckets.data&quot; } }, &quot;data_extra_pool&quot;: &quot;default.rgw.buckets.non-ec&quot;, &quot;index_type&quot;: 0 } }, { &quot;key&quot;: &quot;slow&quot;, &quot;val&quot;: { &quot;index_pool&quot;: &quot;slow.rgw.buckets.index&quot;, &quot;storage_classes&quot;: { &quot;STANDARD&quot;: { &quot;data_pool&quot;: &quot;slow.rgw.buckets.data&quot; } }, &quot;data_extra_pool&quot;: &quot;slow.rgw.buckets.non-ec&quot;, &quot;index_type&quot;: 0 } } ], &quot;metadata_heap&quot;: &quot;&quot;, &quot;realm_id&quot;: &quot;&quot;} 配置完成後，建立相對應的 pools 1234567891011$ ceph osd pool create slow.rgw.buckets.index 8 8pool &apos;slow.rgw.buckets.index&apos; created$ ceph osd pool create slow.rgw.buckets.data 8 8pool &apos;slow.rgw.buckets.data&apos; created$ ceph osd pool create slow.rgw.buckets.non-ec 8 8pool &apos;slow.rgw.buckets.non-ec&apos; created 更改新建立 pools 的 Crush Rule 1234567891011$ ceph osd pool set slow.rgw.buckets.index crush_rule slowset pool 6 crush_rule to slow$ ceph osd pool set slow.rgw.buckets.data crush_rule slowset pool 7 crush_rule to slow$ ceph osd pool set slow.rgw.buckets.non-ec crush_rule slowset pool 8 crush_rule to slow 確認新建立 pool 的 Crush Rule 1234567$ ceph osd pool ls detail...pool 5 &apos;default.rgw.buckets.index&apos; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 31 flags hashpspool stripe_width 0 application rgwpool 6 &apos;slow.rgw.buckets.index&apos; replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 68 flags hashpspool stripe_width 0pool 7 &apos;slow.rgw.buckets.data&apos; replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 70 flags hashpspool stripe_width 0pool 8 &apos;slow.rgw.buckets.non-ec&apos; replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 72 flags hashpspool stripe_width 0 crush_rule 為 1 表示已更改為 slow Crush Rule，若讀者實際操作可能值會與範例不同 重新啟動 RGW 1$ systemctl restart ceph-radosgw@radosgw.gateway 建立測試使用者 12345678910111213141516171819202122232425262728293031323334353637383940$ radosgw-admin user create --uid only-slow --display-name &quot;only-slow&quot;{ &quot;user_id&quot;: &quot;only-slow&quot;, &quot;display_name&quot;: &quot;only-slow&quot;, &quot;email&quot;: &quot;&quot;, &quot;suspended&quot;: 0, &quot;max_buckets&quot;: 1000, &quot;subusers&quot;: [], &quot;keys&quot;: [ { &quot;user&quot;: &quot;only-slow&quot;, &quot;access_key&quot;: &quot;91W6AWWF1J9OIPBVQZG7&quot;, &quot;secret_key&quot;: &quot;GsQzXqK9nfXmAvuiXF1hg2VIfrvffppeqaCaoyMJ&quot; } ], &quot;swift_keys&quot;: [], &quot;caps&quot;: [], &quot;op_mask&quot;: &quot;read, write, delete&quot;, &quot;default_placement&quot;: &quot;&quot;, &quot;default_storage_class&quot;: &quot;&quot;, &quot;placement_tags&quot;: [], &quot;bucket_quota&quot;: { &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 }, &quot;user_quota&quot;: { &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 }, &quot;temp_url_keys&quot;: [], &quot;type&quot;: &quot;rgw&quot;, &quot;mfa_ids&quot;: []} 將使用者的 metadata 匯出並且更改 default_placement 與 placement_tags 再匯入 1234567891011$ radosgw-admin metadata get user:only-slow &gt; only-slow.json$ vim only-slow.json...&quot;op_mask&quot;: &quot;read, write, delete&quot;,&quot;default_placement&quot;: &quot;slow&quot;,&quot;default_storage_class&quot;: &quot;&quot;,&quot;placement_tags&quot;: [&quot;slow&quot;],...$ radosgw-admin metadata put user:only-slow &lt; only-slow.json 結果利用 boto3 套件進行測試，測試程式碼如下： 1234567891011121314151617import boto3bucket = &quot;test2&quot;#location = &quot;default:default-placement&quot;location = &quot;default:slow&quot;s3 = boto3.client( &apos;s3&apos;, endpoint_url=&quot;http://172.17.1.100:7480&quot;, aws_access_key_id=&quot;91W6AWWF1J9OIPBVQZG7&quot;, aws_secret_access_key=&quot;GsQzXqK9nfXmAvuiXF1hg2VIfrvffppeqaCaoyMJ&quot;,)s3.create_bucket( Bucket=bucket, CreateBucketConfiguration={&apos;LocationConstraint&apos;:location},) location 為 &lt;region&gt;:&lt;placement-id&gt;，使用 default:slow 可成功建立，而 default:defualt-placement 會得到 AccessDenied 結果。 參考","link":"/2019/08/24/Ceph-Object-Storage-Placement-介紹/"},{"title":"Ceph RGW + OpenStack Keystone 配置教學(上篇)","text":"Ceph Object Storage 除了可以自身管理使用者訊息與驗證外，也可以透過 OpenStack Keystone 進行管理。本篇會先介紹如何在 CentOS 7 上安裝 OpenStack Keystone (Version Stein)，後續會在下篇時介紹如何與 Ceph 進行串接！ 前置條件 需具備一台 CentOS 7 實體機器或虛擬機器 安裝機器需具備 MariaDB 安裝在 /etc/selinux/config 將 SELINUX 註解移除並把值更改為 disabled 來關閉 SELinux，更改完儲存並重新開機 12345678$ setenforce 0$ vim /etc/selinux/config...SELINUX=disabled...$ reboot 添加 OpenStack Keystine(Stein) rpm 並安裝 123$ yum install http://repos.fedorapeople.org/repos/openstack/openstack-stein/rdo-release-stein-1.noarch.rpm$ yum update$ yum install openstack-keystone httpd mod_wsgi python-openstackclient gcc 在 MariaDB 建立 Keystone 資料庫並且建立 keystone 使用者賦予 任意 及 localhost 存取 keystone 資料表權限 12345$ mysql -u root -p$ CREATE DATABASE keystone;$ GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&lt;KEYSTONE_USER_PASSWORD&gt;&apos;;$ GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;%&apos; IDENTIFIED BY &apos;&lt;KEYSTONE_USER_PASSWORD&gt;&apos;;$ exit 修改 OpenStack Keystone 配置檔，在 [database] 加入資料庫配置 12345$ vim /etc/keystone/keystone.conf...[database]connection = mysql+pymysql://keystone:&lt;KEYSTONE_USER_PASSWORD&gt;@&lt;DATABASE_IP&gt;:&lt;DATABASE_PORT&gt;/keystone... 在 [token] 配置以 fernet 提供 Token 讓其他元件可以進行驗證，更改完後儲存並離開 1234...[token]provider = fernet... 根據 OpenStack Keystone 需具備的資料表來同步資料庫 1$ su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone 利用 fernet 針對 Token 進行初始化與建立驗證所需的 Token 12$ keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone$ keystone-manage credential_setup --keystone-user keystone --keystone-group keystone 建立 OpenStack Keystone 的 admin 使用者與建立預設的 domain, project 與 endpoint 等 12345$ keystone-manage bootstrap --bootstrap-password &lt;ADMIN_PASSWORD&gt; \\ --bootstrap-admin-url http://&lt;KEYSTONE_IP&gt;:5000/v3/ \\ --bootstrap-internal-url http://&lt;KEYSTONE_IP&gt;:5000/v3/ \\ --bootstrap-public-url http://&lt;KEYSTONE_IP&gt;:5000/v3/ \\ --bootstrap-region-id RegionOne 啟動 OpenStack Keystone 服務 123$ systemctl enable httpd.service$ systemctl start httpd.service$ systemctl status httpd.service 建立 adminrc 檔案來儲存 admin 使用者的環境變數，輸入完成後儲存並且離開 123456789$ vim adminrcexport OS_USERNAME=adminexport OS_PASSWORD=12345678export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_AUTH_URL=http://172.17.1.110:5000/v3export OS_IDENTITY_API_VERSION=3 在使用任何 OpenStack 指令時記得先將環境變數添加到系統中 1$ source adminrc 結果利用 OpenStack 指令任意呼叫服務，確認是否收到 OpenStack 回應。以下是利用查詢當前 OpenStack 已存在的服務為例，若是剛建立的環境，可以看到系統回應 Keystone 服務的資訊 123456$ openstack service list+----------------------------------+----------+----------+| ID | Name | Type |+----------------------------------+----------+----------+| 826222db3f2649d9b14be428f4873e8c | keystone | identity |+----------------------------------+----------+----------+ 常見問題OpenStack 套件沒有提供 version 指令查詢當前安裝的 Keystone 版本。因此我們需要透過 yum 當前已安裝的套件來確認當前 Keystone 安裝的版本 1234567891011121314151617$ yum list | grep keystoneopenstack-keystone.noarch 1:15.0.0-1.el7 @openstack-steinpython2-keystone.noarch 1:15.0.0-1.el7 @openstack-steinpython2-keystoneauth1.noarch 3.13.1-1.el7 @openstack-steinpython2-keystoneclient.noarch 1:3.19.0-1.el7 @openstack-steinpython2-keystonemiddleware.noarch 6.0.0-1.el7 @openstack-steinopenstack-barbican-keystone-listener.noarchopenstack-keystone-doc.noarch 1:15.0.0-1.el7 openstack-steinpuppet-keystone.noarch 14.4.0-1.el7 openstack-steinpython-keystoneauth1-doc.noarch 3.13.1-1.el7 openstack-steinpython-keystoneclient-doc.noarch 1:3.19.0-1.el7 openstack-steinpython-keystonemiddleware-doc.noarch 6.0.0-1.el7 openstack-steinpython2-keystone-tests.noarch 1:15.0.0-1.el7 openstack-steinpython2-keystone-tests-tempest.noarch 0.2.0-1.el7 openstack-steinpython2-keystone-tests-tempest-doc.noarchpython2-keystoneclient-tests.noarch 1:3.19.0-1.el7 openstack-stein 若遇到系統回應 Missing value auth-url required for auth plugin password 訊息，表示當前驗證發生問題，即可能是為將使用者的環境變數加至系統中。若以 admin 使用者為例，可以利用安裝最後所建立的 adminrc 來解決此問題 1$ source adminrc 在建置過程中，可以針對不同的使用者建立不同的 rc 檔來儲存使用者的參數，以方便做使用者切換 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2019/09/22/Ceph-RGW-OpenStack-Keystone-配置教學-上篇/"},{"title":"Ceph RGW + OpenStack Keystone 配置教學(下篇)","text":"事隔多日來繼續介紹後續 Ceph RGW 與 OpenStack Keystone 的整合配置，還沒有讀上篇的讀者可以先閱讀上篇，若有興趣也可逐步跟著這系列的文章進行操作，或許你會對於內容更有感覺唷！ 話不多說就開始吧！！ 前置條件 在虛擬機器或實體機器上已安裝 Ceph RGW 與 OpenStack Keystone 環境變數 OS_USERNAME=admin OS_PASSWORD=password OS_PROJECT_NAME=admin OS_USER_DOMAIN_NAME=Default OS_PROJECT_DOMAIN_NAME=Default OS_AUTH_URL=http://172.17.1.100:5000/v3 OS_IDENTITY_API_VERSION=3 注意，注意，注意，很重要所以講三次！！本篇介紹採用環境變數與上篇有些許不同。讀者需特別注意！ 配置首先修改 /etc/ceph/ceph.conf 檔案，並加入 OpenStack keystone 相關配置，如：domain, project, user …等。 整體配置如下： 1234567891011121314151617181920$ vim /etc/ceph/ceph.conf...[client.radosgw.gateway]host = 172.17.1.110:6789keyring = /etc/ceph/ceph.radosgw.keyringlog file = /var/log/ceph/client.radosgw.gateway.logrgw enable usage log = true# For keystone of openstackrgw keystone api version = 3# Enable Keystone Auth for S3 APIsrgw keystone url = http://172.17.1.100:5000rgw s3 auth use keystone = truergw keystone admin user = adminrgw keystone admin password = passwordrgw keystone admin domain = Defaultrgw keystone admin project = adminrgw keystone accepted roles = adminrgw keystone verify ssl = false OpenStack Keystone 參數部分讀者需依照自己建置 OpenStack Keystone 的設定自行修改。 Ceph RGW 配置完成後，需重新啟動 1$ systemctl restart ceph-radosgw@radosgw.gateway 配置完成後，就可以開始在建立 OpenStack Keystone 內建立 Ceph RGW 的使用者了，透過 openstack ec2 credentials 指令進行建立。 123456789101112$ openstack ec2 credentials create --project admin --user admin+------------+--------------------------------------------------------------------------------------------------------------------------------------+| Field | Value |+------------+--------------------------------------------------------------------------------------------------------------------------------------+| access | 25f332ce156248e6b1370a30276db861 || links | {u&apos;self&apos;: u&apos;http://172.17.1.100:5000/v3/users/4a6ccdd4c5d44b90951afd1c7f0051e0/credentials/OS-EC2/25f332ce156248e6b1370a30276db861&apos;} || project_id | 11545854141449f7abb6b7782f61cbce || secret | ddd08e575e7949e0ba0efeed5386d679 || trust_id | None || user_id | 4a6ccdd4c5d44b90951afd1c7f0051e0 |+------------+--------------------------------------------------------------------------------------------------------------------------------------+ 輸入完成後，系統會回應如上的資訊，其中 access 與 secret 就是 Ceph RGW 的 access key 與 secret key。 結果在 OpenStack Keystone 使用者建立完成後，需要任意呼叫一支 Ceph RGW API 或透過第三方工具，如: s3browser, cyberduck 進行登入，才會在 Ceph RGW 看到該使用者資訊。 登入前檢查 Ceph RGW 當前使用者列表。 12345$ radosgw-admin user list[ &quot;admin&quot;] 開啟 Cyberduck 並點擊視窗中左下角的”+”符號新增一個”S3(HTTP)”的連線，並且輸入您 Server URL 與 RGW Port，以及本篇介紹在 OpenStack Keystone 所建立的 ec2 credential access key 與 secret key 進行登入。 最後，我們再透過 Ceph RGW 指令檢查當前使用者列表。 123456$ radosgw-admin user list[ &quot;admin&quot;, &quot;11545854141449f7abb6b7782f61cbce&quot;] 從上述可以看到多一個名為 11545854141449f7abb6b7782f61cbce 的使用者，該名稱來自於 OpenStack Project Name。 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2020/06/19/Ceph-RGW-OpenStack-Keystone-配置教學-下篇/"},{"title":"手動部署 Ceph Nautilus(14.2.2)","text":"對於剛進入 Ceph 領域的讀者，部署 Ceph 可能會採用相對應的部署工具，例如：ceph-deploy。雖然它可以幫助我們快速建立 Ceph 叢集，但是對於 Ceph 整體部署流程與過程中每個 Components(如：MON, MDS)等，運作方式可能會有疑惑。 因此希望可以透過本篇文章一步一步帶領讀者安裝 Ceph 叢集，讓讀者可以理解每個步驟用途與叢集運作的流程。 環境本篇文章利用 VirutalBox 建立一台 Ubuntu 16.04 虛擬機做 Ceph All in One，並且在實體機器規劃一個虛擬硬碟空間作為 Ceph OSD 使用。 在 VitualBox 掛載一顆虛擬硬碟後，可以透過 fdisk 指令再作進一步的 partitions。 參數 數值 Operating System Ubuntu 16.04 LTS Host Name ceph Private Network 172.17.1.100 Ceph Version 14.2.2 安裝Ceph MON更新套件集。12$ apt update$ apt upgrade -y 增加 release.asc Key。1$ wget -q -O- &apos;https://download.ceph.com/keys/release.asc&apos; | sudo apt-key add - 增加 Ceph Repo 到 source list。1$ apt-add-repository &apos;deb https://download.ceph.com/debian-nautilus/ xenial main&apos; 再次更新套件1$ apt update 安裝 Ceph 與 Ceph RADOSGW 相關套件。1$ apt install -y ceph radosgw radosgw-agent 確認 Ceph 版本(Version)。1$ ceph -v 替 Ceph 產生 UUID12$ uuidgencb9d566e-48e5-4810-9163-5c7784c8b3c0 目錄切換至 /etc/ceph/ 並且建立 ceph.conf 配置檔，將以下配置內容增加至 ceph.conf 配置檔內。12345678910111213141516171819202122$ cd /etc/ceph/$ vim ceph.conf# Version 14.2.2[global]fsid = cb9d566e-48e5-4810-9163-5c7784c8b3c0mon initial members = monmon host = 172.17.1.100public network = 172.17.1.0/24cluster network = 172.17.1.0/24auth cluster required = cephxauth service required = cephxauth client required = cephxmon max pg per osd = 1000mon allow pool delete = trueosd crush update on start = false fsid 填入前步驟所建立的 UUID 建立一把名為 ceph.mon.keyring 的 Ceph MON secret key 並且儲存於 /tmp 底下，賦予 mon 全部存取權限。123$ ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &apos;allow *&apos;creating /tmp/ceph.mon.keyring –gen-key 表示系統自動產生 secret key 建立一把名為 ceph.client.admin.keyring 的 Ceph clent.admin key 並儲存於 /etc/ceph/ 底下，賦予全部(MON, OSD, MDS 與 MGR) 存取權限。123$ ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon &apos;allow *&apos; --cap osd &apos;allow *&apos; --cap mds &apos;allow *&apos; --cap mgr &apos;allow *&apos;creating /etc/ceph/ceph.client.admin.keyring 將 ceph.mon.keyring 內容複製至 ceph.client.admin.keyring 內。123$ ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyringimporting contents of /etc/ceph/ceph.client.admin.keyring into /tmp/ceph.mon.keyring 替 Ceph 建立 monitor map。12345$ monmaptool --create --add mon 172.17.1.100:6789 --fsid `ceph-conf --lookup fsid` /tmp/monmapmonmaptool: monmap file /tmp/monmapmonmaptool: set fsid to cb9d566e-48e5-4810-9163-5c7784c8b3c0monmaptool: writing epoch 0 to /tmp/monmap (1 monitors) 修改相關檔案擁有者與群組。1234$ chown -R ceph:ceph /etc/ceph$ chown -R ceph:ceph /var/lib/ceph/mon$ chown -R ceph:ceph /tmp/monmap$ chown -R ceph:ceph /tmp/ceph.mon.keyring 初始化 Ceph MON。1$ ceph-mon --mkfs --id mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 修改相關檔案擁有者與群組。1$ chown ceph:ceph -R /var/lib/ceph/mon 啟動 Ceph MON。123456789101112$ systemctl enable ceph-mon@mon.service$ systemctl start ceph-mon@mon.service$ systemctl status ceph-mon@mon.service● ceph-mon@mon.service - Ceph cluster monitor daemon Loaded: loaded (/lib/systemd/system/ceph-mon@.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2019-08-18 04:32:05 UTC; 33ms ago Main PID: 21154 ((ceph-mon)) CGroup: /system.slice/system-ceph\\x2dmon.slice/ceph-mon@mon.service └─21154 (ceph-mon)Aug 18 04:32:05 ceph systemd[1]: Started Ceph cluster monitor daemon. Ceph MGR建立 bootstrap keyrings。1$ ceph-create-keys --id mon 利用 client.admin key 建立名為 client.bootstrap-osd 的 ceph.keyring。1$ ceph-authtool -C ceph.keyring -n client.bootstrap-osd -a AQBp1lhdBuU7JRAAPx5h/9bsviXgNcbtGWulcA== 步驟內 AQBp1lhdBuU7JRAAPx5h/9bsviXgNcbtGWulcA== 為 client.admin key，可利用 cat ceph.client.admin.keyring 指令查詢。 將 key 增加至 auth entries 內並賦予對應存取權限。1$ ceph auth get-or-create mgr.mgr mon &apos;allow profile mgr&apos; osd &apos;allow *&apos; mds &apos;allow *&apos; 將 mgr.mgr key 匯出並儲存於 keyring 檔案內。1$ ceph auth get-or-create mgr.mgr -o keyring 在 /var/lib/ceph/mgr/ 建立 ceph-mgr 目錄，並將 keyring 移至該目錄內。12$ mkdir -p /var/lib/ceph/mgr/ceph-mgr$ mv keyring /var/lib/ceph/mgr/ceph-mgr 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/mgr/ceph-mgr 啟動 Ceph MGR。123$ systemctl enable ceph-mgr@mgr.service$ systemctl start ceph-mgr@mgr.service$ systemctl status ceph-mgr@mgr.service OSD將 client.bootstrap-osd key 匯出並儲存於 /var/lib/ceph/bootstrap-osd/ 目錄下 ceph.keyring 檔案內。1$ ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/bootstrap-osd 將 /dev/sdc1 至 /dev/sdc4 作為 Ceph OSD。1234$ ceph-volume lvm create --bluestore --data /dev/sdc1$ ceph-volume lvm create --bluestore --data /dev/sdc2$ ceph-volume lvm create --bluestore --data /dev/sdc3$ ceph-volume lvm create --bluestore --data /dev/sdc4 修改相關檔案擁有者與群組。1234$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-0$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-1$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-2$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-3 在 Ceph CRUSH Map 中規劃 OSD01 - OSD04 host。123456789101112131415$ ceph osd crush add-bucket OSD01 hostadded bucket OSD01 type host to crush map$ ceph osd crush add-bucket OSD02 hostadded bucket OSD02 type host to crush map$ ceph osd crush add-bucket OSD03 hostadded bucket OSD03 type host to crush map$ ceph osd crush add-bucket OSD04 hostadded bucket OSD04 type host to crush map 將所有的 host(OSD01 - OSD04) 移至 default root 底下。123456789101112131415$ ceph osd crush move OSD01 root=defaultmoved item id -3 name &apos;OSD01&apos; to location {root=default} in crush map$ ceph osd crush move OSD02 root=defaultmoved item id -4 name &apos;OSD02&apos; to location {root=default} in crush map$ ceph osd crush move OSD03 root=defaultmoved item id -5 name &apos;OSD03&apos; to location {root=default} in crush map$ ceph osd crush move OSD04 root=defaultmoved item id -6 name &apos;OSD04&apos; to location {root=default} in crush map 將 osd.0 - osd.4 綁定至對應的 OSD01 - OSD04 並賦予全部權重皆為 1.0。123456789101112131415$ ceph osd crush add osd.0 1.0 root=default host=OSD01add item id 0 name &apos;osd.0&apos; weight 1 at location {host=OSD01,root=default} to crush map$ ceph osd crush add osd.1 1.0 root=default host=OSD02add item id 1 name &apos;osd.1&apos; weight 1 at location {host=OSD02,root=default} to crush map$ ceph osd crush add osd.2 1.0 root=default host=OSD03add item id 2 name &apos;osd.2&apos; weight 1 at location {host=OSD03,root=default} to crush map$ ceph osd crush add osd.3 1.0 root=default host=OSD04add item id 3 name &apos;osd.3&apos; weight 1 at location {host=OSD04,root=default} to crush map MDS在 keyring 內增加 mds.mds key。12345678$ ceph-authtool --create-keyring keyring --gen-key -n mds.mds[mds.mds] key = AQDJ3Fhd4cISDRAAWKhKoqlGnmm39CaC5oTcWw==$ ceph auth add mds.mds osd &quot;allow rwx&quot; mds &quot;allow&quot; mon &quot;allow profile mds&quot; -i keyringadded key for mds.mds 在 /var/lib/ceph/mds/ 建立 ceph-mds 目錄，並將 keyring 移至該目錄內。12$ mkdir -p /var/lib/ceph/mds/ceph-mds$ mv keyring /var/lib/ceph/mds/ceph-mds 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/mds/ 啟動 Ceph MDS。123$ systemctl enable ceph-mds@mds.service$ systemctl start ceph-mds@mds.service$ systemctl status ceph-mds@mds.service 結果利用 ceph -s 指令查詢當前 Ceph 叢集狀態。123456789101112131415root@ceph:/etc/ceph# ceph -s cluster: id: cb9d566e-48e5-4810-9163-5c7784c8b3c0 health: HEALTH_OK services: mon: 1 daemons, quorum mon (age 30m) mgr: mgr(active, since 18m) osd: 4 osds: 4 up (since 12m), 4 in (since 12m) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 61 MiB used, 3.9 GiB / 4 GiB avail pgs: 參考 https://docs.ceph.com/docs/master/","link":"/2019/08/18/手動部署-Ceph-Nautilus/"}],"tags":[{"name":"load balancer","slug":"load-balancer","link":"/tags/load-balancer/"},{"name":"Ceph","slug":"Ceph","link":"/tags/Ceph/"},{"name":"Object Storage","slug":"Object-Storage","link":"/tags/Object-Storage/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Cloud Native","slug":"Cloud-Native","link":"/tags/Cloud-Native/"},{"name":"OpenStack","slug":"OpenStack","link":"/tags/OpenStack/"},{"name":"Keystone","slug":"Keystone","link":"/tags/Keystone/"},{"name":"RGW","slug":"RGW","link":"/tags/RGW/"}],"categories":[{"name":"load balancer","slug":"load-balancer","link":"/categories/load-balancer/"},{"name":"Ceph","slug":"Ceph","link":"/categories/Ceph/"},{"name":"OpenStack","slug":"OpenStack","link":"/categories/OpenStack/"},{"name":"Keystone","slug":"OpenStack/Keystone","link":"/categories/OpenStack/Keystone/"},{"name":"Ceph","slug":"OpenStack/Keystone/Ceph","link":"/categories/OpenStack/Keystone/Ceph/"},{"name":"RGW","slug":"OpenStack/Keystone/Ceph/RGW","link":"/categories/OpenStack/Keystone/Ceph/RGW/"}]}