{"pages":[{"title":"tags","text":"","link":"/tags/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"透過 Elasticsearch 提供 Ceph RGW Metadata 搜尋","text":"使用 Object Storage 一陣子以後，或許你會發現儲存的檔案愈多要找檔案愈麻煩。因此 Ceph 在 K(Kraken) 版以後有提供一個 sync module 可以將資料同步到 Elasticsearch，方便使用者或管理人員利用 query 語法快速找到你想要搜尋檔案的 metadata。 ElasticsearchElasticsearch 是基於 Lucene 的分散式全文搜尋引擎，並在 Apache 許可證下作為開源軟體發佈。現今廣泛用於資料探勘領域，與 Logstash (數據收集與日誌解析引擎) 和 Kibana (數據可視覺化平台) 合稱 ELK。 環境 參數 數值 Operating System Ubuntu 16.04 LTS Ceph Cluster IP 192.168.1.226 RGW1 192.168.1.226:8001 RGW2 192.168.1.226:8002 Realm Name test-realm Zonegroup Name test-zonegroup Zone1 test-zone-1 Zone2 test-zone-2 Elasticsearch Version 5.6+, &lt; 6.0 Elasticsearch IP 192.168.1.226:9200 筆者在 Elasticsearch 這邊卡蠻久的，安裝了 6.6.x 版本以上都無法正常提供服務，後來降到 5.6.x 服務就正常了。看了 Ceph source codes 內似乎有註明 v5，若有其他大大們知道原因也可以底下留言如何解決 6.6.x 無法正常提供服務的方法。 架構實作時需要建立至少兩個 Zone，一個是提供使用者利用 s3 potocal 透過 Radosgw 對 Ceph Object Storage 進行操作，另一個 Zone 是同步數據使用，並將 Metadata 儲存至 Elasticsearch 上。這邊的例子採用的是同一座 Ceph Cluster 並且以不同的 port 分配兩個 RGW(Radosgw) 並個別建立兩個 Zone 在同一個 Zonegroup 與 Realm 上。 安裝 Elasticsearch更新套件集1$ apt-get update 安裝 java1$ apt-get install openjdk-8-jdk -y 確認 java 環境是否正常12345$ java -versionopenjdk version &quot;1.8.0_191&quot;OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12)OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) 下載並安裝 Elasticsearch 套件12$ curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.16.deb$ dpkg -i elasticsearch-5.6.16.deb 啟動 Elasticsearch 服務1$ systemctl start elasticsearch 編輯 Elasticsearch Configuration，將原先 network.host 註解移除並填入 0.0.0.0，以提供所有 IP Address 皆可進行存取。1234$ vim /etc/elasticsearch/elasticsearch.yml…network.host 0.0.0.0… 重新啟動 Elasticsearch 服務1$ systemctl restart elasticsearch 確認環境是否正常提供服務1234567891011121314$ curl 192.168.1.226:9200{ &quot;name&quot; : &quot;QNbZkhV&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;MU9qYysiSLSDdpi-TXnWIw&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;5.6.16&quot;, &quot;build_hash&quot; : &quot;3a740d1&quot;, &quot;build_date&quot; : &quot;2019-03-13T15:33:36.565Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 注意：IP Address 記得更換為自己的 IP Address 建立 test-realm1234567$ radosgw-admin realm create --rgw-realm=test-realm --default{ &quot;id&quot;: &quot;96cf396e-796b-47e5-9ae2-2d59fb9e43df&quot;, &quot;name&quot;: &quot;test-realm&quot;, &quot;current_period&quot;: &quot;e1376cfd-1270-4e0f-bead-56a9cfbb1f8a&quot;, &quot;epoch&quot;: 1} 建立 test-zonegroup123456789101112131415$ radosgw-admin zonegroup create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --endpoints=http://192.168.1.226:8001 --master --default{ &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, &quot;api_name&quot;: &quot;test-zonegroup&quot;, &quot;is_master&quot;: &quot;true&quot;, &quot;endpoints&quot;: [ &quot;http://192.168.1.226:8001&quot; ], &quot;hostnames&quot;: [], &quot;hostnames_s3website&quot;: [], &quot;master_zone&quot;: &quot;&quot;, &quot;zones&quot;: [], ...} 建立 test-zone-11234567891011121314$ radosgw-admin zone create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone --endpoints=http://192.168.1.226:8001 --access-key=test --secret=test --master --default{ ... &quot;placement_pools&quot;: [ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;index_pool&quot;: &quot;test-zone.rgw.buckets.index&quot;, ... } } ], ...} 建立 test 使用者123456789101112131415$ radosgw-admin user create --uid=test --display-name=&quot;test&quot; --access-key=test --secret=test --system{ &quot;user_id&quot;: &quot;test&quot;, &quot;display_name&quot;: &quot;test&quot;, &quot;email&quot;: &quot;&quot;, ... &quot;keys&quot;: [ { &quot;user&quot;: &quot;test&quot;, &quot;access_key&quot;: &quot;test&quot;, &quot;secret_key&quot;: &quot;test&quot; } ], ...} 更新當前 Realm 底下數據123456789101112131415161718$ radosgw-admin period update --commit{ &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;epoch&quot;: 1, ... &quot;period_map&quot;: { &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;zonegroups&quot;: [ { &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, ... } ], ... }, ...} 編輯 Radosgw Configuration12345678910111213141516$ vim /etc/ceph/radosgw.conf[client.radosgw.gateway]mon_host = 192.168.1.226:6789keyring = /etc/ceph/ceph.client.radosgw.keyringlog file = /var/log/ceph/client.radosgw.test-zone-1.logrgw enable usage log = truergw_frontends = civetweb port=8001rgw_zone = test-zone[client.radosgw.gateway2]mon_host = 192.168.1.226:6789keyring = /etc/ceph/ceph.client.radosgw.keyringlog file = /var/log/ceph/client.radosgw.test-zone-2.logrgw enable usage log = truergw frontends = civetweb port=8002rgw_zone = test-zone-2 注意：需在 /etc/ceph/ceph.client.radosgw.keyring 中加入 client.radosgw.gateway2 的 key 啟動 client.radosgw.gateway1$ radosgw -n client.radosgw.gateway -c /etc/ceph/radosgw.conf 建立 test-zone-21234567891011$ radosgw-admin zone create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone-2 --access-key=test --secret=test --endpoints=http://192.168.1.226:8002{ &quot;id&quot;: &quot;d0a13b78-251b-4314-b026-722dfbe79ff1&quot;, &quot;name&quot;: &quot;test-zone-2&quot;, &quot;domain_root&quot;: &quot;test-zone-2.rgw.meta:root&quot;, &quot;control_pool&quot;: &quot;test-zone-2.rgw.control&quot;, &quot;gc_pool&quot;: &quot;test-zone-2.rgw.log:gc&quot;, &quot;lc_pool&quot;: &quot;test-zone-2.rgw.log:lc&quot;, &quot;log_pool&quot;: &quot;test-zone-2.rgw.log&quot;, ...} 修改 test-zone-2 的配置，更改 tier-type 與 tier-config 並指向 Elasticsearch 的 Port。12345678910111213$ radosgw-admin zone modify --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone-2 --tier-type=elasticsearch --tier-config=endpoint=http://192.168.1.226:9200{ &quot;id&quot;: &quot;d0a13b78-251b-4314-b026-722dfbe79ff1&quot;, &quot;name&quot;: &quot;test-zone-2&quot;, &quot;domain_root&quot;: &quot;test-zone-2.rgw.meta:root&quot;, &quot;control_pool&quot;: &quot;test-zone-2.rgw.control&quot;, &quot;gc_pool&quot;: &quot;test-zone-2.rgw.log:gc&quot;, &quot;lc_pool&quot;: &quot;test-zone-2.rgw.log:lc&quot;, &quot;log_pool&quot;: &quot;test-zone-2.rgw.log&quot;, &quot;intent_log_pool&quot;: &quot;test-zone-2.rgw.log:intent&quot;, &quot;usage_log_pool&quot;: &quot;test-zone-2.rgw.log:usage&quot;, ...} 更新當前 Realm 底下數據123456789101112131415161718$ radosgw-admin period update --commit{ &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;epoch&quot;: 2, ... &quot;period_map&quot;: { &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;zonegroups&quot;: [ { &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, ... } ], ... }, ...} 啟動 client.radosgw.gateway21$ radosgw -n client.radosgw.gateway2 -c /etc/ceph/radosgw.conf 開啟瀏覽器輸入 192.168.1.226:8002 確認 Radosgw 服務正常 注意：Radosgw2 無法提供服務是正常的，因為若宣告 type 為 Elasticsearch 系統不會建立 bucket.data pool。 結果建立 Bucket 與上傳檔案，可利用 s3 api 或 s3 browser 進行測試。檢視 Pool 狀態可以發現 Radosgw2 系統沒有建立 bucket.data 1234567891011121314151617$ rados dfPOOL_NAME USED ….rgw.root 6.05KiB test-zone-2.rgw.buckets.index 0B test-zone-2.rgw.control 0B test-zone-2.rgw.log 5.07KiB test-zone-2.rgw.meta 0B test-zone.rgw.buckets.data 3B test-zone.rgw.buckets.index 0B test-zone.rgw.control 0B test-zone.rgw.log 0B test-zone.rgw.meta 663B total_objects 766total_used 4.22GiBtotal_avail 35.8GiBtotal_space 40.0GiB 可以看到 test-zone-2 因為宣告的 type 為 elasticsearch 因此不會建立 buckets.data。換句話說，使用者無法透過 RGW2 對 Ceph Object Storage 進行請求存取。 使用 Elasticsearch 查詢語法查詢 metadata12$ curl http://192.168.1.226:9200/_search?q=name:testTest1{&quot;took&quot;:10,&quot;timed_out&quot;:false,&quot;_shards&quot;:{&quot;total&quot;:26,&quot;successful&quot;:26,&quot;skipped&quot;:0,&quot;failed&quot;:0},&quot;hits&quot;:{&quot;total&quot;:1,&quot;max_score&quot;:0.6931472,&quot;hits&quot;:[{&quot;_index&quot;:&quot;rgw-test-realm-313b4db1&quot;,&quot;_type&quot;:&quot;object&quot;,&quot;_id&quot;:&quot;cf4bd9d8-eb23-4773-bd29-b5d640040f36.14828.2:testTest1:null&quot;,&quot;_score&quot;:0.6931472,&quot;_source&quot;:{&quot;bucket&quot;:&quot;bucket1&quot;,&quot;name&quot;:&quot;testTest1&quot;,&quot;instance&quot;:&quot;null&quot;,&quot;versioned_epoch&quot;:0,&quot;owner&quot;:{&quot;id&quot;:&quot;test&quot;,&quot;display_name&quot;:&quot;test&quot;},&quot;permissions&quot;:[&quot;test&quot;],&quot;meta&quot;:{&quot;size&quot;:550474,&quot;mtime&quot;:&quot;2019-03-20T09:07:47.272Z&quot;,&quot;etag&quot;:&quot;bf4fef418d8f1bb996200529e60bed00&quot;,&quot;tail_tag&quot;:&quot;cf4bd9d8-eb23-4773-bd29-b5d640040f36.14828.62968&quot;,&quot;x-amz-content-sha256&quot;:&quot;a4bb4653d3480dd8d5d58f76bca04dcc08e607bdb72b616b44feb980b3a387c6&quot;,&quot;x-amz-date&quot;:&quot;20190320T090745Z&quot;}}}]}} 注意：檔案名稱記得更換為自己所上傳的檔案名稱 參考 http://blog.umcloud.com/metasearch/ http://docs.ceph.com/docs/mimic/radosgw/elastic-sync-module/ https://ceph.com/rgw/new-luminous-rgw-metadata-search/","link":"/2019/03/25/Ceph-Object-Storage-Elasticsearch-實作快速搜尋-metadata/"},{"title":"手動部署 Ceph Nautilus(14.2.2)","text":"對於剛進入 Ceph 領域的讀者，部署 Ceph 可能會採用相對應的部署工具，例如：ceph-deploy。雖然它可以幫助我們快速建立 Ceph 叢集，但是對於 Ceph 整體部署流程與過程中每個 Components(如：MON, MDS)等，運作方式可能會有疑惑。 因此希望可以透過本篇文章一步一步帶領讀者安裝 Ceph 叢集，讓讀者可以理解每個步驟用途與叢集運作的流程。 環境本篇文章利用 VirutalBox 建立一台 Ubuntu 16.04 虛擬機做 Ceph All in One，並且在實體機器規劃一個虛擬硬碟空間作為 Ceph OSD 使用。 在 VitualBox 掛載一顆虛擬硬碟後，可以透過 fdisk 指令再作進一步的 partitions。 參數 數值 Operating System Ubuntu 16.04 LTS Host Name ceph Private Network 172.17.1.100 Ceph Version 14.2.2 安裝Ceph MON更新套件集。12$ apt update$ apt upgrade -y 增加 release.asc Key。1$ wget -q -O- &apos;https://download.ceph.com/keys/release.asc&apos; | sudo apt-key add - 增加 Ceph Repo 到 source list。1$ apt-add-repository &apos;deb https://download.ceph.com/debian-nautilus/ xenial main&apos; 再次更新套件1$ apt update 安裝 Ceph 與 Ceph RADOSGW 相關套件。1$ apt install -y ceph radosgw radosgw-agent 確認 Ceph 版本(Version)。1$ ceph -v 替 Ceph 產生 UUID12$ uuidgencb9d566e-48e5-4810-9163-5c7784c8b3c0 目錄切換至 /etc/ceph/ 並且建立 ceph.conf 配置檔，將以下配置內容增加至 ceph.conf 配置檔內。12345678910111213141516171819202122$ cd /etc/ceph/$ vim ceph.conf# Version 14.2.2[global]fsid = cb9d566e-48e5-4810-9163-5c7784c8b3c0mon initial members = monmon host = 172.17.1.100public network = 172.17.1.0/24cluster network = 172.17.1.0/24auth cluster required = cephxauth service required = cephxauth client required = cephxmon max pg per osd = 1000mon allow pool delete = trueosd crush update on start = false fsid 填入前步驟所建立的 UUID 建立一把名為 ceph.mon.keyring 的 Ceph MON secret key 並且儲存於 /tmp 底下，賦予 mon 全部存取權限。123$ ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &apos;allow *&apos;creating /tmp/ceph.mon.keyring –gen-key 表示系統自動產生 secret key 建立一把名為 ceph.client.admin.keyring 的 Ceph clent.admin key 並儲存於 /etc/ceph/ 底下，賦予全部(MON, OSD, MDS 與 MGR) 存取權限。123$ ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon &apos;allow *&apos; --cap osd &apos;allow *&apos; --cap mds &apos;allow *&apos; --cap mgr &apos;allow *&apos;creating /etc/ceph/ceph.client.admin.keyring 將 ceph.mon.keyring 內容複製至 ceph.client.admin.keyring 內。123$ ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyringimporting contents of /etc/ceph/ceph.client.admin.keyring into /tmp/ceph.mon.keyring 替 Ceph 建立 monitor map。12345$ monmaptool --create --add mon 172.17.1.100:6789 --fsid `ceph-conf --lookup fsid` /tmp/monmapmonmaptool: monmap file /tmp/monmapmonmaptool: set fsid to cb9d566e-48e5-4810-9163-5c7784c8b3c0monmaptool: writing epoch 0 to /tmp/monmap (1 monitors) 修改相關檔案擁有者與群組。1234$ chown -R ceph:ceph /etc/ceph$ chown -R ceph:ceph /var/lib/ceph/mon$ chown -R ceph:ceph /tmp/monmap$ chown -R ceph:ceph /tmp/ceph.mon.keyring 初始化 Ceph MON。1$ ceph-mon --mkfs --id mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 修改相關檔案擁有者與群組。1$ chown ceph:ceph -R /var/lib/ceph/mon 啟動 Ceph MON。123456789101112$ systemctl enable ceph-mon@mon.service$ systemctl start ceph-mon@mon.service$ systemctl status ceph-mon@mon.service● ceph-mon@mon.service - Ceph cluster monitor daemon Loaded: loaded (/lib/systemd/system/ceph-mon@.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2019-08-18 04:32:05 UTC; 33ms ago Main PID: 21154 ((ceph-mon)) CGroup: /system.slice/system-ceph\\x2dmon.slice/ceph-mon@mon.service └─21154 (ceph-mon)Aug 18 04:32:05 ceph systemd[1]: Started Ceph cluster monitor daemon. Ceph MGR建立 bootstrap keyrings。1$ ceph-create-keys --id mon 利用 client.admin key 建立名為 client.bootstrap-osd 的 ceph.keyring。1$ ceph-authtool -C ceph.keyring -n client.bootstrap-osd -a AQBp1lhdBuU7JRAAPx5h/9bsviXgNcbtGWulcA== 步驟內 AQBp1lhdBuU7JRAAPx5h/9bsviXgNcbtGWulcA== 為 client.admin key，可利用 cat ceph.client.admin.keyring 指令查詢。 將 key 增加至 auth entries 內並賦予對應存取權限。1$ ceph auth get-or-create mgr.mgr mon &apos;allow profile mgr&apos; osd &apos;allow *&apos; mds &apos;allow *&apos; 將 mgr.mgr key 匯出並儲存於 keyring 檔案內。1$ ceph auth get-or-create mgr.mgr -o keyring 在 /var/lib/ceph/mgr/ 建立 ceph-mgr 目錄，並將 keyring 移至該目錄內。12$ mkdir -p /var/lib/ceph/mgr/ceph-mgr$ mv keyring /var/lib/ceph/mgr/ceph-mgr 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/mgr/ceph-mgr 啟動 Ceph MGR。123$ systemctl enable ceph-mgr@mgr.service$ systemctl start ceph-mgr@mgr.service$ systemctl status ceph-mgr@mgr.service OSD將 client.bootstrap-osd key 匯出並儲存於 /var/lib/ceph/bootstrap-osd/ 目錄下 ceph.keyring 檔案內。1$ ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/bootstrap-osd 將 /dev/sdc1 至 /dev/sdc4 作為 Ceph OSD。1234$ ceph-volume lvm create --bluestore --data /dev/sdc1$ ceph-volume lvm create --bluestore --data /dev/sdc2$ ceph-volume lvm create --bluestore --data /dev/sdc3$ ceph-volume lvm create --bluestore --data /dev/sdc4 修改相關檔案擁有者與群組。1234$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-0$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-1$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-2$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-3 在 Ceph CRUSH Map 中規劃 OSD01 - OSD04 host。123456789101112131415$ ceph osd crush add-bucket OSD01 hostadded bucket OSD01 type host to crush map$ ceph osd crush add-bucket OSD02 hostadded bucket OSD02 type host to crush map$ ceph osd crush add-bucket OSD03 hostadded bucket OSD03 type host to crush map$ ceph osd crush add-bucket OSD04 hostadded bucket OSD04 type host to crush map 將所有的 host(OSD01 - OSD04) 移至 default root 底下。123456789101112131415$ ceph osd crush move OSD01 root=defaultmoved item id -3 name &apos;OSD01&apos; to location {root=default} in crush map$ ceph osd crush move OSD02 root=defaultmoved item id -4 name &apos;OSD02&apos; to location {root=default} in crush map$ ceph osd crush move OSD03 root=defaultmoved item id -5 name &apos;OSD03&apos; to location {root=default} in crush map$ ceph osd crush move OSD04 root=defaultmoved item id -6 name &apos;OSD04&apos; to location {root=default} in crush map 將 osd.0 - osd.4 綁定至對應的 OSD01 - OSD04 並賦予全部權重皆為 1.0。123456789101112131415$ ceph osd crush add osd.0 1.0 root=default host=OSD01add item id 0 name &apos;osd.0&apos; weight 1 at location {host=OSD01,root=default} to crush map$ ceph osd crush add osd.1 1.0 root=default host=OSD02add item id 1 name &apos;osd.1&apos; weight 1 at location {host=OSD02,root=default} to crush map$ ceph osd crush add osd.2 1.0 root=default host=OSD03add item id 2 name &apos;osd.2&apos; weight 1 at location {host=OSD03,root=default} to crush map$ ceph osd crush add osd.3 1.0 root=default host=OSD04add item id 3 name &apos;osd.3&apos; weight 1 at location {host=OSD04,root=default} to crush map MDS在 keyring 內增加 mds.mds key。12345678$ ceph-authtool --create-keyring keyring --gen-key -n mds.mds[mds.mds] key = AQDJ3Fhd4cISDRAAWKhKoqlGnmm39CaC5oTcWw==$ ceph auth add mds.mds osd &quot;allow rwx&quot; mds &quot;allow&quot; mon &quot;allow profile mds&quot; -i keyringadded key for mds.mds 在 /var/lib/ceph/mds/ 建立 ceph-mds 目錄，並將 keyring 移至該目錄內。12$ mkdir -p /var/lib/ceph/mds/ceph-mds$ mv keyring /var/lib/ceph/mds/ceph-mds 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/mds/ 啟動 Ceph MDS。123$ systemctl enable ceph-mds@mds.service$ systemctl start ceph-mds@mds.service$ systemctl status ceph-mds@mds.service 結果利用 ceph -s 指令查詢當前 Ceph 叢集狀態。123456789101112131415root@ceph:/etc/ceph# ceph -s cluster: id: cb9d566e-48e5-4810-9163-5c7784c8b3c0 health: HEALTH_OK services: mon: 1 daemons, quorum mon (age 30m) mgr: mgr(active, since 18m) osd: 4 osds: 4 up (since 12m), 4 in (since 12m) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 61 MiB used, 3.9 GiB / 4 GiB avail pgs: 參考 https://docs.ceph.com/docs/master/","link":"/2019/08/18/手動部署-Ceph-Nautilus/"}],"tags":[{"name":"Ceph","slug":"Ceph","link":"/tags/Ceph/"},{"name":"Object Storage","slug":"Object-Storage","link":"/tags/Object-Storage/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Cloud Native","slug":"Cloud-Native","link":"/tags/Cloud-Native/"}],"categories":[{"name":"Ceph","slug":"Ceph","link":"/categories/Ceph/"}]}