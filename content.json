{"pages":[{"title":"About Me","text":"目前在 ASUS OCIS 擔任研發替代役，在學習技術過程中經常吸取前輩們的經驗來提昇自己的專業知識，也因如此，希望自己也可以將研究過程中的問題與解決方法提供給更多的技術愛好者作為參考。 Skill JavaScript, ECMAScript 6/7 Vue2.0, Vuex Node.js, Express, Koa2 PHP, Laravel Docker, docker-compose, dockerfile OpenStack Kubernetes Tensorflow, Keras Ethereum Ceph Slides SDN x Cloud Native Meetup #18 基於 Ceph RGW 之上層 API Service 開發與效能分析 SDN x Cloud Native Meetup #4 Build the blockchain as a Service using Ethereum on Kubernetes 台中自由軟體愛好者社群(TFC) Blockchain &amp; Docker &amp; Kubernetes Kubernetes「容企新航向」巡迴論壇－台中場 Build the blockchain as a Service using Ethereum on Kubernetes","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"透過 Elasticsearch 提供 Ceph RGW Metadata 搜尋","text":"使用 Object Storage 一陣子以後，或許你會發現儲存的檔案愈多要找檔案愈麻煩。因此 Ceph 在 K(Kraken) 版以後有提供一個 sync module 可以將資料同步到 Elasticsearch，方便使用者或管理人員利用 query 語法快速找到你想要搜尋檔案的 metadata。 ElasticsearchElasticsearch 是基於 Lucene 的分散式全文搜尋引擎，並在 Apache 許可證下作為開源軟體發佈。現今廣泛用於資料探勘領域，與 Logstash (數據收集與日誌解析引擎) 和 Kibana (數據可視覺化平台) 合稱 ELK。 環境 參數 數值 Operating System Ubuntu 16.04 LTS Ceph Cluster IP 192.168.1.226 RGW1 192.168.1.226:8001 RGW2 192.168.1.226:8002 Realm Name test-realm Zonegroup Name test-zonegroup Zone1 test-zone-1 Zone2 test-zone-2 Elasticsearch Version 5.6+, &lt; 6.0 Elasticsearch IP 192.168.1.226:9200 架構實作時需要建立至少兩個 Zone，一個是提供使用者利用 s3 potocal 透過 Radosgw 對 Ceph Object Storage 進行操作，另一個 Zone 是同步數據使用，並將 Metadata 儲存至 Elasticsearch 上。這邊的例子採用的是同一座 Ceph Cluster 並且以不同的 port 分配兩個 RGW(Radosgw) 並個別建立兩個 Zone 在同一個 Zonegroup 與 Realm 上。 安裝 Elasticsearch更新套件集 1$ apt-get update 安裝 java 1$ apt-get install openjdk-8-jdk -y 確認 java 環境是否正常 12345$ java -versionopenjdk version &quot;1.8.0_191&quot;OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12)OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) 下載並安裝 Elasticsearch 套件 12$ curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.16.deb$ dpkg -i elasticsearch-5.6.16.deb 啟動 Elasticsearch 服務 1$ systemctl start elasticsearch 編輯 Elasticsearch Configuration，將原先 network.host 註解移除並填入 0.0.0.0，以提供所有 IP Address 皆可進行存取。 1234$ vim /etc/elasticsearch/elasticsearch.yml…network.host 0.0.0.0… 重新啟動 Elasticsearch 服務1$ systemctl restart elasticsearch 確認環境是否正常提供服務1234567891011121314$ curl 192.168.1.226:9200{ &quot;name&quot; : &quot;QNbZkhV&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;MU9qYysiSLSDdpi-TXnWIw&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;5.6.16&quot;, &quot;build_hash&quot; : &quot;3a740d1&quot;, &quot;build_date&quot; : &quot;2019-03-13T15:33:36.565Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 注意：IP Address 記得更換為自己的 IP Address 建立 test-realm 1234567$ radosgw-admin realm create --rgw-realm=test-realm --default{ &quot;id&quot;: &quot;96cf396e-796b-47e5-9ae2-2d59fb9e43df&quot;, &quot;name&quot;: &quot;test-realm&quot;, &quot;current_period&quot;: &quot;e1376cfd-1270-4e0f-bead-56a9cfbb1f8a&quot;, &quot;epoch&quot;: 1} 建立 test-zonegroup123456789101112131415$ radosgw-admin zonegroup create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --endpoints=http://192.168.1.226:8001 --master --default{ &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, &quot;api_name&quot;: &quot;test-zonegroup&quot;, &quot;is_master&quot;: &quot;true&quot;, &quot;endpoints&quot;: [ &quot;http://192.168.1.226:8001&quot; ], &quot;hostnames&quot;: [], &quot;hostnames_s3website&quot;: [], &quot;master_zone&quot;: &quot;&quot;, &quot;zones&quot;: [], ...} 建立 test-zone-11234567891011121314$ radosgw-admin zone create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone --endpoints=http://192.168.1.226:8001 --access-key=test --secret=test --master --default{ ... &quot;placement_pools&quot;: [ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;index_pool&quot;: &quot;test-zone.rgw.buckets.index&quot;, ... } } ], ...} 建立 test 使用者123456789101112131415$ radosgw-admin user create --uid=test --display-name=&quot;test&quot; --access-key=test --secret=test --system{ &quot;user_id&quot;: &quot;test&quot;, &quot;display_name&quot;: &quot;test&quot;, &quot;email&quot;: &quot;&quot;, ... &quot;keys&quot;: [ { &quot;user&quot;: &quot;test&quot;, &quot;access_key&quot;: &quot;test&quot;, &quot;secret_key&quot;: &quot;test&quot; } ], ...} 更新當前 Realm 底下數據123456789101112131415161718$ radosgw-admin period update --commit{ &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;epoch&quot;: 1, ... &quot;period_map&quot;: { &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;zonegroups&quot;: [ { &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, ... } ], ... }, ...} 編輯 Radosgw Configuration12345678910111213141516$ vim /etc/ceph/radosgw.conf[client.radosgw.gateway]mon_host = 192.168.1.226:6789keyring = /etc/ceph/ceph.client.radosgw.keyringlog file = /var/log/ceph/client.radosgw.test-zone-1.logrgw enable usage log = truergw_frontends = civetweb port=8001rgw_zone = test-zone[client.radosgw.gateway2]mon_host = 192.168.1.226:6789keyring = /etc/ceph/ceph.client.radosgw.keyringlog file = /var/log/ceph/client.radosgw.test-zone-2.logrgw enable usage log = truergw frontends = civetweb port=8002rgw_zone = test-zone-2 注意：需在 /etc/ceph/ceph.client.radosgw.keyring 中加入 client.radosgw.gateway2 的 key 啟動 client.radosgw.gateway1$ radosgw -n client.radosgw.gateway -c /etc/ceph/radosgw.conf 建立 test-zone-21234567891011$ radosgw-admin zone create --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone-2 --access-key=test --secret=test --endpoints=http://192.168.1.226:8002{ &quot;id&quot;: &quot;d0a13b78-251b-4314-b026-722dfbe79ff1&quot;, &quot;name&quot;: &quot;test-zone-2&quot;, &quot;domain_root&quot;: &quot;test-zone-2.rgw.meta:root&quot;, &quot;control_pool&quot;: &quot;test-zone-2.rgw.control&quot;, &quot;gc_pool&quot;: &quot;test-zone-2.rgw.log:gc&quot;, &quot;lc_pool&quot;: &quot;test-zone-2.rgw.log:lc&quot;, &quot;log_pool&quot;: &quot;test-zone-2.rgw.log&quot;, ...} 修改 test-zone-2 的配置，更改 tier-type 與 tier-config 並指向 Elasticsearch 的 Port。12345678910111213$ radosgw-admin zone modify --rgw-realm=test-realm --rgw-zonegroup=test-zonegroup --rgw-zone=test-zone-2 --tier-type=elasticsearch --tier-config=endpoint=http://192.168.1.226:9200{ &quot;id&quot;: &quot;d0a13b78-251b-4314-b026-722dfbe79ff1&quot;, &quot;name&quot;: &quot;test-zone-2&quot;, &quot;domain_root&quot;: &quot;test-zone-2.rgw.meta:root&quot;, &quot;control_pool&quot;: &quot;test-zone-2.rgw.control&quot;, &quot;gc_pool&quot;: &quot;test-zone-2.rgw.log:gc&quot;, &quot;lc_pool&quot;: &quot;test-zone-2.rgw.log:lc&quot;, &quot;log_pool&quot;: &quot;test-zone-2.rgw.log&quot;, &quot;intent_log_pool&quot;: &quot;test-zone-2.rgw.log:intent&quot;, &quot;usage_log_pool&quot;: &quot;test-zone-2.rgw.log:usage&quot;, ...} 更新當前 Realm 底下數據123456789101112131415161718$ radosgw-admin period update --commit{ &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;epoch&quot;: 2, ... &quot;period_map&quot;: { &quot;id&quot;: &quot;0d0783ec-c5c7-4e31-a1e5-ee9a81f9b08a&quot;, &quot;zonegroups&quot;: [ { &quot;id&quot;: &quot;ff420afa-3caa-4cdb-8e2e-595f8c0dc150&quot;, &quot;name&quot;: &quot;test-zonegroup&quot;, ... } ], ... }, ...} 啟動 client.radosgw.gateway21$ radosgw -n client.radosgw.gateway2 -c /etc/ceph/radosgw.conf 開啟瀏覽器輸入 192.168.1.226:8002 確認 Radosgw 服務正常 注意：Radosgw2 無法提供服務是正常的，因為若宣告 type 為 Elasticsearch 系統不會建立 bucket.data pool。 結果建立 Bucket 與上傳檔案，可利用 s3 api 或 s3 browser 進行測試。檢視 Pool 狀態可以發現 Radosgw2 系統沒有建立 bucket.data 1234567891011121314151617$ rados dfPOOL_NAME USED ….rgw.root 6.05KiB test-zone-2.rgw.buckets.index 0B test-zone-2.rgw.control 0B test-zone-2.rgw.log 5.07KiB test-zone-2.rgw.meta 0B test-zone.rgw.buckets.data 3B test-zone.rgw.buckets.index 0B test-zone.rgw.control 0B test-zone.rgw.log 0B test-zone.rgw.meta 663B total_objects 766total_used 4.22GiBtotal_avail 35.8GiBtotal_space 40.0GiB 可以看到 test-zone-2 因為宣告的 type 為 elasticsearch 因此不會建立 buckets.data。換句話說，使用者無法透過 RGW2 對 Ceph Object Storage 進行請求存取。 使用 Elasticsearch 查詢語法查詢 metadata12$ curl http://192.168.1.226:9200/_search?q=name:testTest1{&quot;took&quot;:10,&quot;timed_out&quot;:false,&quot;_shards&quot;:{&quot;total&quot;:26,&quot;successful&quot;:26,&quot;skipped&quot;:0,&quot;failed&quot;:0},&quot;hits&quot;:{&quot;total&quot;:1,&quot;max_score&quot;:0.6931472,&quot;hits&quot;:[{&quot;_index&quot;:&quot;rgw-test-realm-313b4db1&quot;,&quot;_type&quot;:&quot;object&quot;,&quot;_id&quot;:&quot;cf4bd9d8-eb23-4773-bd29-b5d640040f36.14828.2:testTest1:null&quot;,&quot;_score&quot;:0.6931472,&quot;_source&quot;:{&quot;bucket&quot;:&quot;bucket1&quot;,&quot;name&quot;:&quot;testTest1&quot;,&quot;instance&quot;:&quot;null&quot;,&quot;versioned_epoch&quot;:0,&quot;owner&quot;:{&quot;id&quot;:&quot;test&quot;,&quot;display_name&quot;:&quot;test&quot;},&quot;permissions&quot;:[&quot;test&quot;],&quot;meta&quot;:{&quot;size&quot;:550474,&quot;mtime&quot;:&quot;2019-03-20T09:07:47.272Z&quot;,&quot;etag&quot;:&quot;bf4fef418d8f1bb996200529e60bed00&quot;,&quot;tail_tag&quot;:&quot;cf4bd9d8-eb23-4773-bd29-b5d640040f36.14828.62968&quot;,&quot;x-amz-content-sha256&quot;:&quot;a4bb4653d3480dd8d5d58f76bca04dcc08e607bdb72b616b44feb980b3a387c6&quot;,&quot;x-amz-date&quot;:&quot;20190320T090745Z&quot;}}}]}} 注意：檔案名稱記得更換為自己所上傳的檔案名稱 參考 http://blog.umcloud.com/metasearch/ http://docs.ceph.com/docs/mimic/radosgw/elastic-sync-module/ https://ceph.com/rgw/new-luminous-rgw-metadata-search/","link":"/2019/03/25/Ceph-Object-Storage-Elasticsearch-實作快速搜尋-metadata/"},{"title":"Ceph Object Storage Placement 介紹","text":"在使用儲存服務的時候，時常會根據使用者的權限(付的錢？XD)來決定這個使用者可以使用哪些硬體支援，常見的像是 SSD, HDD 或者 Tape 等等。 本篇就是專門介紹在 Ceph 中透過 Crush Rule 將硬體支援歸納出多種不同的 Partitions，並且配置每個 RGW 使用者可以儲存的 Placement tags 與 default Placement，以達到如上述限制使用者可儲存的硬體支援。 前置條件 需已建置一個 Ceph 叢集 Ceph 叢集需提供 RGW 服務 環境 參數 數值 Operating System CentOS 7 Host Name ceph Private Network 172.17.1.100 Ceph Version 14.2.2 配置確認當前 Crush Map 包含兩個 root1234567891011121314151617$ ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -9 3.00000 root slow-10 1.00000 host OSD04 3 hdd 1.00000 osd.3 up 1.00000 1.00000-11 1.00000 host OSD05 4 hdd 1.00000 osd.4 up 1.00000 1.00000-12 1.00000 host OSD06 5 hdd 1.00000 osd.5 up 1.00000 1.00000 -1 3.00000 root default -3 1.00000 host OSD01 0 hdd 1.00000 osd.0 up 1.00000 1.00000 -4 1.00000 host OSD02 1 hdd 1.00000 osd.1 up 1.00000 1.00000 -5 1.00000 host OSD03 2 hdd 1.00000 osd.2 up 1.00000 1.00000 建立一個新的 Crush Rule 名為 slow 並且 Type 為 replicated 1$ ceph osd crush rule create-replicated slow slow host 確認當前已存在的 Crush Rule 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ ceph osd crush rule dump[ { &quot;rule_id&quot;: 0, &quot;rule_name&quot;: &quot;replicated_rule&quot;, &quot;ruleset&quot;: 0, &quot;type&quot;: 1, &quot;min_size&quot;: 1, &quot;max_size&quot;: 10, &quot;steps&quot;: [ { &quot;op&quot;: &quot;take&quot;, &quot;item&quot;: -1, &quot;item_name&quot;: &quot;default&quot; }, { &quot;op&quot;: &quot;chooseleaf_firstn&quot;, &quot;num&quot;: 0, &quot;type&quot;: &quot;host&quot; }, { &quot;op&quot;: &quot;emit&quot; } ] }, { &quot;rule_id&quot;: 1, &quot;rule_name&quot;: &quot;slow&quot;, &quot;ruleset&quot;: 1, &quot;type&quot;: 1, &quot;min_size&quot;: 1, &quot;max_size&quot;: 10, &quot;steps&quot;: [ { &quot;op&quot;: &quot;take&quot;, &quot;item&quot;: -9, &quot;item_name&quot;: &quot;slow&quot; }, { &quot;op&quot;: &quot;chooseleaf_firstn&quot;, &quot;num&quot;: 0, &quot;type&quot;: &quot;host&quot; }, { &quot;op&quot;: &quot;emit&quot; } ] }] 在 default Zonegroup 增加新的 Placement target 名為 slow 並且賦予 tag 為 slow 1234567891011121314151617181920212223242526$ radosgw-admin zonegroup placement add --rgw-zonegroup default --placement-id slow --tags slow[ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;name&quot;: &quot;default-placement&quot;, &quot;tags&quot;: [], &quot;storage_classes&quot;: [ &quot;STANDARD&quot; ] } }, { &quot;key&quot;: &quot;slow&quot;, &quot;val&quot;: { &quot;name&quot;: &quot;slow&quot;, &quot;tags&quot;: [ &quot;slow&quot; ], &quot;storage_classes&quot;: [ &quot;STANDARD&quot; ] } }] 替 default-placement Placement target 新增名為 default-placement tag 12345678910111213141516171819202122232425262728$ radosgw-admin zonegroup placement modify --rgw-zonegroup default --placement-id default-placement --tags default-placement[ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;name&quot;: &quot;default-placement&quot;, &quot;tags&quot;: [ &quot;default-placement&quot; ], &quot;storage_classes&quot;: [ &quot;STANDARD&quot; ] } }, { &quot;key&quot;: &quot;slow&quot;, &quot;val&quot;: { &quot;name&quot;: &quot;slow&quot;, &quot;tags&quot;: [ &quot;slow&quot; ], &quot;storage_classes&quot;: [ &quot;STANDARD&quot; ] } }] 在 default Zone 增加新的 placement pool，並且命名為 slow。建置過程中需配置 index-pool, data-pool 與 –data-extra-pool 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ radosgw-admin zone placement add --rgw-zone default --placement-id slow --index-pool slow.rgw.buckets.index --data-pool slow.rgw.buckets.data --data-extra-pool slow.rgw.buckets.non-ec{ &quot;id&quot;: &quot;67798670-9929-4675-a595-a27b10576b5e&quot;, &quot;name&quot;: &quot;default&quot;, &quot;domain_root&quot;: &quot;default.rgw.meta:root&quot;, &quot;control_pool&quot;: &quot;default.rgw.control&quot;, &quot;gc_pool&quot;: &quot;default.rgw.log:gc&quot;, &quot;lc_pool&quot;: &quot;default.rgw.log:lc&quot;, &quot;log_pool&quot;: &quot;default.rgw.log&quot;, &quot;intent_log_pool&quot;: &quot;default.rgw.log:intent&quot;, &quot;usage_log_pool&quot;: &quot;default.rgw.log:usage&quot;, &quot;reshard_pool&quot;: &quot;default.rgw.log:reshard&quot;, &quot;user_keys_pool&quot;: &quot;default.rgw.meta:users.keys&quot;, &quot;user_email_pool&quot;: &quot;default.rgw.meta:users.email&quot;, &quot;user_swift_pool&quot;: &quot;default.rgw.meta:users.swift&quot;, &quot;user_uid_pool&quot;: &quot;default.rgw.meta:users.uid&quot;, &quot;otp_pool&quot;: &quot;default.rgw.otp&quot;, &quot;system_key&quot;: { &quot;access_key&quot;: &quot;&quot;, &quot;secret_key&quot;: &quot;&quot; }, &quot;placement_pools&quot;: [ { &quot;key&quot;: &quot;default-placement&quot;, &quot;val&quot;: { &quot;index_pool&quot;: &quot;default.rgw.buckets.index&quot;, &quot;storage_classes&quot;: { &quot;STANDARD&quot;: { &quot;data_pool&quot;: &quot;default.rgw.buckets.data&quot; } }, &quot;data_extra_pool&quot;: &quot;default.rgw.buckets.non-ec&quot;, &quot;index_type&quot;: 0 } }, { &quot;key&quot;: &quot;slow&quot;, &quot;val&quot;: { &quot;index_pool&quot;: &quot;slow.rgw.buckets.index&quot;, &quot;storage_classes&quot;: { &quot;STANDARD&quot;: { &quot;data_pool&quot;: &quot;slow.rgw.buckets.data&quot; } }, &quot;data_extra_pool&quot;: &quot;slow.rgw.buckets.non-ec&quot;, &quot;index_type&quot;: 0 } } ], &quot;metadata_heap&quot;: &quot;&quot;, &quot;realm_id&quot;: &quot;&quot;} 配置完成後，建立相對應的 pools 1234567891011$ ceph osd pool create slow.rgw.buckets.index 8 8pool &apos;slow.rgw.buckets.index&apos; created$ ceph osd pool create slow.rgw.buckets.data 8 8pool &apos;slow.rgw.buckets.data&apos; created$ ceph osd pool create slow.rgw.buckets.non-ec 8 8pool &apos;slow.rgw.buckets.non-ec&apos; created 更改新建立 pools 的 Crush Rule 1234567891011$ ceph osd pool set slow.rgw.buckets.index crush_rule slowset pool 6 crush_rule to slow$ ceph osd pool set slow.rgw.buckets.data crush_rule slowset pool 7 crush_rule to slow$ ceph osd pool set slow.rgw.buckets.non-ec crush_rule slowset pool 8 crush_rule to slow 確認新建立 pool 的 Crush Rule 1234567$ ceph osd pool ls detail...pool 5 &apos;default.rgw.buckets.index&apos; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 31 flags hashpspool stripe_width 0 application rgwpool 6 &apos;slow.rgw.buckets.index&apos; replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 68 flags hashpspool stripe_width 0pool 7 &apos;slow.rgw.buckets.data&apos; replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 70 flags hashpspool stripe_width 0pool 8 &apos;slow.rgw.buckets.non-ec&apos; replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 72 flags hashpspool stripe_width 0 crush_rule 為 1 表示已更改為 slow Crush Rule，若讀者實際操作可能值會與範例不同 重新啟動 RGW 1$ systemctl restart ceph-radosgw@radosgw.gateway 建立測試使用者 12345678910111213141516171819202122232425262728293031323334353637383940$ radosgw-admin user create --uid only-slow --display-name &quot;only-slow&quot;{ &quot;user_id&quot;: &quot;only-slow&quot;, &quot;display_name&quot;: &quot;only-slow&quot;, &quot;email&quot;: &quot;&quot;, &quot;suspended&quot;: 0, &quot;max_buckets&quot;: 1000, &quot;subusers&quot;: [], &quot;keys&quot;: [ { &quot;user&quot;: &quot;only-slow&quot;, &quot;access_key&quot;: &quot;91W6AWWF1J9OIPBVQZG7&quot;, &quot;secret_key&quot;: &quot;GsQzXqK9nfXmAvuiXF1hg2VIfrvffppeqaCaoyMJ&quot; } ], &quot;swift_keys&quot;: [], &quot;caps&quot;: [], &quot;op_mask&quot;: &quot;read, write, delete&quot;, &quot;default_placement&quot;: &quot;&quot;, &quot;default_storage_class&quot;: &quot;&quot;, &quot;placement_tags&quot;: [], &quot;bucket_quota&quot;: { &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 }, &quot;user_quota&quot;: { &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 }, &quot;temp_url_keys&quot;: [], &quot;type&quot;: &quot;rgw&quot;, &quot;mfa_ids&quot;: []} 將使用者的 metadata 匯出並且更改 default_placement 與 placement_tags 再匯入 1234567891011$ radosgw-admin metadata get user:only-slow &gt; only-slow.json$ vim only-slow.json...&quot;op_mask&quot;: &quot;read, write, delete&quot;,&quot;default_placement&quot;: &quot;slow&quot;,&quot;default_storage_class&quot;: &quot;&quot;,&quot;placement_tags&quot;: [&quot;slow&quot;],...$ radosgw-admin metadata put user:only-slow &lt; only-slow.json 結果利用 boto3 套件進行測試，測試程式碼如下： 1234567891011121314151617import boto3bucket = &quot;test2&quot;#location = &quot;default:default-placement&quot;location = &quot;default:slow&quot;s3 = boto3.client( &apos;s3&apos;, endpoint_url=&quot;http://172.17.1.100:7480&quot;, aws_access_key_id=&quot;91W6AWWF1J9OIPBVQZG7&quot;, aws_secret_access_key=&quot;GsQzXqK9nfXmAvuiXF1hg2VIfrvffppeqaCaoyMJ&quot;,)s3.create_bucket( Bucket=bucket, CreateBucketConfiguration={&apos;LocationConstraint&apos;:location},) location 為 &lt;region&gt;:&lt;placement-id&gt;，使用 default:slow 可成功建立，而 default:defualt-placement 會得到 AccessDenied 結果。 參考","link":"/2019/08/24/Ceph-Object-Storage-Placement-介紹/"},{"title":"Ceph RGW + OpenStack Barbican 配置教學(上篇)","text":"本篇要介紹如何將 Ceph RGW 與 OpenStack Barbican 進行整合，將使用者透過 Ceph Radosgw 上傳的檔案在 Server Site 進行 Server Side Encription - SSE(伺服器端加密)，可以有效提升資料安全性，讓儲存在伺服器端的資料不再是明碼。但相對也存在一定的風險，若儲存在 OpenStack Barbican 的 Secret 刪除，則可能導致資料無法正常被解碼，因此讀者在使用上需特別注意。 前置條件 在虛擬機器或實體機器上已安裝 Ceph RGW, OpenStack Keystone 與 MariaDB 看到這裡讀者或許會想問，為什麼需要 OpenStack Keystone，因為 OpenStack 每個 component 在使用都需要透過 OpenStack Keystone 進行驗證，因此若僅需要使用 Ceph Object SSE 也需要安裝 OpenStack Keystone。 安裝首先安裝 OpenStack Barbican 相關套件，本篇採用 OpenStack stein Version。 123# yum install https://repos.fedorapeople.org/repos/openstack/openstack-queens/rdo-release-queens-1.noarch.rpm# yum -y update# yum install -y openstack-barbican-api python2-barbicanclient 配置安裝完成後，進入資料庫建立 barbican 資料庫與使用者。 12345# mysql -u root -p$ CREATE DATABASE barbican;$ GRANT ALL PRIVILEGES ON barbican.* TO &apos;barbican&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;barbican&apos;;$ GRANT ALL PRIVILEGES ON barbican.* TO &apos;barbican&apos;@&apos;%&apos; IDENTIFIED BY &apos;barbican&apos;;$ exit; 建立一個 barbican 使用者。 123456789101112# openstack user create --domain default --password-prompt barbican+---------------------+----------------------------------+| Field | Value |+---------------------+----------------------------------+| domain_id | default || enabled | True || id | c33e4a4492944c278cd0f53791c98231 || name | barbican || options | {} || password_expires_at | None |+---------------------+----------------------------------+ 建立一個新的 project 並命名為 service。 1234567891011121314# openstack project create service+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | || domain_id | default || enabled | True || id | 31a3464dcf244931883c317fef836bbc || is_domain | False || name | service || parent_id | default || tags | [] |+-------------+----------------------------------+ 將 barbican User 指派為 service Project 的 admin Role。 1# openstack role add --project service --user barbican admin 建立 barbican Service。 1234567891011# openstack service create --name barbican --description &quot;Key Manager&quot; key-manager+-------------+----------------------------------+| Field | Value |+-------------+----------------------------------+| description | Key Manager || enabled | True || id | 018639d5972545b4b12406c23f9af2d9 || name | barbican || type | key-manager |+-------------+----------------------------------+ 建立 barbican endpoints。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# openstack endpoint create --region RegionOne key-manager public http://172.17.1.100:9311+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | e08911cc96584d9c8b8cdba8d98288e9 || interface | public || region | RegionOne || region_id | RegionOne || service_id | 018639d5972545b4b12406c23f9af2d9 || service_name | barbican || service_type | key-manager || url | http://172.17.1.100:9311 |+--------------+----------------------------------+# openstack endpoint create --region RegionOne key-manager internal http://172.17.1.100:9311+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | ab71ed47d2b241afa771551521b1341f || interface | internal || region | RegionOne || region_id | RegionOne || service_id | 018639d5972545b4b12406c23f9af2d9 || service_name | barbican || service_type | key-manager || url | http://172.17.1.100:9311 |+--------------+----------------------------------+# openstack endpoint create --region RegionOne key-manager admin http://172.17.1.100:9311+--------------+----------------------------------+| Field | Value |+--------------+----------------------------------+| enabled | True || id | bca4f788e5a6497aa314a810c637f545 || interface | admin || region | RegionOne || region_id | RegionOne || service_id | 018639d5972545b4b12406c23f9af2d9 || service_name | barbican || service_type | key-manager || url | http://172.17.1.100:9311 |+--------------+----------------------------------+ 接著，修改 /etc/barbican/barbican.conf。 1234567891011121314151617# vim /etc/barbican/barbican.conf[DEFAULT]...sql_connection = mysql+pymysql://barbican:barbican@172.17.1.100/barbican...[keystone_authtoken]...www_authenticate_uri = http://172.17.1.100:5000auth_url = http://172.17.1.100:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = barbicanpassword = password 建立 OpenStack Barbican 相關資料表。 1# su -s /bin/sh -c &quot;barbican-manage db upgrade&quot; barbican 建立 httpd 啟動 OpenStack Barbican 配置檔。 1234567891011121314151617181920212223242526# vim /etc/httpd/conf.d/wsgi-barbican.confListen 9311&lt;VirtualHost *:9311&gt; ## Logging ErrorLog &quot;/var/log/httpd/barbican_wsgi_main_error_ssl.log&quot; LogLevel debug ServerSignature Off CustomLog &quot;/var/log/httpd/barbican_wsgi_main_access_ssl.log&quot; combined WSGIApplicationGroup %{GLOBAL} WSGIDaemonProcess barbican-api display-name=barbican-api group=barbican processes=2 threads=8 user=barbican WSGIProcessGroup barbican-api WSGIScriptAlias / &quot;/usr/lib/python2.7/site-packages/barbican/api/app.wsgi&quot; WSGIPassAuthorization On &lt;Directory /usr/lib&gt; &lt;IfVersion &gt;= 2.4&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.4&gt; Order allow,deny Allow from all &lt;/IfVersion&gt; &lt;/Directory&gt;&lt;/VirtualHost&gt; 啟動 OpenStack Barbican。 123# systemctl enable httpd.service# systemctl restart httpd.service# systemctl status httpd.service 確認 Barbican Port 已啟動並佔用。 123# netstat -ntlp | grep 9311tcp6 0 0 :::9311 :::* LISTEN 5924/httpd 建立 OpenStack Barbican 環境變數檔。 123456789# vim barbicanrcexport OS_USERNAME=barbicanexport OS_PASSWORD=passwordexport OS_PROJECT_NAME=serviceexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_AUTH_URL=http://172.17.1.100:5000/v3export OS_IDENTITY_API_VERSION=3 利用 source 指令更新 OpenStack 環境變數 1# source barbicanrc 結果建立第一把 KEM Key。 12345678910111213141516# openstack secret store --name mysecret --payload YXN1c2FzdXNhc3VzYXN1c2FzdXNhc3VzYXN1c2FzdXM= --payload-content-type application/octet-stream --payload-content-encoding base64 --secret-type symmetric+---------------+-----------------------------------------------------------------------+| Field | Value |+---------------+-----------------------------------------------------------------------+| Secret href | http://localhost:9311/v1/secrets/309092d6-6def-4e8c-aad4-637fda3722ca || Name | mysecret || Created | None || Status | None || Content types | {u&apos;default&apos;: u&apos;application/octet-stream&apos;} || Algorithm | aes || Bit length | 256 || Secret type | symmetric || Mode | cbc || Expiration | None |+---------------+-----------------------------------------------------------------------+ 確認當前已註冊的 Key。 1234567# openstack secret list+-----------------------------------------------------------------------+----------+---------------------------+--------+-------------------------------------------+-----------+------------+-------------+------+------------+| Secret href | Name | Created | Status | Content types | Algorithm | Bit length | Secret type | Mode | Expiration |+-----------------------------------------------------------------------+----------+---------------------------+--------+-------------------------------------------+-----------+------------+-------------+------+------------+| http://localhost:9311/v1/secrets/309092d6-6def-4e8c-aad4-637fda3722ca | mysecret | 2020-06-20T14:16:58+00:00 | ACTIVE | {u&apos;default&apos;: u&apos;application/octet-stream&apos;} | aes | 256 | symmetric | cbc | None |+-----------------------------------------------------------------------+----------+---------------------------+--------+-------------------------------------------+-----------+------------+-------------+------+------------+ 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2020/06/20/Ceph-RGW-OpenStack-Barbican-配置教學-上篇/"},{"title":"Ceph RGW + OpenStack Keystone 配置教學(上篇)","text":"Ceph Object Storage 除了可以自身管理使用者訊息與驗證外，也可以透過 OpenStack Keystone 進行管理。本篇會先介紹如何在 CentOS 7 上安裝 OpenStack Keystone (Version Stein)，後續會在下篇時介紹如何與 Ceph 進行串接！ 前置條件 需具備一台 CentOS 7 實體機器或虛擬機器 安裝機器需具備 MariaDB 安裝在 /etc/selinux/config 將 SELINUX 註解移除並把值更改為 disabled 來關閉 SELinux，更改完儲存並重新開機 12345678$ setenforce 0$ vim /etc/selinux/config...SELINUX=disabled...$ reboot 添加 OpenStack Keystine(Stein) rpm 並安裝 123$ yum install http://repos.fedorapeople.org/repos/openstack/openstack-stein/rdo-release-stein-1.noarch.rpm$ yum update$ yum install openstack-keystone httpd mod_wsgi python-openstackclient gcc 在 MariaDB 建立 Keystone 資料庫並且建立 keystone 使用者賦予 任意 及 localhost 存取 keystone 資料表權限 12345$ mysql -u root -p$ CREATE DATABASE keystone;$ GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&lt;KEYSTONE_USER_PASSWORD&gt;&apos;;$ GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;%&apos; IDENTIFIED BY &apos;&lt;KEYSTONE_USER_PASSWORD&gt;&apos;;$ exit 修改 OpenStack Keystone 配置檔，在 [database] 加入資料庫配置 12345$ vim /etc/keystone/keystone.conf...[database]connection = mysql+pymysql://keystone:&lt;KEYSTONE_USER_PASSWORD&gt;@&lt;DATABASE_IP&gt;:&lt;DATABASE_PORT&gt;/keystone... 在 [token] 配置以 fernet 提供 Token 讓其他元件可以進行驗證，更改完後儲存並離開 1234...[token]provider = fernet... 根據 OpenStack Keystone 需具備的資料表來同步資料庫 1$ su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone 利用 fernet 針對 Token 進行初始化與建立驗證所需的 Token 12$ keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone$ keystone-manage credential_setup --keystone-user keystone --keystone-group keystone 建立 OpenStack Keystone 的 admin 使用者與建立預設的 domain, project 與 endpoint 等 12345$ keystone-manage bootstrap --bootstrap-password &lt;ADMIN_PASSWORD&gt; \\ --bootstrap-admin-url http://&lt;KEYSTONE_IP&gt;:5000/v3/ \\ --bootstrap-internal-url http://&lt;KEYSTONE_IP&gt;:5000/v3/ \\ --bootstrap-public-url http://&lt;KEYSTONE_IP&gt;:5000/v3/ \\ --bootstrap-region-id RegionOne 啟動 OpenStack Keystone 服務 123$ systemctl enable httpd.service$ systemctl start httpd.service$ systemctl status httpd.service 建立 adminrc 檔案來儲存 admin 使用者的環境變數，輸入完成後儲存並且離開 123456789$ vim adminrcexport OS_USERNAME=adminexport OS_PASSWORD=12345678export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_AUTH_URL=http://172.17.1.110:5000/v3export OS_IDENTITY_API_VERSION=3 在使用任何 OpenStack 指令時記得先將環境變數添加到系統中 1$ source adminrc 結果利用 OpenStack 指令任意呼叫服務，確認是否收到 OpenStack 回應。以下是利用查詢當前 OpenStack 已存在的服務為例，若是剛建立的環境，可以看到系統回應 Keystone 服務的資訊 123456$ openstack service list+----------------------------------+----------+----------+| ID | Name | Type |+----------------------------------+----------+----------+| 826222db3f2649d9b14be428f4873e8c | keystone | identity |+----------------------------------+----------+----------+ 常見問題OpenStack 套件沒有提供 version 指令查詢當前安裝的 Keystone 版本。因此我們需要透過 yum 當前已安裝的套件來確認當前 Keystone 安裝的版本 1234567891011121314151617$ yum list | grep keystoneopenstack-keystone.noarch 1:15.0.0-1.el7 @openstack-steinpython2-keystone.noarch 1:15.0.0-1.el7 @openstack-steinpython2-keystoneauth1.noarch 3.13.1-1.el7 @openstack-steinpython2-keystoneclient.noarch 1:3.19.0-1.el7 @openstack-steinpython2-keystonemiddleware.noarch 6.0.0-1.el7 @openstack-steinopenstack-barbican-keystone-listener.noarchopenstack-keystone-doc.noarch 1:15.0.0-1.el7 openstack-steinpuppet-keystone.noarch 14.4.0-1.el7 openstack-steinpython-keystoneauth1-doc.noarch 3.13.1-1.el7 openstack-steinpython-keystoneclient-doc.noarch 1:3.19.0-1.el7 openstack-steinpython-keystonemiddleware-doc.noarch 6.0.0-1.el7 openstack-steinpython2-keystone-tests.noarch 1:15.0.0-1.el7 openstack-steinpython2-keystone-tests-tempest.noarch 0.2.0-1.el7 openstack-steinpython2-keystone-tests-tempest-doc.noarchpython2-keystoneclient-tests.noarch 1:3.19.0-1.el7 openstack-stein 若遇到系統回應 Missing value auth-url required for auth plugin password 訊息，表示當前驗證發生問題，即可能是為將使用者的環境變數加至系統中。若以 admin 使用者為例，可以利用安裝最後所建立的 adminrc 來解決此問題 1$ source adminrc 在建置過程中，可以針對不同的使用者建立不同的 rc 檔來儲存使用者的參數，以方便做使用者切換 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2019/09/22/Ceph-RGW-OpenStack-Keystone-配置教學-上篇/"},{"title":"Ceph RGW + OpenStack Keystone 配置教學(下篇)","text":"事隔多日來繼續介紹後續 Ceph RGW 與 OpenStack Keystone 的整合配置，還沒有讀上篇的讀者可以先閱讀上篇，若有興趣也可逐步跟著這系列的文章進行操作，或許你會對於內容更有感覺唷！ 話不多說就開始吧！！ 前置條件 在虛擬機器或實體機器上已安裝 Ceph RGW 與 OpenStack Keystone 環境變數 OS_USERNAME=admin OS_PASSWORD=password OS_PROJECT_NAME=admin OS_USER_DOMAIN_NAME=Default OS_PROJECT_DOMAIN_NAME=Default OS_AUTH_URL=http://172.17.1.100:5000/v3 OS_IDENTITY_API_VERSION=3 注意，注意，注意，很重要所以講三次！！本篇介紹採用環境變數與上篇有些許不同。讀者需特別注意！ 配置首先修改 /etc/ceph/ceph.conf 檔案，並加入 OpenStack keystone 相關配置，如：domain, project, user …等。 整體配置如下： 1234567891011121314151617181920$ vim /etc/ceph/ceph.conf...[client.radosgw.gateway]host = 172.17.1.110:6789keyring = /etc/ceph/ceph.radosgw.keyringlog file = /var/log/ceph/client.radosgw.gateway.logrgw enable usage log = true# For keystone of openstackrgw keystone api version = 3# Enable Keystone Auth for S3 APIsrgw keystone url = http://172.17.1.100:5000rgw s3 auth use keystone = truergw keystone admin user = adminrgw keystone admin password = passwordrgw keystone admin domain = Defaultrgw keystone admin project = adminrgw keystone accepted roles = adminrgw keystone verify ssl = false OpenStack Keystone 參數部分讀者需依照自己建置 OpenStack Keystone 的設定自行修改。 Ceph RGW 配置完成後，需重新啟動 1$ systemctl restart ceph-radosgw@radosgw.gateway 配置完成後，就可以開始在建立 OpenStack Keystone 內建立 Ceph RGW 的使用者了，透過 openstack ec2 credentials 指令進行建立。 123456789101112$ openstack ec2 credentials create --project admin --user admin+------------+--------------------------------------------------------------------------------------------------------------------------------------+| Field | Value |+------------+--------------------------------------------------------------------------------------------------------------------------------------+| access | 25f332ce156248e6b1370a30276db861 || links | {u&apos;self&apos;: u&apos;http://172.17.1.100:5000/v3/users/4a6ccdd4c5d44b90951afd1c7f0051e0/credentials/OS-EC2/25f332ce156248e6b1370a30276db861&apos;} || project_id | 11545854141449f7abb6b7782f61cbce || secret | ddd08e575e7949e0ba0efeed5386d679 || trust_id | None || user_id | 4a6ccdd4c5d44b90951afd1c7f0051e0 |+------------+--------------------------------------------------------------------------------------------------------------------------------------+ 輸入完成後，系統會回應如上的資訊，其中 access 與 secret 就是 Ceph RGW 的 access key 與 secret key。 結果在 OpenStack Keystone 使用者建立完成後，需要任意呼叫一支 Ceph RGW API 或透過第三方工具，如: s3browser, cyberduck 進行登入，才會在 Ceph RGW 看到該使用者資訊。 登入前檢查 Ceph RGW 當前使用者列表。 12345$ radosgw-admin user list[ &quot;admin&quot;] 開啟 Cyberduck 並點擊視窗中左下角的”+”符號新增一個”S3(HTTP)”的連線，並且輸入您 Server URL 與 RGW Port，以及本篇介紹在 OpenStack Keystone 所建立的 ec2 credential access key 與 secret key 進行登入。 最後，我們再透過 Ceph RGW 指令檢查當前使用者列表。 123456$ radosgw-admin user list[ &quot;admin&quot;, &quot;11545854141449f7abb6b7782f61cbce&quot;] 從上述可以看到多一個名為 11545854141449f7abb6b7782f61cbce 的使用者，該名稱來自於 OpenStack Project Name。 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2020/06/19/Ceph-RGW-OpenStack-Keystone-配置教學-下篇/"},{"title":"動態配置 HAProxy Configuration ","text":"在現在任何的雲端服務開發或部署時，都會面臨如何能處理大量使用者同時存取服務所產生大量請求的問題。然而大多數的處理方式免不了需要透過負載平衡服務，來將使用者請求分派至不同的實體或虛擬機器進行處理，因此負載平衡服務就扮演極為重要的角色。 本篇就以 HAProxy 為例，將使用者的請求以 Round Robin 的方式平均分派至不同機器進行處理，並結合 Data Plane API 提供一個 RESTful API 介面以提供動態修改 HAProxy 配置的功能。 軟體版本 Software Name Version HAProxy 2.2.4 HAProxy Data Plane API 2.1.0 環境 OS Host Name Private Network CentOS 7 haproxy 192.168.5.2 CentOS 7 httpd1 192.168.5.3 CentOS 7 httpd2 192.168.5.4 安裝 HAProxy更新 CentOS 上的 packags 與安裝相依套件 12$ yum update$ yum install wget systemd-devel 至 HAProxy Conmmunity Edition 下載 HAProxy 2.2.4(LTS) 並解壓縮 12$ wget http://www.haproxy.org/download/2.2/src/haproxy-2.2.4.tar.gz$ tar -zxvf haproxy-2.2.4.tar.gz 進入 haproxy-2.2.4/ 目錄，並進行編譯 12$ cd haproxy-2.2.4$ make TARGET=linux-glibc USE_SYSTEMD=1 安裝需要 make 與 gcc 兩個套件，若是使用 CentOS 作業系統可透過 yum install 指令進行安裝。 安裝 HAProxy 2.2.4 1$ make install 進入 haproxy-2.2.4/contrib/systemd/ 目錄編譯 haproxy.service 檔案，並將產生出來的 haproxy.service 移動至 /lib/systemd/system/ 目錄底下 12$ make$ mv haproxy.service /lib/systemd/system/ 建立 HAProxy 配置檔放置目錄，並寫入基本配置 1234567891011121314151617181920212223242526272829$ mkdir -p /etc/haproxy$ mkdir -p /var/lib/haproxy$ vim /etc/haproxy/haproxy.cfgglobal daemon chroot /var/lib/haproxy user haproxy group haproxy stats timeout 30sdefaults mode http log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000frontend http_front bind *:80 stats uri /haproxy?stats default_backend http_backbackend http_back balance roundrobin server server_name1 192.168.5.3:80 check server server_name2 192.168.5.4:80 check 建立 haproxy use 與 group 1$ useradd -r haproxy 啟動 HAProxy 服務 12345678910$ systemctl start haproxy$ systemctl status haproxy● haproxy.service - SYSV: HA-Proxy is a TCP/HTTP reverse proxy which is particularly suited for high availability environments. Loaded: loaded (/etc/rc.d/init.d/haproxy; bad; vendor preset: disabled) Active: active (running) since Sun 2020-10-25 09:01:42 UTC; 1s ago Docs: man:systemd-sysv-generator(8) Process: 13370 ExecStart=/etc/rc.d/init.d/haproxy start (code=exited, status=0/SUCCESS) Main PID: 13381 (haproxy) CGroup: /system.slice/haproxy.service └─13381 /usr/sbin/haproxy -D -f /etc/haproxy/haproxy.cfg -p /var/run/haproxy.pid 到這裡，基本的 HAProxy 已經安裝完成，接下來進行 HAProxy Data Plane API 安裝 安裝 HAProxy Data Plane API下載 HAProxy Data Plane API 1$ wget https://github.com/haproxytech/dataplaneapi/releases/download/v2.1.0/dataplaneapi_2.1.0_Linux_x86_64.tar.gz 解壓縮 dataplaneapi_2.1.0_Linux_x86_64.tar.gz 檔案並將 build/dataplaneapi 檔案複製至 /usr/local/bin 目錄 12$ tar -zxvf dataplaneapi_2.1.0_Linux_x86_64.tar.gz$ cp build/dataplaneapi /usr/sbin/ 在 /etc/haproxy/haproxy.cfg 中加入 userlist api 來設定呼叫 HAPorxy Data Plane API 驗證的使用者名稱與密碼。以本例 james 為帳號， mypassword 為密碼。 1234$ vim /etc/haproxy/haproxy.cfg userlist api user james insecure-password mypassword 修改完成後，重新啟動 haproxy 1$ systemctl restart haproxy 最後啟動 HAProxy Data Plane API 1$ dataplaneapi --host 0.0.0.0 --port 5555 --haproxy-bin /usr/sbin/haproxy --config-file /etc/haproxy/haproxy.cfg --reload-cmd &quot;systemctl reload haproxy&quot; --reload-delay 5 --userlist api HAProxy 也很貼心的提供一個 API 文件，只要啟動 HAProxy Data Plane API 後透過 http://127.0.0.1:5555/v2/docs 即可看到如下文件內容 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2020/10/25/HAProxy-Data-Plane-API/"},{"title":"Kubernetes CoreDNS 解析 Domain","text":"筆者最近在部署 Kubernetes 過程中，好奇每個元件究竟安裝與不安裝對 Kubernetes 造成什麼影響？CoreDNS 主要的功能之一是當 Pods 做套件更新或安裝時，可以提供 DNS Forward 功能，將請求轉發至外部進行 Domain 解析，並從正確的位址下載套件進行安裝。 本篇就聚焦在 CoreDNS 將 Pods 的請求轉發至外部 DNS Server 進行解析的運作流程。 問題描述首先先檢視當前 Kubernetes 上運行的 Pods。 123456789101112131415161718192021$ kubectl get po -A -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScalico-system calico-kube-controllers-56689cf96-wlv2h 1/1 Running 0 22h 192.168.11.0 k8s-m3 &lt;none&gt; &lt;none&gt;calico-system calico-node-57fd5 1/1 Running 0 22h 192.168.20.11 k8s-m2 &lt;none&gt; &lt;none&gt;calico-system calico-node-hl589 1/1 Running 0 22h 192.168.20.13 k8s-n1 &lt;none&gt; &lt;none&gt;calico-system calico-node-k7w68 1/1 Running 0 22h 192.168.20.12 k8s-m3 &lt;none&gt; &lt;none&gt;calico-system calico-node-mgxwl 1/1 Running 0 22h 192.168.20.10 k8s-m1 &lt;none&gt; &lt;none&gt;calico-system calico-node-sckxv 1/1 Running 0 22h 192.168.20.14 k8s-n2 &lt;none&gt; &lt;none&gt;calico-system calico-typha-5bc9cc898d-4drrk 1/1 Running 0 22h 192.168.20.14 k8s-n2 &lt;none&gt; &lt;none&gt;calico-system calico-typha-5bc9cc898d-l9hrw 1/1 Running 0 22h 192.168.20.10 k8s-m1 &lt;none&gt; &lt;none&gt;calico-system calico-typha-5bc9cc898d-r2knp 1/1 Running 0 22h 192.168.20.13 k8s-n1 &lt;none&gt; &lt;none&gt;calico-system calico-typha-5bc9cc898d-zfnlv 1/1 Running 0 22h 192.168.20.12 k8s-m3 &lt;none&gt; &lt;none&gt;default nginx-deployment-66b6c48dd5-kwmkx 1/1 Running 0 5h3m 192.168.111.194 k8s-n2 &lt;none&gt; &lt;none&gt;default nginx-deployment-66b6c48dd5-lssjq 1/1 Running 0 5h3m 192.168.215.65 k8s-n1 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-7vn8h 1/1 Running 0 22h 192.168.20.14 k8s-n2 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-8mxrg 1/1 Running 0 22h 192.168.20.12 k8s-m3 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-gzrlh 1/1 Running 0 22h 192.168.20.13 k8s-n1 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-swk6f 1/1 Running 0 22h 192.168.20.11 k8s-m2 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-zmnv8 1/1 Running 0 22h 192.168.20.10 k8s-m1 &lt;none&gt; &lt;none&gt;tigera-operator tigera-operator-b6c4bfdd9-jgl2d 1/1 Running 1 22h 192.168.20.11 k8s-m2 &lt;none&gt; &lt;none&gt; 在尚未安裝 CoreDNS 情境時，您會發現在任一個 Pod 內無法向外部資源進行存取。 這邊筆者先建立一個 Nginx 的 Deployment，並且進去其中一個 Pod 執行 apt update 指令，這時候會發現如下的情況： 123$ kubectl exec -ti nginx-deployment-66b6c48dd5-kwmkx bash$ apt update0% [Connecting to deb.debian.org] [Connecting to security.debian.org] 問題在於無法解析 Ubuntu 套件 Domain，因此無法正常更新套件與安裝套件。 解決方法回想一下，不管在安裝 Master Node 或 Worker Node 時，都會啟動 Kubelet，並且帶入 --config=/var/lib/kubelet/config.yml 參數。 在 /var/lib/kubelet/config.yml 檔案內會需要配置 clusterDNS。這就是運行在這個節點上的 Pods 預設的 DNS Server。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172$ cat /var/lib/kubelet/config.ymlapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0port: 10250readOnlyPort: 10255authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30scgroupDriver: cgroupfscgroupsPerQOS: trueclusterDNS:- 10.96.0.1 // HEREclusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000maxPods: 110nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: truestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0s 如上述情境，若在這個 Worker Node 上運行一個 Pod，預設 DNS Server 為 10.96.0.10。 實際我們進入 nginx-deployment-66b6c48dd5-kwmkx Pod，並且檢視 /etc/resole.conf 檔案。 123456$ kubectl exec -ti nginx-deployment-66b6c48dd5-kwmkx bash$ cat /etc/resolv.confnameserver 10.96.0.10search default.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 由上述結果可以看出 Pod 會採用 10.96.0.10 作為預設 DNS Server。 接著我們就必須利用 CoreDNS 啟動時，使用 10.96.0.10 作為 ClusterIP，將所有 Pods 請求 Forward 到外部 8.8.8.8 進行 Domain 解析。 因此在啟動 CoreDNS 時需要在 ConfigMap 中加入 forward . 8.8.8.8:53 配置，並且在 Service 將 ClusterIP 設定為 10.96.0.10。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354$ cat coredns.yml...apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 8.8.8.8:53 cache 30 loop reload loadbalance }... apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: &quot;9153&quot; prometheus.io/scrape: &quot;true&quot; labels: k8s-app: kube-dns kubernetes.io/cluster-service: &quot;true&quot; kubernetes.io/name: &quot;CoreDNS&quot;spec: selector: k8s-app: kube-dns clusterIP: 10.96.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP 配置完成後建立 CoreDNS 服務。 1$ kubectl create -f coredns.yml 啟動完成後，檢視當前所有 Serivce 狀態。 123456$ kubectl get svc -ANAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEcalico-system calico-typha ClusterIP 10.105.208.37 &lt;none&gt; 5473/TCP 22hdefault kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 28hkube-system kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 4h39m 由上述可以看出 CoreDNS 已經被啟動，並且 ClusterIP 為 10.96.0.10。 最後，我們在進入 nginx-deployment-66b6c48dd5-kwmkx Pod 再次進行套件更新。 12345678910111213141516$ kubectl exec -ti nginx-deployment-66b6c48dd5-kwmkx bash$ apt updateIgn:1 http://deb.debian.org/debian stretch InReleaseGet:2 http://security.debian.org/debian-security stretch/updates InRelease [53.0 kB]Get:3 http://deb.debian.org/debian stretch-updates InRelease [93.6 kB]Get:4 http://security.debian.org/debian-security stretch/updates/main amd64 Packages [649 kB]Get:5 http://deb.debian.org/debian stretch Release [118 kB]Get:6 http://deb.debian.org/debian stretch-updates/main amd64 Packages [2596 B]Get:7 http://deb.debian.org/debian stretch Release.gpg [2410 B]Get:8 http://deb.debian.org/debian stretch/main amd64 Packages [7080 kB]Fetched 7998 kB in 24s (332 kB/s)Reading package lists... DoneBuilding dependency treeReading state information... Done25 packages can be upgraded. Run &apos;apt list --upgradable&apos; to see them. 由上述可以看出現在 Pod 已經可以至外部進行套件更新與安裝。 總結由下圖來解釋上述所做的行為，在 Nginx Pod 內下 apt update 指令時，請求根據 DNS Server 轉發至 CoreDNS Pod(10.96.0.10)，再由 CoreDNS Pod 將請求 Forward 到外部 DNS Server(8.8.8.8) 進行解析。因此就可以在 Pod 內正常進行套件的更新與安裝等行為。 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2021/02/07/Kubernetes-CoreDNS/"},{"title":"HTTPS 驗證解析","text":"時常在開發任何應用服務時，會利用 HTTP 協定來串接服務與服務之間的通訊，然而這樣的方式可能存在著一定的風險，因此就有人會想是否可以在傳輸過程中加入驗證機制，來提高資料的安全性，進而衍伸出 TLS 雙向驗證機制。 筆者在了解其中運作原理時，意外在網路上看到一篇文章，用故事方式來描述這之間的關係，覺得還不錯分享給讀者們。 大意如下： 有一天 A 公司的業務要去 B 公司進行業務接洽，當業務到達 B 公司時，被 B 公司的警衛攔下，並要求業務出示身份。不過 B 公司沒有人認識這位業務，因此業務只能拿出蓋有 A 公司印章的文件來證明自己是 A 公司的員工。 聽完以上的敘述雖然可能大致上沒什麼問題，不過有人可能會想到，如果這樣 B 公司有 100 的合作廠商，不就需要記住 100 個公司的印章才能身任身份驗證的任務。 因此，C 公司看準了這個商機，成為了第三方的驗證機構。在 B 公司信任 C 公司與 C 公司信任 A 的的基礎上，只要 A 公司向 B 公司提出自己與 C 公司的印章，則為合法。如此一來 B 公司只要記得 C 公司印章即可。 上述 C 公司就是 CA 的角色，驗證的過程如故事情境，接著我們就實際來建置完整的驗證流程。 cfssl 簡介我們使用 cfssl 這個工具來產生金鑰與憑證，cfssl 是 CloudFlare 公司是基於 Golang 開源的 PKI/TLS 工具。當前僅支援 ECDSA 與 RSA 兩種方式。 安裝首先要先安裝 cfssl 工具 1234$ export CFSSL_URL=https://pkg.cfssl.org/R1.2$ wget ${CFSSL_URL}/cfssl_linux-amd64 -O /usr/local/bin/cfssl$ wget ${CFSSL_URL}/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson 安裝完成後，利用指令檢查當前安裝版本。 12345$ cfssl versionVersion: 1.2.0Revision: devRuntime: go1.6 配置在建立服務憑證前，我們需要先建立 ca/ca-csr.json 檔案來產生 CA 憑證與金鑰。ca/ca-csr.json 大致上會描述如下。 12345678910111213141516171819$ mkdir ca$ vim ca-csr.json{ &quot;CN&quot;: &quot;james-blog.com&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;james company&quot;, &quot;OU&quot;: &quot;james company&quot; } ]} CN(Common Name) 用來識別使用的 Domain。 Key 描述用 RSA 進行加密，並且長度為 2048(當前僅支援 2048 - 8192) Name.C 表示國家 Name.ST 與 L 表示城市 Name.O 表示公司 Name.OU 表示公司部門 建立完成後，初始化 CA 憑證與金鑰。 12345678$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca/ca2021/02/08 07:49:22 [INFO] generating a new CA key and certificate from CSR2021/02/08 07:49:22 [INFO] generate received request2021/02/08 07:49:22 [INFO] received CSR2021/02/08 07:49:22 [INFO] generating key: rsa-20482021/02/08 07:49:23 [INFO] encoded CSR2021/02/08 07:49:23 [INFO] signed certificate with serial number 512432585453518655479838519048416241349248297204 檢視當前 ca/ 目錄內的檔案，即 ca-key.pem 為金鑰，ca.pem 為憑證。 123$ ls ca/ca.csr ca-key.pem ca.pem 完成後，接著利用 CA 來建立服務的金耀與憑證。我們利用 nginx 作為示範，建立 nginx/nginx-csr.json 檔案。 12345678910111213141516171819$ mkdir nginx$ vim nginx/nginx-csr.json{ &quot;CN&quot;: &quot;james-blog.com&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;james company&quot;, &quot;OU&quot;: &quot;james company&quot; } ]} 這邊我們就用與 CA 相同的檔案內容。 建立 ca-config.json 檔案。 1234567891011121314151617181920$ vim ca-config.json{ &quot;signing&quot;: { &quot;default&quot;: { &quot;expiry&quot;: &quot;87600h&quot; }, &quot;profiles&quot;: { &quot;nginx&quot;: { &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; } } }} expiry 表示可用時間為一年 signing 表示可用於其他簽名證書 server auth 表示 Client 可以 CA 對 Server 提供的憑證進行驗證 client auth 表示 Server 可以 CA 對 Client 提供的憑證進行驗證 基於 CA 的憑證與金鑰，產生 nginx 服務憑證與金鑰。 12345678910111213$ cfssl gencert \\ -ca=ca/ca.pem \\ -ca-key=ca/ca-key.pem \\ -config=ca-config.json \\ -hostname=james-blog.com \\ -profile=nginx \\ nginx/nginx-csr.json | cfssljson -bare nginx/nginx 2021/02/08 08:03:09 [INFO] generate received request2021/02/08 08:03:09 [INFO] received CSR2021/02/08 08:03:09 [INFO] generating key: rsa-20482021/02/08 08:03:09 [INFO] encoded CSR2021/02/08 08:03:09 [INFO] signed certificate with serial number 67599991771298537291786410610924862791459550062 檢視當前 nginx/ 目錄內的檔案，即 nginx-key.pem 為金鑰，nginx.pem 為憑證。 123$ ls nginx/nginx.csr nginx-csr.json nginx-key.pem nginx.pem 接著將 nginx/nginx.pem 與 nginx/nginx-key.pem 內容合成一份名為 tls.pem 檔案以提供 HAProxy 配置使用。 12$ cat nginx/nginx.pem &gt; tls.pem$ cat nginx/nginx-key.pem &gt;&gt; tls.pem 檢視當前 tls.pem 內容。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ cat tls.pem-----BEGIN CERTIFICATE-----MIIEGjCCAwKgAwIBAgIUC9dJ2RtjdZYV/EvQvYWwUrl0M24wDQYJKoZIhvcNAQELBQAweDELMAkGA1UEBhMCVFcxDzANBgNVBAgTBlRhaXBlaTEPMA0GA1UEBxMGVGFpcGVpMRYwFAYDVQQKEw1qYW1lcyBjb21wYW55MRYwFAYDVQQLEw1qYW1lcyBjb21wYW55MRcwFQYDVQQDEw5qYW1lcy1ibG9nLmNvbTAeFw0yMTAyMDgwNzU4MDBaFw0zMTAyMDYwNzU4MDBaMHgxCzAJBgNVBAYTAlRXMQ8wDQYDVQQIEwZUYWlwZWkxDzANBgNVBAcTBlRhaXBlaTEWMBQGA1UEChMNamFtZXMgY29tcGFueTEWMBQGA1UECxMNamFtZXMgY29tcGFueTEXMBUGA1UEAxMOamFtZXMtYmxvZy5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDxLh+7I7avapikDuiP9JQhHJu4hVlxrcvKtrMcXP9QcqiRkrgJ/Z2kWxM1KRq1TEFpArBCaLBxDpRVDuH3zG5b+gbZSF+uAK1dWBy7ANFnSjj49/XtD54gU9mu0DUoGgQFU0zfI7RDOKwXbVmL04WBVI1avS88qxVajjaKCwvz4ubvIHrDhJRzVIzKrGuC5F7lRoahQTfL6Qu6kD89AgpiunPnAeMaQULULKt4ScdPrZsbnd6hlRBt8SsFx6ain/Z9J2FG/J27XwtV78nO409owZ4Is/ZwANQCoI3BdvEO4i2p5B0f5517FKM8p13ZQ02WbFPHpch47XDUgpJWqQ8BAgMBAAGjgZswgZgwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBSLQt4fZ6N7eBFxFRfGrb/Yy8sTOjAfBgNVHSMEGDAWgBTgpE193bdCcTryuM5SoEVqxDpUFDAZBgNVHREEEjAQgg5qYW1lcy1ibG9nLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEATsIqEmyayWvrfapcEMnzrirrr1uIPRHlWYjzWx0HRtaxnWzk/mgtDSDUTzAVM7+unQFqy7ZsJoI7zsDuhfidv8WG3ChdSs773grz5m6GB3Wy8n+P5LNmqp3uKN80e1A0fjccqoKcec29YdMjlCRWj/El7OF9vOQECROFiGDiXvz9Ou0BsAB79GgTjMRQfwIVTjgVhh9rVR5Q4eDtaUqnOF5eWxo8erHBw8wIhfjrSI7Mfa09MWwF4ohC3U1tHnkb44gjdz6wfZ1xFv/7rw6LA1bGbD5mj2FUKoMKnzL9FkQhoe8H5YBchdB5IUS6MZygmwAgQ4L89dByLes3QJAT6Q==-----END CERTIFICATE----------BEGIN RSA PRIVATE KEY-----MIIEowIBAAKCAQEA8S4fuyO2r2qYpA7oj/SUIRybuIVZca3LyrazHFz/UHKokZK4Cf2dpFsTNSkatUxBaQKwQmiwcQ6UVQ7h98xuW/oG2UhfrgCtXVgcuwDRZ0o4+Pf17Q+eIFPZrtA1KBoEBVNM3yO0QzisF21Zi9OFgVSNWr0vPKsVWo42igsL8+Lm7yB6w4SUc1SMyqxrguRe5UaGoUE3y+kLupA/PQIKYrpz5wHjGkFC1CyreEnHT62bG53eoZUQbfErBcemop/2fSdhRvydu18LVe/JzuNPaMGeCLP2cADUAqCNwXbxDuItqeQdH+edexSjPKdd2UNNlmxTx6XIeO1w1IKSVqkPAQIDAQABAoH/QuOH6V7+S6hErTt1RHeQnZ5Rkdtp8x1AZ/hDtJoWNTmXhsgqQpsUHYEk0pmTzrRXb8hPGhvu02w3t19pTFBmSxwMyjZIUvndGwZU8YhIi10KcAJVMmoicNTQiqs0EuskMlXn+/rrA7m0AMtTCnSfkj7g9UmC0FCim3rVpXNq3TiG9H+tFPnOFadTbRJi0CkADgMRrmywliBe/7DdB6Xcp/2jxHeoMiqqOcszDgIxG5CoE03mghqqfxalJXzioMexaMyMH4epoYFaKVsIKfOIo1Fie8JPCtOP0DEi3BOolaFd9Nrj3RwgMIoQQ0ZeOdkJUvZRklOxJHwG3hEQ8sNRAoGBAPJM9SueSnDqqhofjcRl8a5CKY9y7zwTiXy92VQFhsK21fG8EviEgs50577EHE4Cb5gxQxsPSK54gxOOZ2ZbP11ynaKbI2ZbPUUgNppiaPc+ERG6xBL4e0/rVzAqd5kAZZUov5oi2RW6cBtQeONhgdRGN3Fq0an1QCMaJe9TRcRlAoGBAP7Q8vLpL7rAs8jqllD+Ijx7aDzLMDHD2k2iWatSe6uIXghyqOD7VmSI4goYYqtcipRv53iHdwWm/c6BtqwHrsOgM12ZiqotDDOUhgJIe29IEdGHG+fJ9rfefAbeV53I8CadL+G/wtmaMDzq9h6FZmMPT2lzvYj9oeBKQAfqw7BtAoGBAIjD5583Oc2Cp4EXTm4NHN2/erX9qgu++1vtzT4f4HEHwrsv7YVZRnxCgIytJUzjExpUtAwSFHRmkZX4S2T7HEki6NdfuhuMZIkgJbH+2kC0R+45/XK3zuLNc+k8D0XNc4k99uiJwv8AvUatpY6y+xVWjPT31mCYjhtCJydvWXIBAoGBAMC0ECc3xgq7lLsK/WZ+6jFHOotPNkFMVhmD/8AafsA4PrSw0ZpjOPCKvDbaPjRNpdef0TNLbu1tXl//pL/wh3AWBQJyDXWo36NaXQX4/rAnlqIYRThDejuPG8it+SCwRz1MfluBA8BAZN6M6lgmlkmv2GRtTRb+iJ7wSAA0wIz5AoGBAJhIzuMCMuUpywBPJr6L4zwWBswhqds/KZX31EcYCFVxL/rtkNQhjy/bmL+JGNARMbYHadqP/wobwqhY2WI9ZtBb+A+mVKl/kO38q+hwUCigOumo2P3U76SR9ofrV/klBKDE8Az+D5M0zwWvmosutoIQDpgiif20qlySdX1N9U7g-----END RSA PRIVATE KEY----- 最後，編輯 /etc/haproxy/haproxy.cfg 檔案，加入 TLS 配置。 1234567891011121314$ vim /etc/haproxy/haproxy.cfg...global ... tune.ssl.default-dh-param 2048frontend f_nginx mode http bind *:8443 ssl crt /home/vagrant/tls.pem default_backend b_nginxbackend b_nginx server nginx-1 192.168.10.10:80 check 在 global 加入 tune.ssl.default-dh-param 2048 並在最底部加入上述配置，其原因為 RSA 為長度 2048，預設 HAProxy 為 1024，若無調整則啟動時會產生警告。 重新啟動 HAProxy 服務。 1$ systemctl restart haproxy 檢視當前 HAProxy 狀態。 12345678910111213141516171819202122$ systemctl status haproxy● haproxy.service - HAProxy Load Balancer Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2021-02-08 08:11:44 UTC; 1s ago Docs: man:haproxy(1) file:/usr/share/doc/haproxy/configuration.txt.gz Process: 15325 ExecStartPre=/usr/sbin/haproxy -f $CONFIG -c -q $EXTRAOPTS (code=exited, status=0/SUCCESS) Main PID: 15336 (haproxy) Tasks: 2 (limit: 1074) Memory: 6.6M CGroup: /system.slice/haproxy.service ├─15336 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock └─15337 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sockFeb 08 08:11:44 ubuntu systemd[1]: Starting HAProxy Load Balancer...Feb 08 08:11:44 ubuntu haproxy[15336]: Proxy f_nginx started.Feb 08 08:11:44 ubuntu haproxy[15336]: Proxy f_nginx started.Feb 08 08:11:44 ubuntu haproxy[15336]: Proxy b_nginx started.Feb 08 08:11:44 ubuntu haproxy[15336]: Proxy b_nginx started.Feb 08 08:11:44 ubuntu haproxy[15336]: [NOTICE] 038/081144 (15336) : New worker #1 (15337) forkedFeb 08 08:11:44 ubuntu systemd[1]: Started HAProxy Load Balancer. 結果我們需要在 /etc/hosts 中加入 192.168.10.10 james-blog.com 讓系統認得這個 Domain。 1234$ vim /etc/hosts...192.168.10.10 james-blog.com 利用 cUrl 指令並帶入憑證進行驗證。 123456789101112131415161718192021222324252627$ curl --cacert nginx/nginx.pem https://james-blog.com:8443&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 若想要透過瀏覽器進行測試，在 MacOS 系統就需要將憑證加入到”鑰匙圈存取”中，並且開啟”永遠信任”。 若成功，則利用 Chrome 瀏覽器可以看到網址左邊有一個”鎖”的 icon。 問題因為我們在建立 nginx 服務憑證時，有註明使用的 hostname 為 james-blog.com，因此若使用其他們 domain 會發生錯誤。 12345678$ curl --cacert nginx/nginx.pem https://james1-blog.com:8443curl: (60) SSL: no alternative certificate subject name matches target host name &apos;james1-blog.com&apos;More details here: https://curl.haxx.se/docs/sslcerts.htmlcurl failed to verify the legitimacy of the server and therefore could notestablish a secure connection to it. To learn more about this situation andhow to fix it, please visit the web page mentioned above. 參考 https://www.cnblogs.com/LangXian/p/11282986.html","link":"/2021/02/08/PKI-驗證解析/"},{"title":"手動部署 Ceph Nautilus(14.2.2)","text":"對於剛進入 Ceph 領域的讀者，部署 Ceph 可能會採用相對應的部署工具，例如：ceph-deploy。雖然它可以幫助我們快速建立 Ceph 叢集，但是對於 Ceph 整體部署流程與過程中每個 Components(如：MON, MDS)等，運作方式可能會有疑惑。 因此希望可以透過本篇文章一步一步帶領讀者安裝 Ceph 叢集，讓讀者可以理解每個步驟用途與叢集運作的流程。 環境本篇文章利用 VirutalBox 建立一台 Ubuntu 16.04 虛擬機做 Ceph All in One，並且在實體機器規劃一個虛擬硬碟空間作為 Ceph OSD 使用。 在 VitualBox 掛載一顆虛擬硬碟後，可以透過 fdisk 指令再作進一步的 partitions。 參數 數值 Operating System Ubuntu 16.04 LTS Host Name ceph Private Network 172.17.1.100 Ceph Version 14.2.2 安裝Ceph MON更新套件集。12$ apt update$ apt upgrade -y 增加 release.asc Key。1$ wget -q -O- &apos;https://download.ceph.com/keys/release.asc&apos; | sudo apt-key add - 增加 Ceph Repo 到 source list。1$ apt-add-repository &apos;deb https://download.ceph.com/debian-nautilus/ xenial main&apos; 再次更新套件1$ apt update 安裝 Ceph 與 Ceph RADOSGW 相關套件。1$ apt install -y ceph radosgw radosgw-agent 確認 Ceph 版本(Version)。1$ ceph -v 替 Ceph 產生 UUID12$ uuidgencb9d566e-48e5-4810-9163-5c7784c8b3c0 目錄切換至 /etc/ceph/ 並且建立 ceph.conf 配置檔，將以下配置內容增加至 ceph.conf 配置檔內。12345678910111213141516171819202122$ cd /etc/ceph/$ vim ceph.conf# Version 14.2.2[global]fsid = cb9d566e-48e5-4810-9163-5c7784c8b3c0mon initial members = monmon host = 172.17.1.100public network = 172.17.1.0/24cluster network = 172.17.1.0/24auth cluster required = cephxauth service required = cephxauth client required = cephxmon max pg per osd = 1000mon allow pool delete = trueosd crush update on start = false fsid 填入前步驟所建立的 UUID 建立一把名為 ceph.mon.keyring 的 Ceph MON secret key 並且儲存於 /tmp 底下，賦予 mon 全部存取權限。123$ ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &apos;allow *&apos;creating /tmp/ceph.mon.keyring –gen-key 表示系統自動產生 secret key 建立一把名為 ceph.client.admin.keyring 的 Ceph clent.admin key 並儲存於 /etc/ceph/ 底下，賦予全部(MON, OSD, MDS 與 MGR) 存取權限。123$ ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon &apos;allow *&apos; --cap osd &apos;allow *&apos; --cap mds &apos;allow *&apos; --cap mgr &apos;allow *&apos;creating /etc/ceph/ceph.client.admin.keyring 將 ceph.mon.keyring 內容複製至 ceph.client.admin.keyring 內。123$ ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyringimporting contents of /etc/ceph/ceph.client.admin.keyring into /tmp/ceph.mon.keyring 替 Ceph 建立 monitor map。12345$ monmaptool --create --add mon 172.17.1.100:6789 --fsid `ceph-conf --lookup fsid` /tmp/monmapmonmaptool: monmap file /tmp/monmapmonmaptool: set fsid to cb9d566e-48e5-4810-9163-5c7784c8b3c0monmaptool: writing epoch 0 to /tmp/monmap (1 monitors) 修改相關檔案擁有者與群組。1234$ chown -R ceph:ceph /etc/ceph$ chown -R ceph:ceph /var/lib/ceph/mon$ chown -R ceph:ceph /tmp/monmap$ chown -R ceph:ceph /tmp/ceph.mon.keyring 初始化 Ceph MON。1$ ceph-mon --mkfs --id mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 修改相關檔案擁有者與群組。1$ chown ceph:ceph -R /var/lib/ceph/mon 啟動 Ceph MON。123456789101112$ systemctl enable ceph-mon@mon.service$ systemctl start ceph-mon@mon.service$ systemctl status ceph-mon@mon.service● ceph-mon@mon.service - Ceph cluster monitor daemon Loaded: loaded (/lib/systemd/system/ceph-mon@.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2019-08-18 04:32:05 UTC; 33ms ago Main PID: 21154 ((ceph-mon)) CGroup: /system.slice/system-ceph\\x2dmon.slice/ceph-mon@mon.service └─21154 (ceph-mon)Aug 18 04:32:05 ceph systemd[1]: Started Ceph cluster monitor daemon. Ceph MGR建立 bootstrap keyrings。1$ ceph-create-keys --id mon 利用 client.admin key 建立名為 client.bootstrap-osd 的 ceph.keyring。1$ ceph-authtool -C ceph.keyring -n client.bootstrap-osd -a AQBp1lhdBuU7JRAAPx5h/9bsviXgNcbtGWulcA== 步驟內 AQBp1lhdBuU7JRAAPx5h/9bsviXgNcbtGWulcA== 為 client.admin key，可利用 cat ceph.client.admin.keyring 指令查詢。 將 key 增加至 auth entries 內並賦予對應存取權限。1$ ceph auth get-or-create mgr.mgr mon &apos;allow profile mgr&apos; osd &apos;allow *&apos; mds &apos;allow *&apos; 將 mgr.mgr key 匯出並儲存於 keyring 檔案內。1$ ceph auth get-or-create mgr.mgr -o keyring 在 /var/lib/ceph/mgr/ 建立 ceph-mgr 目錄，並將 keyring 移至該目錄內。12$ mkdir -p /var/lib/ceph/mgr/ceph-mgr$ mv keyring /var/lib/ceph/mgr/ceph-mgr 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/mgr/ceph-mgr 啟動 Ceph MGR。123$ systemctl enable ceph-mgr@mgr.service$ systemctl start ceph-mgr@mgr.service$ systemctl status ceph-mgr@mgr.service OSD將 client.bootstrap-osd key 匯出並儲存於 /var/lib/ceph/bootstrap-osd/ 目錄下 ceph.keyring 檔案內。1$ ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/bootstrap-osd 將 /dev/sdc1 至 /dev/sdc4 作為 Ceph OSD。1234$ ceph-volume lvm create --bluestore --data /dev/sdc1$ ceph-volume lvm create --bluestore --data /dev/sdc2$ ceph-volume lvm create --bluestore --data /dev/sdc3$ ceph-volume lvm create --bluestore --data /dev/sdc4 修改相關檔案擁有者與群組。1234$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-0$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-1$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-2$ chown -R ceph:ceph /var/lib/ceph/osd/ceph-3 在 Ceph CRUSH Map 中規劃 OSD01 - OSD04 host。123456789101112131415$ ceph osd crush add-bucket OSD01 hostadded bucket OSD01 type host to crush map$ ceph osd crush add-bucket OSD02 hostadded bucket OSD02 type host to crush map$ ceph osd crush add-bucket OSD03 hostadded bucket OSD03 type host to crush map$ ceph osd crush add-bucket OSD04 hostadded bucket OSD04 type host to crush map 將所有的 host(OSD01 - OSD04) 移至 default root 底下。123456789101112131415$ ceph osd crush move OSD01 root=defaultmoved item id -3 name &apos;OSD01&apos; to location {root=default} in crush map$ ceph osd crush move OSD02 root=defaultmoved item id -4 name &apos;OSD02&apos; to location {root=default} in crush map$ ceph osd crush move OSD03 root=defaultmoved item id -5 name &apos;OSD03&apos; to location {root=default} in crush map$ ceph osd crush move OSD04 root=defaultmoved item id -6 name &apos;OSD04&apos; to location {root=default} in crush map 將 osd.0 - osd.4 綁定至對應的 OSD01 - OSD04 並賦予全部權重皆為 1.0。123456789101112131415$ ceph osd crush add osd.0 1.0 root=default host=OSD01add item id 0 name &apos;osd.0&apos; weight 1 at location {host=OSD01,root=default} to crush map$ ceph osd crush add osd.1 1.0 root=default host=OSD02add item id 1 name &apos;osd.1&apos; weight 1 at location {host=OSD02,root=default} to crush map$ ceph osd crush add osd.2 1.0 root=default host=OSD03add item id 2 name &apos;osd.2&apos; weight 1 at location {host=OSD03,root=default} to crush map$ ceph osd crush add osd.3 1.0 root=default host=OSD04add item id 3 name &apos;osd.3&apos; weight 1 at location {host=OSD04,root=default} to crush map MDS在 keyring 內增加 mds.mds key。12345678$ ceph-authtool --create-keyring keyring --gen-key -n mds.mds[mds.mds] key = AQDJ3Fhd4cISDRAAWKhKoqlGnmm39CaC5oTcWw==$ ceph auth add mds.mds osd &quot;allow rwx&quot; mds &quot;allow&quot; mon &quot;allow profile mds&quot; -i keyringadded key for mds.mds 在 /var/lib/ceph/mds/ 建立 ceph-mds 目錄，並將 keyring 移至該目錄內。12$ mkdir -p /var/lib/ceph/mds/ceph-mds$ mv keyring /var/lib/ceph/mds/ceph-mds 修改相關檔案擁有者與群組。1$ chown -R ceph:ceph /var/lib/ceph/mds/ 啟動 Ceph MDS。123$ systemctl enable ceph-mds@mds.service$ systemctl start ceph-mds@mds.service$ systemctl status ceph-mds@mds.service 結果利用 ceph -s 指令查詢當前 Ceph 叢集狀態。123456789101112131415root@ceph:/etc/ceph# ceph -s cluster: id: cb9d566e-48e5-4810-9163-5c7784c8b3c0 health: HEALTH_OK services: mon: 1 daemons, quorum mon (age 30m) mgr: mgr(active, since 18m) osd: 4 osds: 4 up (since 12m), 4 in (since 12m) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 61 MiB used, 3.9 GiB / 4 GiB avail pgs: 參考 https://docs.ceph.com/docs/master/","link":"/2019/08/18/手動部署-Ceph-Nautilus/"},{"title":"Vitess 介紹","text":"Vitess 是一套基於 MySQL 或 MariaDB 的叢集式資料庫系統，並且提供 Sharding 的概念達到水平擴展的機制。Vitess 採用單一端口的方式，使用者僅需透過單一端口進行資料庫的寫入，讀取或刪除行為，不必在意底層資料庫如何處理該行為，就能達到資料的分散儲存效果。Vitess 目前已經從 CNCF 專案中畢業，已有許多世界級大廠使用，如 Youtube, slack, GitHub… 等等。 官方文件中提到在選擇 MySQL 或 MariaDB 時，建議採用 MySQL 並且版本為 5.7。若要採用 MariaDB 則需要 10.0+ 特色 可擴展性：Vitess 整合 MySQL 多項重要的特點，提供開發者不必在應用程式中寫入 Shard 邏輯就能實作像是 NoSQL 的可擴展性 效能優化：Vitess 提供的 Cache 機制來處理重複的 Query 行為以提升整體資料庫查詢效能 可管理性：Vitess 提供主動與被動處理 Master 故障的問題排除與備份功能 連線池：Vitess 改善 MySQL 連線的高成本，以提供簡易的連線機制可同時提供上千個連線 Shard 管理：MySQL 不提供 Shard 概念，但是若資料庫規模逐漸擴大則可能需要透過 Shard 來進行多台主機的資料分片儲存，以提供整體讀寫效能 專有名詞Cell可視為一個 Zone 的概念。一個 Cell 會包含一台或一台以上的伺服器與一整個網路架構，並且每個 Cell 都是獨立的個體，彼此間是隔離的環境，可以比擬成一個資料中心或者一個群集的資料中心。 KeyspaceKeyspace 是一個虛擬的資料庫邏輯，可以分成兩種不同的情境來討論。第一種若採用 Sharding，則一個 Keyspace 底層會對應多個資料庫行程(process)，第二種若沒有採用 Sharding，則一個 Keyspace 底層僅對應一個資料庫行程。但無論如何，從應用程的角度看來不管上述兩種情境下，都只會有一個單一入口。因此應用程式取得資料就像是從一個資料庫讀取資料，不必理會底層如何處理這個讀取行為。但因為需要保持資料讀取一致性的情況下，Vitess 會由 Master 或 Raplicas 取得資料以提供呼叫端。對於程式開發者而言可以無痛地將原本呼叫 MySQL 的 Query 直接在 Vitess 上使用。 ShardShard 是單一個 Keyspace 的一部份，一個 Shard 通常會包含一個 Master 與多個 Replicas，並且每個 Shard 內的 MySQL 資料會是一致的。Replicas 僅提供 Read 而不提供 Write 功能，但有時候也會因為一些情況扮演不同的角色，如當前資料備份則需更改為 Backup，或當有新的 MySQL 啟動時需先進行資料恢復則會更改為 Restore…等 Shard 命名有一套規則： 一個數字範圍區間，其中左邊數字是包含的，而右邊數字不包含 數字是一個十六進制碼 採用 “-“ 符號進行左右兩數字切割 -80 == 00-80 == 0000-8000 == 000000-8000000 80- != 80-FF 80-FF == 8000-FF00 Vitess 也提供 Resharding 功能，提供已上線的 Shard 進行動態調整。在 Redharding 過程中，資料會由來源 Shard 複製到目的 Shard，並且可以確保來源與目的的資料一致性，最後將新的 Query 引導至目的 Shard 後將來源 Shard 移除。 VTTabletVTTablet 通常會與一個 MySQL 行程共同存在於一台實體機器中，每個 VTTablet 都會扮演以下的其中一種角色，處理不同的工作項目： Master負責的工作項目與 Replica 雷同，但 Master 具備處理 Write 相關的 Query 操作。並且每個 Shard 都須包含一個以上的 Master 才能正常的提供服務 Replica負責同步當前的資料與提供 Read 的 Query 操作，在 Master 發生故障時可以透過指定方式晉升為新的 Master 讓服務可以繼續正常運作 RdOnly負責同步當前的資料與提供 Read 的 Query 操作，但是無法像 Replica 能晉升為新的 Master。除此之外能提供一些背景的處理，如資料備份, 資料同步至其他儲存系統, Resharding…等等 Backup負責將當前資料進行快照，並將其 MySQL 行程停止，並將資料的快照儲存至後端儲存系統中，如 NFS, Ceph…等等。完成備份後將其角色切換為原本的 Replica 或 RdOnly 並且啟動 MySQL 行程繼續提供服務 Restore當一個 Tablet 啟動時不包含資料，則會在最新的備份中進行資料復原，當其完成資料復原後，則會更改為 Replica 或 RdOnly 角色 Topology ServiceTopology Service 是 Vitess 用來儲存當前拓樸資料的分散式儲存系統，當前可支援的有 etcd, zookeeper, k8s…等等，預設採用 etcd Topology Service 的存在目的如下： 提供叢集間的 tablet 可以彼此相互通訊與協調 讓 Vitess 知道 tablet 的狀態以提供轉發 Query 使用 儲存 Vitess 相關的配置 每個 Vitess 叢集都會包含一個 Global Topology 與每個 Cell 包含一個 Local Topology Global TopologyGlobal Topology 負責儲存整個 Vitess 不會被頻繁被更動的資料，像是 Keyspaces, shards 與每個 Shard 的 Master tablet 資訊等 Global Topology 通常運用情境不多，但如 Reparenting 或 Resharding 才會被使用 為了防止單一 Cell 發生故障的情形下，Global Topology 應該被儲存在不同的 Cell 的節點中，以防止單一個 Cell 發生故障而導致影響整個 Vitess 的運作 Local Topology每個 Cell 通常會儲存自己的 Local Topology，像是 Cell 本身的 Tablets, Keyspace Graph, Replication Graph..等等 vtctlvtctl 是一個 command-line 的 Vitess 叢集管理工具，提供了識別 Master 與 Replicas 資料庫的功能, 建立 Tablet, 故障處理或 Resharding 等等 vtctldvtctld 是一個 HTTP Server，並且提供查看 Topology 服務的資訊，在故障排除或想了解當前叢集狀態都可提供有幫助的訊息 VTGateVTGate 是一個輕量的 Proxy 服務，負責將使用者請求轉發至正確的 VTTablet 進行處理，並將結果回傳給使用者。VTGate 同時提供 MySQL 與 gRPC 兩種 Protocol，應用程式可以透過連結 VTGate 來發送 Query 取得資訊，就如同對 MySQL 發送 Query 得到相同的體驗 參考 Vitess Offical Document 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2020/12/13/Vitess-介紹/"},{"title":"從 Source Codes 開始打造 HA Kubernetes Cluster","text":"在網路上可以找到大量使用 kubeadm 等類似工具建置 Kubernetes Cluster 的教學，雖然使用這類工具可以快速建立一個 Kubernetes Cluster ，甚至連 HA 也考量進去了。但以手動方式建置好處莫過於可以了解每個元件之間的關係與建置過程可能會遇到的問題以及從中學習排除問題的經驗。因此筆者就開始了這條不歸路….本篇主要紀錄建置一個 HA Kubernetes Cluster 的過程，從 Github 下載 Source Code 開始到建置完成。 整體架構如下圖所示： 環境與軟體資訊版本資訊 Kubernetes: v1.19.7 Calico: v3.8.2 Etcd: v3.3.8 Docker: 20.10.2 網路資訊 Cluster IP CIDR: 10.244.0.0/16 Service Cluster IP CIDR: 10.96.0.0/12 Service DNS IP: 10.96.0.10 DNS DN: cluster.local Kubernetes API VIP: 192.168.20.20 節點資訊 IP Address Hostname CPU Memory 192.168.20.10 k8s-m1 1 2GB 192.168.20.11 k8s-m2 1 2GB 192.168.20.12 k8s-m3 1 2GB 192.168.20.13 k8s-n1 1 2GB 192.168.20.14 k8s-n2 1 2GB 環境是在作者的工作主機上採用 VM 方式運行，因為硬體支援上的限制，因此將所有節點配置為 1 core, 2GB memory。實際運行建議針對自行需求與官方建議最低硬體規格進行更改。 前置作業配置節點間傳輸採用 Key 方式在安裝 Kubernetes Cluster 前需讓所有節點之間的驗證皆採用 key 方式，主要目的是方便在部署過程中，可以不必輸入密碼即可將配置檔案在節點之間傳輸，以加快與方便部署。因此在 k8s-m1, k8s-m2, k8s-m3, k8s-n1 與 k8s-n2 皆建立一對 public key 與 private key。 1$ ssh-keygen 資訊可以直接按 enter 忽略到底 到 k8s-m1 機器並取得 public key 12$ cat ~/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC8wjXDlJD0MT1Y/4UAzs4MHCtOvMHDKjsECMgP3u4iemZrntpz247Qn0jZlhM1LBTIWZV6B3vPkfZ52uY0AW3we2prl4z2/E48AQi09D/RnU5h974EtXAaB0C6XfZwLjfzJmG+8iEVOPjcdUHK5ay4XOaJpncazvyt/B75pfeYIi6P8MxLropiDKLghcenmmQtXGDnNQ/wB/SfeOdcEoU4SiM8U65ZdHf8TaVhtODyat6jBktyFkrbgWLZPLEaz/eEe9b7+ybIpEqSV8TGLFGHi0xORGrpoD+3VnSfWRKu/emX1q9CDpRlFK+sCmftMasw1sOUBVLBbcD77pWPNA73HjwuVax/4JXMXwSdkuupZ3bB6nDn+xnuuMaNRFz/JLERCZx+RXup3Nvz9bGgtzTV4BAG806Q1tNW7ogm6GzCjd6pBBp58lEPHpmzhogRl1RdEHsIJjj96qfm6PQEmVuQWhrU9a8B7Fyh+VjifzaIJaDDAx0gs7d3gP/qRo2bsEU= root@k8s-m1 重複上述步驟，取得 k8s-m1, k8s-m2, k8s-m3, k8s-n1 與 k8s-n2 上所有的 public key 並寫入到 ~/.ssh/authorized_keys 檔案內，寫入完成後記得儲存並退出。 1234567$ vim ~/.ssh/authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC8wjXDlJD0MT1Y/4UAzs4MHCtOvMHDKjsECMgP3u4iemZrntpz247Qn0jZlhM1LBTIWZV6B3vPkfZ52uY0AW3we2prl4z2/E48AQi09D/RnU5h974EtXAaB0C6XfZwLjfzJmG+8iEVOPjcdUHK5ay4XOaJpncazvyt/B75pfeYIi6P8MxLropiDKLghcenmmQtXGDnNQ/wB/SfeOdcEoU4SiM8U65ZdHf8TaVhtODyat6jBktyFkrbgWLZPLEaz/eEe9b7+ybIpEqSV8TGLFGHi0xORGrpoD+3VnSfWRKu/emX1q9CDpRlFK+sCmftMasw1sOUBVLBbcD77pWPNA73HjwuVax/4JXMXwSdkuupZ3bB6nDn+xnuuMaNRFz/JLERCZx+RXup3Nvz9bGgtzTV4BAG806Q1tNW7ogm6GzCjd6pBBp58lEPHpmzhogRl1RdEHsIJjj96qfm6PQEmVuQWhrU9a8B7Fyh+VjifzaIJaDDAx0gs7d3gP/qRo2bsEU= root@k8s-m1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDBEK6W4AbGoTJf3s78Vud22I3PJ7DowCUi94e0wTrzHc4OQ125ve+lVXHd7rb+tXNwe8taW9YFK1RTxsJo7zIyBPeLhpaGJEYh6TdG2C6lDurv+L2UnZx6pH4eqDJetDpHFfeHd+6Ih9c+oEmIYdBIYJGSsrGRmOq7HCbVgMgs49Is/bOZ2gpnGHvUzUCniq2WVjp5Ur4bSO+rF3UkC7c8dWQqN65ltRR1h6S/t5ZgLcB9+ipuxwxehosGTCsHY2tt9x24juYAqCzhFTjT+QIlvVsecFXyOS2kKlksruDCfCW4FEs+fgZF0AvktHvOSemw+R6POaBfi6RQcFgcQMFD6Eou1O2T5D6onRMprBHCPK2WEA/OjaKDKIsIe5SM/mkMf8pAM8GkxJbfnSj8zjnq0VvGfRivMeCMt4S084WNXMit3z4c2usa5ijXnCFSjtkqzwKXlFpIB+rGEBdOPPaPAOoHqk8bjHTVVjXRyVvQ0JxXiqMn10iYGBM722rYQj8= root@k8s-m2ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDJYVOpjomcDnjinpcEilH45X5ybZixLGy31cRdIR/bmJ0KJhikfnhGrF1mVlIJCDyJrEVMf3HZaG/EY14JJrW+XKcgnXtfcV/da52phY4aJtPWuSfaGezA3fr4DywE6R4JuHt0QfqEZjW8PeGMU5aXYEndxHiR/Ztk7dDw0qYR02T5CRbdYZmy8JHf9ERjwh904dan9mqkkgYNHZVu9ZbFFl9659U4wRWPrbYqcn5gVwxCpb+Pfan0RDujMwxooCVBJY84hnw7H6THqHLEyvQeZ/55n9CXGJoDwfUudzbWO7hkcHF0UdTAUA56yxga37s62qgeTrfBMN0laaq2I9rDX2AeMaAcrxXz+KGjEfEPNjhmqTqGCX9WOSLzz6FzqgW7f8Gstv8RL3zJ6FPjEosvozI9A1/D1II1B63saJULgRg22zz1PoEmXJqNE9HpT9uU3HpvyU4Urq08k8tpXc/wQV/XFWCQbfWlDsXLHtq0NttdKYgDFmorH4dzuDfavJ8= root@k8s-m3ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9ZNGaJ7mYgGTk8cSr+DdRN4l8PAi6DtnHFdSKEnClqq6gb2F2ubFezc9BCVo1qZygYEve1eZ+FkSDn3N/U5k1fD9sR4eFXtxSdzYkCVaWO4g/SGDXJCikqjI9UMvjbk0AypYmKtnjupsaPMmwV0j0seIHHjhF5sXZAYHXsXwRyglUWMHgWfhlLDNvTpWCRuOj+AKu6/afMReJv+AOlvMjoHewIGuq2TT3A/DXv+L3iivETx2PiCRKmLtrcgVhT+atAGdqqra8WHzmiW7vKssPMWtwXGad6CC18mO1Ym69VfTSaXEjLpHY1e7/keKx+dakBjHds9G3Nv1qiXuQrqMU99AQlpCFV1UakUt7qkwf3pAzlTv6/Y7WhP00Gcw8KCDM4kyNfJn9loJaHeYv5w8YsN7lBqUtvTRK0dHF22dvR7G3Itsd3taSWMhu9w4KhUFvz45l3958cza22Y/luj6IFOZ+UaWSkywj9Do68XfnowONQY+Ctt2Y2D4IZZiiT2M= root@k8s-n1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCqKk7tZOwjcXae7xW5hr3gYpcEof9+C4QULEIEgbQWra/RUGKOp0Hvj0/JoBZmGwXvKSiRdQ1DizS0Y1vrCUpi1aYwIwjTs7cyJQ3Q5t763yFMA3CfeM7lfs8AXhRmimWjkUAyXcOeJ+I7h8yxEaN7EBP/ylv5dw/lD/qr38v3fojYlPCbcqwC8y8R0zzla6QVVBXy5ImxhONV53erAcp1o7z0IKIt+6MECb7SebWBqlfcfO7jMM60bOSFKogSgH3Rb/tIeXEf0FY4n4lplptJxohhA2/1GhM6E48soaY+SL5J4FmLlhxO3bs0RwKmvusUbt8qTMwn78dZXVfnP8dNGnf6O5I7vC+cOI/JvmKngjI5i3xxUSvackBv80HyytfedhFeV+effjjJvKYwHvfis8+lg3iGXP4gYfOSffWl4IoBo+G0XO/20+z06/A4/ep6O2qe+SeOfdz+b2gzC4UxJWFfI3MbHljn5QKuCRst93wKzy19bsfuhr+1+Q1hdZU= root@k8s-n2 切記讀者勿直接使用上面例子的 public key 直接寫到 ~/.ssh/authorized_keys 目錄，需自行透過前一步驟產生後，取得您機器的 public key 再行寫入。 重複上述步驟，將其分別複製到 k8s-m2, k8s-m3, k8s-n1 與 k8s-n2 的 ~/.ssh/authorized_keys 檔案內。 完成後，將所有節點的 IP Address 與 Hostname 註冊到 k8s-m1, k8s-m2, k8s-m3, k8s-n1 與 k8s-n2 節點。 12345678$ vim /etc/hosts...192.168.20.10 k8s-m1192.168.20.11 k8s-m2192.168.20.12 k8s-m3192.168.20.13 k8s-n1192.168.20.14 k8s-n2 針對讀者自行分配至 IP Address 與 Hostname 寫入到所有節點的 /etc/hosts 檔案內最下方。 完成上述所有步驟後，確認所有節點彼此間登入皆不需要密碼即可登入。 1234$ ssh k8s-m2The authenticity of host &apos;k8s-m2 (192.168.20.11)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:FtCGiDnzzrbYlo8+1w8nPocBOm1fX92/5YUbRItfN60.Are you sure you want to continue connecting (yes/no/[fingerprint])? 第一次登入會有如上的訊息，輸入 yes 即可。 安裝 Docker CE在安裝 Kubernetes 前，需要選擇一種 CRI 安裝在 k8s-m1, k8s-m2, k8s-m3, k8s-n1 與 k8s-n2 以提供容器運行，常見如：Docker, Containerd, CRI-O 等等。本篇採用 Docker 作為示範。 更新與安裝相依套件。 1234567$ apt update$ apt install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common 加入 Docker 官方 GPG Key。 1$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 設定安裝 stable 版本。 1234$ sudo add-apt-repository \\ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable&quot; 更新並安裝 Docker CE。 12$ apt update$ apt install -y docker-ce docker-ce-cli containerd.io 安裝完畢後，確認 Docker 環境。 123456789101112131415161718192021222324252627282930$ docker versionClient: Docker Engine - Community Version: 20.10.3 API version: 1.41 Go version: go1.13.15 Git commit: 48d30b5 Built: Fri Jan 29 14:33:21 2021 OS/Arch: linux/amd64 Context: default Experimental: trueServer: Docker Engine - Community Engine: Version: 20.10.3 API version: 1.41 (minimum version 1.12) Go version: go1.13.15 Git commit: 46229ca Built: Fri Jan 29 14:31:32 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.3 GitCommit: 269548fa27e0089a8b8278fc4fc781d7f65a939b runc: Version: 1.0.0-rc92 GitCommit: ff819c7e9184c13b7c2607fe6c30ae19403a7aff docker-init: Version: 0.19.0 GitCommit: de40ad0 編譯 Kubernetes Source Codes(僅在 k8s-m1 操作)安裝相關套件在編譯 Kubernetes 時，需具備 make 指令，與因為部分 source codes 為 C 語言，因此需要安裝 gcc 套件。 12$ apt update$ apt install -y make gcc 安裝 Golang因為本篇介紹內容包含如何從 Source Codes 編譯出執行檔，因此需要具備 Golang 環境才能進行編譯。我們選擇使用 Golang 1.15.7 版本，可以依照需求自行調整。 下載 Golang 1.15.7 1$ wget https://golang.org/dl/go1.15.7.linux-amd64.tar.gz 下載完成後，對 go1.15.7.linux-amd64.tar.gz 檔案進行解壓縮，並儲存至 /usr/local/ 目錄。 1$ tar -C /usr/local -xzf go1.15.7.linux-amd64.tar.gz 將 /usr/local/go/bin 目錄加至 PATH 環境變數中。 1$ export PATH=$PATH:/usr/local/go/bin 確認當前 Golang 環境是否可以正常使用。 1$ go version 下載與編譯 Kubernetes Source Codes從 Kubernetes Github 選擇想要安裝的版本，本篇以 v1.19.7 為例，下載對應的 .tar.gz 檔案。 1$ wget https://github.com/kubernetes/kubernetes/archive/v1.19.7.tar.gz 下載完成後，對 v1.19.7.tar.gz 檔案進行解壓縮。 1$ tar -xvf v1.19.7.tar.gz 解壓縮完成後，可以在當前目錄看到 kubernetes-1.19.7 目錄。進入 kubernetes-1.19.7 目錄，進行執行檔編譯。 123$ cd kubernetes-1.19.7$ make$ cd .. 編譯過程需要一些時間 … 完成編譯後，可以在 kubernetes-1.19.7 目錄裡面看到多出一個 _output 目錄，接著我們進入到 kubernetes-1.19.7/_output/bin/ 目錄底下可以看到所有被編譯出來的執行檔。 123$ ls kubernetes-1.19.7/_output/binapiextensions-apiserver deepcopy-gen e2e_node.test gendocs genman genyaml go2make go-runner kube-aggregator kube-controller-manager kubelet kube-proxy linkcheck openapi-genconversion-gen defaulter-gen e2e.test genkubedocs genswaggertypedocs ginkgo go-bindata kubeadm kube-apiserver kubectl kubemark kube-scheduler mounter prerelease-lifecycle-gen 將執行檔複製到對應的節點。 1234567891011121314151617$ cp kubernetes-1.19.7/_output/bin/kubectl /usr/local/bin$ cp kubernetes-1.19.7/_output/bin/kube-apiserver /usr/local/bin$ cp kubernetes-1.19.7/_output/bin/kube-scheduler /usr/local/bin$ cp kubernetes-1.19.7/_output/bin/kube-controller-manager /usr/local/bin$ cp kubernetes-1.19.7/_output/bin/kubelet /usr/local/bin$ scp kubernetes-1.19.7/_output/bin/kubectl k8s-m2:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kube-apiserver k8s-m2:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kube-scheduler k8s-m2:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kube-controller-manager k8s-m2:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kubelet k8s-m2:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kubectl k8s-m3:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kube-apiserver k8s-m3:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kube-scheduler k8s-m3:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kube-controller-manager k8s-m3:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kubelet k8s-m3:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kubelet k8s-n1:/usr/local/bin/$ scp kubernetes-1.19.7/_output/bin/kubelet k8s-n2:/usr/local/bin/ 建立 buildrc 環境變數檔，方便後續安裝使用。 12345678910$ vim buildrcCFSSL_URL=https://pkg.cfssl.org/R1.2DIR=/etc/etcd/sslK8S_DIR=/etc/kubernetesPKI_DIR=${K8S_DIR}/pkiKUBE_APISERVER=https://192.168.20.20:6443TOKEN_ID=4a378cTOKEN_SECRET=0c0751149cb12dd1BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET} TOKEN_ID 可以使用 openssl rand -hex 3 指令產生。 TOKEN_SECRET 可以使用 openssl rand -hex 8 指令產生。 建立完成後，將環境變數檔寫入環境變數中。 1$ source buildrc 建立 CA 與產生 TLS 憑證(僅在 k8s-m1 操作)安裝 cfssl 工具，這將會用來建立 CA ，並產生 TLS 憑證。 123$ wget ${CFSSL_URL}/cfssl_linux-amd64 -O /usr/local/bin/cfssl$ wget ${CFSSL_URL}/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson Etcd建立 etcd-ca-csr.json 檔案並寫入如下內容。 123456789101112131415161718$ vim etcd-ca-csr.json{ &quot;CN&quot;: &quot;etcd&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;Etcd Security&quot; } ]} 建立 /etc/etcd/ssl 目錄並產生 Etcd CA。 123456789$ mkdir -p ${DIR}$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare ${DIR}/etcd-ca2021/02/03 06:57:49 [INFO] generating a new CA key and certificate from CSR2021/02/03 06:57:49 [INFO] generate received request2021/02/03 06:57:49 [INFO] received CSR2021/02/03 06:57:49 [INFO] generating key: rsa-20482021/02/03 06:57:50 [INFO] encoded CSR2021/02/03 06:57:50 [INFO] signed certificate with serial number 32327654678613319205867135393400903527408111164 產生 CA 完成後，建立 ca-config.json 與 etcd-csr.json 檔案。 123456789101112131415161718192021222324252627282930313233343536373839$ vim ca-config.json{ &quot;signing&quot;: { &quot;default&quot;: { &quot;expiry&quot;: &quot;87600h&quot; }, &quot;profiles&quot;: { &quot;kubernetes&quot;: { &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; } } }}$ vim etcd-csr.json{ &quot;CN&quot;: &quot;etcd&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;Etcd Security&quot; } ]} 產生 Etcd 憑證。 12345678910111213$ cfssl gencert \\ -ca=${DIR}/etcd-ca.pem \\ -ca-key=${DIR}/etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,192.168.20.10,192.168.20.11,192.168.20.12,192.168.20.20 \\ -profile=kubernetes \\ etcd-csr.json | cfssljson -bare ${DIR}/etcd 2021/02/03 07:04:11 [INFO] generate received request2021/02/03 07:04:11 [INFO] received CSR2021/02/03 07:04:11 [INFO] generating key: rsa-20482021/02/03 07:04:11 [INFO] encoded CSR2021/02/03 07:04:11 [INFO] signed certificate with serial number 66652898153558259936883336779780180424154374600 刪除不必要的檔案，並檢查 /etc/etcd/ssl 目錄。 123$ rm -rf ${DIR}/*.csr$ ls /etc/etcd/ssletcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem 在 k8s-m2 與 k8s-m3 建立 /etc/etcd/ssl 目錄。 1$ mkdir -p /etc/etcd/ssl 完成建立後，將 k8s-m1 上建立的 etcd 憑證複製到 k8s-m2 與 k8s-m3。 12345678910111213$ scp /etc/etcd/ssl/* root@k8s-m2:/etc/etcd/ssl/etcd-ca-key.pem 100% 1675 1.3MB/s 00:00etcd-ca.pem 100% 1359 2.5MB/s 00:00etcd-key.pem 100% 1675 3.0MB/s 00:00etcd.pem 100% 1444 2.2MB/s 00:00$ scp /etc/etcd/ssl/* root@k8s-m3:/etc/etcd/ssl/etcd-ca-key.pem 100% 1675 1.3MB/s 00:00etcd-ca.pem 100% 1359 2.5MB/s 00:00etcd-key.pem 100% 1675 3.0MB/s 00:00etcd.pem 100% 1444 2.2MB/s 00:00 K8s 元件建立 ca-csr.json 檔案並寫入如下內容。 123456789101112131415161718$ vim ca-csr.json{ &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;Kubernetes&quot;, &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 建立 /etc/kubernetes/pki 目錄並產生 CA 12345678910111213$ mkdir -p ${PKI_DIR}$ cfssl gencert -initca ca-csr.json | cfssljson -bare ${PKI_DIR}/ca2021/02/03 07:16:02 [INFO] generating a new CA key and certificate from CSR2021/02/03 07:16:02 [INFO] generate received request2021/02/03 07:16:02 [INFO] received CSR2021/02/03 07:16:02 [INFO] generating key: rsa-20482021/02/03 07:16:02 [INFO] encoded CSR2021/02/03 07:16:02 [INFO] signed certificate with serial number 528600369434105202111308119706767425710162291488$ ls ${PKI_DIR}/ca*.pem/etc/kubernetes/pki/ca-key.pem /etc/kubernetes/pki/ca.pem KUBE_APISERVER 為 Kubernetes API VIP API Server憑證主要是因為 Kubelet 與 API Server 之間構通採用 TLS 方式，需驗證並符合才能進行存取。 建立 apiserver-csr.json 檔案並寫入如下內容。 123456789101112131415161718$ vim apiserver-csr.json{ &quot;CN&quot;: &quot;kube-apiserver&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;Kubernetes&quot;, &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 產生 CA。 1234567891011121314151617$ cfssl gencert \\ -ca=${PKI_DIR}/ca.pem \\ -ca-key=${PKI_DIR}/ca-key.pem \\ -config=ca-config.json \\ -hostname=10.96.0.1,192.168.20.20,127.0.0.1,kubernetes.default \\ -profile=kubernetes \\ apiserver-csr.json | cfssljson -bare ${PKI_DIR}/apiserver 2021/02/03 07:20:17 [INFO] generate received request2021/02/03 07:20:17 [INFO] received CSR2021/02/03 07:20:17 [INFO] generating key: rsa-20482021/02/03 07:20:17 [INFO] encoded CSR2021/02/03 07:20:17 [INFO] signed certificate with serial number 85874010554668635601954999407030722935833248735$ ls ${PKI_DIR}/apiserver*.pem/etc/kubernetes/pki/apiserver-key.pem /etc/kubernetes/pki/apiserver.pem Front Proxy Client憑證使用於 Authenticating Proxy。 建立 front-proxy-ca-csr.json 與 front-proxy-client-csr.json 檔案並寫入如下內容。 12345678910111213141516171819$ vim front-proxy-ca-csr.json{ &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }}$ vim front-proxy-client-csr.json{ &quot;CN&quot;: &quot;front-proxy-client&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }} 產生 CA。 123456789101112$ cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare ${PKI_DIR}/front-proxy-ca2021/02/03 07:37:06 [INFO] generating a new CA key and certificate from CSR2021/02/03 07:37:06 [INFO] generate received request2021/02/03 07:37:06 [INFO] received CSR2021/02/03 07:37:06 [INFO] generating key: rsa-20482021/02/03 07:37:06 [INFO] encoded CSR2021/02/03 07:37:06 [INFO] signed certificate with serial number 481641924005786643792808437563979167221771356108$ ls ${PKI_DIR}/front-proxy-ca*.pem/etc/kubernetes/pki/front-proxy-ca-key.pem /etc/kubernetes/pki/front-proxy-ca.pem 產生 Front proxy client 憑證 1234567891011121314151617181920$ cfssl gencert \\ -ca=${PKI_DIR}/front-proxy-ca.pem \\ -ca-key=${PKI_DIR}/front-proxy-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ front-proxy-client-csr.json | cfssljson -bare ${PKI_DIR}/front-proxy-client 2021/02/03 07:39:24 [INFO] generate received request2021/02/03 07:39:24 [INFO] received CSR2021/02/03 07:39:24 [INFO] generating key: rsa-20482021/02/03 07:39:24 [INFO] encoded CSR2021/02/03 07:39:24 [INFO] signed certificate with serial number 19050615149406975058808151126741178569714446762021/02/03 07:39:24 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).$ ls ${PKI_DIR}/front-proxy-client*.pem/etc/kubernetes/pki/front-proxy-client-key.pem /etc/kubernetes/pki/front-proxy-client.pem Controller Manager憑證主要讓 Controller Manager 能與 API Server 進行溝通。 建立 manager-csr.json 檔案並寫入如下內容。 123456789101112131415161718vim manager-csr.json{ &quot;CN&quot;: &quot;system:kube-controller-manager&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;system:kube-controller-manager&quot;, &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 產生 Controller Manager 憑證。 1234567891011121314151617181920$ cfssl gencert \\ -ca=${PKI_DIR}/ca.pem \\ -ca-key=${PKI_DIR}/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ manager-csr.json | cfssljson -bare ${PKI_DIR}/controller-manager 2021/02/03 07:43:42 [INFO] generate received request2021/02/03 07:43:42 [INFO] received CSR2021/02/03 07:43:42 [INFO] generating key: rsa-20482021/02/03 07:43:42 [INFO] encoded CSR2021/02/03 07:43:42 [INFO] signed certificate with serial number 4182762609046132934497448115102350269783287990572021/02/03 07:43:42 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).$ ls ${PKI_DIR}/controller-manager*.pem/etc/kubernetes/pki/controller-manager-key.pem /etc/kubernetes/pki/controller-manager.pem 接著利用 kubectl 產生 Controller 的 kubeconfig 檔案。 123456789101112131415161718192021222324252627$ kubectl config set-cluster kubernetes \\ --certificate-authority=${PKI_DIR}/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=${K8S_DIR}/controller-manager.confCluster &quot;kubernetes&quot; set.$ kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=${PKI_DIR}/controller-manager.pem \\ --client-key=${PKI_DIR}/controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=${K8S_DIR}/controller-manager.conf User &quot;system:kube-controller-manager&quot; set.$ kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=${K8S_DIR}/controller-manager.confContext &quot;system:kube-controller-manager@kubernetes&quot; created.$ kubectl config use-context system:kube-controller-manager@kubernetes \\ --kubeconfig=${K8S_DIR}/controller-manager.conf Switched to context &quot;system:kube-controller-manager@kubernetes&quot;. Scheduler憑證會建立 system:kube-scheduler 使用者，並且綁定 RBAC Cluster Role 中的 system:kube-scheduler 讓 Scheduler 元件可以與 API Server 溝通。 建立 manager-csr.json 檔案並寫入如下內容。 123456789101112131415161718$ vim scheduler-csr.json{ &quot;CN&quot;: &quot;system:kube-scheduler&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;system:kube-scheduler&quot;, &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 產生 Scheduler 憑證。 1234567891011121314151617181920$ cfssl gencert \\ -ca=${PKI_DIR}/ca.pem \\ -ca-key=${PKI_DIR}/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ scheduler-csr.json | cfssljson -bare ${PKI_DIR}/scheduler 2021/02/04 02:24:52 [INFO] generate received request2021/02/04 02:24:52 [INFO] received CSR2021/02/04 02:24:52 [INFO] generating key: rsa-20482021/02/04 02:24:53 [INFO] encoded CSR2021/02/04 02:24:53 [INFO] signed certificate with serial number 4661369198047471155641448411972006192897487183642021/02/04 02:24:53 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).$ ls ${PKI_DIR}/scheduler*.pem/etc/kubernetes/pki/scheduler-key.pem /etc/kubernetes/pki/scheduler.pem 接著利用 kubectl 產生 Scheduler 的 kubeconfig 檔案。 123456789101112131415161718192021222324252627$ kubectl config set-cluster kubernetes \\ --certificate-authority=${PKI_DIR}/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=${K8S_DIR}/scheduler.confCluster &quot;kubernetes&quot; set.$ kubectl config set-credentials system:kube-scheduler \\ --client-certificate=${PKI_DIR}/scheduler.pem \\ --client-key=${PKI_DIR}/scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=${K8S_DIR}/scheduler.conf User &quot;system:kube-scheduler&quot; set.$ kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=${K8S_DIR}/scheduler.conf Context &quot;system:kube-scheduler@kubernetes&quot; created.$ kubectl config use-context system:kube-scheduler@kubernetes \\ --kubeconfig=${K8S_DIR}/scheduler.conf Switched to context &quot;system:kube-scheduler@kubernetes&quot;. AdminAdmin 被用來綁定 RBAC Cluster Role 中的 cluster-admin，若使用者想要針對 Kubernetes Cluster 進行任何操作，都需要透過 kubeconfig。 建立 admin-csr.json 檔案並寫入如下內容。 123456789101112131415161718$ vim admin-csr.json{ &quot;CN&quot;: &quot;admin&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 產生 Kubernetes Admin 憑證。 1234567891011121314151617181920$ cfssl gencert \\ -ca=${PKI_DIR}/ca.pem \\ -ca-key=${PKI_DIR}/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare ${PKI_DIR}/admin 2021/02/04 02:31:44 [INFO] generate received request2021/02/04 02:31:44 [INFO] received CSR2021/02/04 02:31:44 [INFO] generating key: rsa-20482021/02/04 02:31:45 [INFO] encoded CSR2021/02/04 02:31:45 [INFO] signed certificate with serial number 7162775264316198471526581525326006531418420934392021/02/04 02:31:45 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).$ ls ${PKI_DIR}/admin*.pem/etc/kubernetes/pki/admin-key.pem /etc/kubernetes/pki/admin.pem 接著利用 kubectl 產生 Admin 的 kubeconfig 檔案。 123456789101112131415161718192021222324252627$ kubectl config set-cluster kubernetes \\ --certificate-authority=${PKI_DIR}/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=${K8S_DIR}/admin.conf Cluster &quot;kubernetes&quot; set.$ kubectl config set-credentials kubernetes-admin \\ --client-certificate=${PKI_DIR}/admin.pem \\ --client-key=${PKI_DIR}/admin-key.pem \\ --embed-certs=true \\ --kubeconfig=${K8S_DIR}/admin.conf User &quot;kubernetes-admin&quot; set.$ kubectl config set-context kubernetes-admin@kubernetes \\ --cluster=kubernetes \\ --user=kubernetes-admin \\ --kubeconfig=${K8S_DIR}/admin.confContext &quot;kubernetes-admin@kubernetes&quot; created.$ kubectl config use-context kubernetes-admin@kubernetes \\ --kubeconfig=${K8S_DIR}/admin.conf Switched to context &quot;kubernetes-admin@kubernetes&quot;. Masters Kubelet採用 Node Authorization 來讓 Master 能存取 API Server。 建立 kubelet-k8s-m1-csr.json 檔案並寫入如下內容。 123456789101112131415161718$ vim kubelet-k8s-m1-csr.json{ &quot;CN&quot;: &quot;system:node:k8s-m1&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;system:nodes&quot;, &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 產生 Kubelet k8s-m1 憑證。 12345678910111213$ cfssl gencert \\ -ca=${PKI_DIR}/ca.pem \\ -ca-key=${PKI_DIR}/ca-key.pem \\ -config=ca-config.json \\ -hostname=k8s-m1 \\ -profile=kubernetes \\ kubelet-k8s-m1-csr.json | cfssljson -bare ${PKI_DIR}/kubelet-k8s-m1; 2021/02/04 05:43:14 [INFO] generate received request2021/02/04 05:43:14 [INFO] received CSR2021/02/04 05:43:14 [INFO] generating key: rsa-20482021/02/04 05:43:14 [INFO] encoded CSR2021/02/04 05:43:14 [INFO] signed certificate with serial number 88551402085838487825161450787890905954599729836 建立 kubelet-k8s-m2-csr.json 檔案並寫入如下內容。 123456789101112131415161718$ vim kubelet-k8s-m2-csr.json{ &quot;CN&quot;: &quot;system:node:k8s-m2&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;system:nodes&quot;, &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 產生 Kubelet k8s-m2 憑證。 12345678910111213$ cfssl gencert \\ -ca=${PKI_DIR}/ca.pem \\ -ca-key=${PKI_DIR}/ca-key.pem \\ -config=ca-config.json \\ -hostname=k8s-m2 \\ -profile=kubernetes \\ kubelet-k8s-m2-csr.json | cfssljson -bare ${PKI_DIR}/kubelet-k8s-m2; 2021/02/04 05:46:01 [INFO] generate received request2021/02/04 05:46:01 [INFO] received CSR2021/02/04 05:46:01 [INFO] generating key: rsa-20482021/02/04 05:46:01 [INFO] encoded CSR2021/02/04 05:46:01 [INFO] signed certificate with serial number 631793009516906574893487594475892882904568266495 建立 kubelet-k8s-m3-csr.json 檔案並寫入如下內容。 123456789101112131415161718$ vim kubelet-k8s-m3-csr.json{ &quot;CN&quot;: &quot;system:node:k8s-m3&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;TW&quot;, &quot;L&quot;: &quot;Taipei&quot;, &quot;ST&quot;: &quot;Taipei&quot;, &quot;O&quot;: &quot;system:nodes&quot;, &quot;OU&quot;: &quot;Kubernetes-manual&quot; } ]} 產生 Kubelet k8s-m3 憑證。 12345678910111213$ cfssl gencert \\ -ca=${PKI_DIR}/ca.pem \\ -ca-key=${PKI_DIR}/ca-key.pem \\ -config=ca-config.json \\ -hostname=k8s-m3 \\ -profile=kubernetes \\ kubelet-k8s-m3-csr.json | cfssljson -bare ${PKI_DIR}/kubelet-k8s-m3; 2021/02/04 05:46:43 [INFO] generate received request2021/02/04 05:46:43 [INFO] received CSR2021/02/04 05:46:43 [INFO] generating key: rsa-20482021/02/04 05:46:43 [INFO] encoded CSR2021/02/04 05:46:43 [INFO] signed certificate with serial number 313125539741502293210284542097458112530534317871 刪除不必要的檔案。 123$ rm -rf ${PKI_DIR}/kubelet-k8s-m1-csr.json$ rm -rf ${PKI_DIR}/kubelet-k8s-m2-csr.json$ rm -rf ${PKI_DIR}/kubelet-k8s-m3-csr.json 檢視已建立的憑證。 123$ ls ${PKI_DIR}/kubelet*.pem/etc/kubernetes/pki/kubelet-k8s-m1-key.pem /etc/kubernetes/pki/kubelet-k8s-m1.pem /etc/kubernetes/pki/kubelet-k8s-m2-key.pem /etc/kubernetes/pki/kubelet-k8s-m2.pem /etc/kubernetes/pki/kubelet-k8s-m3-key.pem /etc/kubernetes/pki/kubelet-k8s-m3.pem 在 k8s-m1 上更改憑證名稱。 123$ cp ${PKI_DIR}/kubelet-k8s-m1-key.pem ${PKI_DIR}/kubelet-key.pem$ cp ${PKI_DIR}/kubelet-k8s-m1.pem ${PKI_DIR}/kubelet.pem$ rm ${PKI_DIR}/kubelet-k8s-m1-key.pem ${PKI_DIR}/kubelet-k8s-m1.pem 在 k8s-m2 建立 /etc/kubernetes/pki 目錄。 1$ mkdir -p /etc/kubernetes/pki 從 k8s-m1 上將憑證複製到 k8s-m2。 123$ scp ${PKI_DIR}/ca.pem k8s-m2:${PKI_DIR}/ca.pem$ scp ${PKI_DIR}/kubelet-k8s-m2-key.pem k8s-m2:${PKI_DIR}/kubelet-key.pem$ scp ${PKI_DIR}/kubelet-k8s-m2.pem k8s-m2:${PKI_DIR}/kubelet.pem 在 k8s-m3 建立 /etc/kubernetes/pki 目錄。 1$ mkdir -p /etc/kubernetes/pki 從 k8s-m1 上將憑證複製到 k8s-m3。 123$ scp ${PKI_DIR}/ca.pem k8s-m3:${PKI_DIR}/ca.pem$ scp ${PKI_DIR}/kubelet-k8s-m3-key.pem k8s-m3:${PKI_DIR}/kubelet-key.pem$ scp ${PKI_DIR}/kubelet-k8s-m3.pem k8s-m3:${PKI_DIR}/kubelet.pem 在 k8s-m1 刪除不必要的檔案。 12$ rm ${PKI_DIR}/kubelet-k8s-m2-key.pem ${PKI_DIR}/kubelet-k8s-m2.pem$ rm ${PKI_DIR}/kubelet-k8s-m3-key.pem ${PKI_DIR}/kubelet-k8s-m3.pem 接著在 k8s-m1 利用 kubectl 產生 Kubelet 的 kubeconfig 檔案。 123456789101112131415161718192021222324252627$ kubectl config set-cluster kubernetes \\ --certificate-authority=${PKI_DIR}/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=${K8S_DIR}/kubelet.conf Cluster &quot;kubernetes&quot; set.$ kubectl config set-credentials system:node:k8s-m1 \\ --client-certificate=${PKI_DIR}/kubelet.pem \\ --client-key=${PKI_DIR}/kubelet-key.pem \\ --embed-certs=true \\ --kubeconfig=${K8S_DIR}/kubelet.conf User &quot;system:node:k8s-m1&quot; set.$ kubectl config set-context system:node:k8s-m1@kubernetes \\ --cluster=kubernetes \\ --user=system:node:k8s-m1 \\ --kubeconfig=${K8S_DIR}/kubelet.conf Context &quot;system:node:k8s-m1@kubernetes&quot; created.$ kubectl config use-context system:node:k8s-m1@kubernetes \\ --kubeconfig=${K8S_DIR}/kubelet.conf Switched to context &quot;system:node:k8s-m1@kubernetes&quot;. 接著從 k8s-m1 複製 buildrc 到 k8s-m2 節點。 1$ scp buildrc k8s-m2:/home/vagrant 在 k8s-m2 利用 kubectl 產生 Kubelet 的 kubeconfig 檔案。 12345678910111213141516171819202122232425262728$ source /home/vagrant/buildrc$ kubectl config set-cluster kubernetes \\ --certificate-authority=${PKI_DIR}/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=${K8S_DIR}/kubelet.conf Cluster &quot;kubernetes&quot; set.$ kubectl config set-credentials system:node:k8s-m2 \\ --client-certificate=${PKI_DIR}/kubelet.pem \\ --client-key=${PKI_DIR}/kubelet-key.pem \\ --embed-certs=true \\ --kubeconfig=${K8S_DIR}/kubelet.conf User &quot;system:node:k8s-m2&quot; set.$ kubectl config set-context system:node:k8s-m2@kubernetes \\ --cluster=kubernetes \\ --user=system:node:k8s-m2 \\ --kubeconfig=${K8S_DIR}/kubelet.confContext &quot;system:node:k8s-m2@kubernetes&quot; created.$ kubectl config use-context system:node:k8s-m2@kubernetes \\ --kubeconfig=${K8S_DIR}/kubelet.conf Switched to context &quot;system:node:k8s-m2@kubernetes&quot;. 接著從 k8s-m1 複製 buildrc 到 k8s-m3 節點。 1$ scp buildrc k8s-m3:/home/vagrant 在 k8s-m3 利用 kubectl 產生 Kubelet 的 kubeconfig 檔案。 12345678910111213141516171819202122232425262728$ source /home/vagrant/buildrc$ kubectl config set-cluster kubernetes \\ --certificate-authority=${PKI_DIR}/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=${K8S_DIR}/kubelet.conf Cluster &quot;kubernetes&quot; set.$ kubectl config set-credentials system:node:k8s-m3 \\ --client-certificate=${PKI_DIR}/kubelet.pem \\ --client-key=${PKI_DIR}/kubelet-key.pem \\ --embed-certs=true \\ --kubeconfig=${K8S_DIR}/kubelet.conf User &quot;system:node:k8s-m3&quot; set.$ kubectl config set-context system:node:k8s-m3@kubernetes \\ --cluster=kubernetes \\ --user=system:node:k8s-m3 \\ --kubeconfig=${K8S_DIR}/kubelet.conf Context &quot;system:node:k8s-m3@kubernetes&quot; created.$ kubectl config use-context system:node:k8s-m3@kubernetes \\ --kubeconfig=${K8S_DIR}/kubelet.conf Switched to context &quot;system:node:k8s-m3@kubernetes&quot;. Service Account KeyKubernetes Controller Manager 利用 Key pair 來產生與簽署 Service Account 的 tokens，建立一組公私鑰來讓 API Server 與 Controller Manager 使用。 在 k8s-m1 建立一組 keys。 1234567891011121314$ openssl genrsa -out ${PKI_DIR}/sa.key 2048Generating RSA private key, 2048 bit long modulus (2 primes)........................................................+++++..................+++++e is 65537 (0x010001)$ openssl rsa -in ${PKI_DIR}/sa.key -pubout -out ${PKI_DIR}/sa.pubwriting RSA key$ ls ${PKI_DIR}/sa.*/etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub 將 keys 從 k8s-m1 複製到 k8s-m2 與 k8s-m3。 12$ scp ${PKI_DIR}/sa.key ${PKI_DIR}/sa.pub k8s-m2:${PKI_DIR}/$ scp ${PKI_DIR}/sa.key ${PKI_DIR}/sa.pub k8s-m3:${PKI_DIR}/ 將 kubeconfig 從 k8s-m1 複製到 k8s-m2 與 k8s-m3。 12$ scp ${K8S_DIR}/admin.conf ${K8S_DIR}/controller-manager.conf ${K8S_DIR}/scheduler.conf k8s-m2:${K8S_DIR}/$ scp ${K8S_DIR}/admin.conf ${K8S_DIR}/controller-manager.conf ${K8S_DIR}/scheduler.conf k8s-m3:${K8S_DIR}/ HAProxyHAProxy 主要的目的在於 Kubernetes Cluster 具備 k8s-m1, k8s-m2 與 k8s-m3 三個 Master，並且上面運行 API Server，我們希望可以透過 VIP 方式將 Request 平均分配到每個 API Server 上，因此藉由 HAProxy 達到 Load Balance 效果。 注意：在 k8s-m1, k8s-m2 與 k8s-m3 皆需安裝與啟動服務 更新套件。 1$ apt update 安裝 HAProxy。 1$ apt install -y haproxy 安裝完成後，可以檢視當前 HAProxy 安裝版本。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ apt info haproxy -aPackage: haproxyVersion: 2.0.13-2ubuntu0.1Priority: optionalSection: netOrigin: UbuntuMaintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;Original-Maintainer: Debian HAProxy Maintainers &lt;haproxy@tracker.debian.org&gt;Bugs: https://bugs.launchpad.net/ubuntu/+filebugInstalled-Size: 3,287 kBPre-Depends: dpkg (&gt;= 1.17.14)Depends: libc6 (&gt;= 2.17), libcrypt1 (&gt;= 1:4.1.0), libgcc-s1 (&gt;= 3.0), liblua5.3-0, libpcre2-8-0 (&gt;= 10.22), libssl1.1 (&gt;= 1.1.1), libsystemd0, zlib1g (&gt;= 1:1.1.4), adduser, lsb-base (&gt;= 3.0-6)Suggests: vim-haproxy, haproxy-docHomepage: http://www.haproxy.org/Download-Size: 1,519 kBAPT-Manual-Installed: yesAPT-Sources: http://archive.ubuntu.com/ubuntu focal-updates/main amd64 PackagesDescription: fast and reliable load balancing reverse proxy HAProxy is a TCP/HTTP reverse proxy which is particularly suited for high availability environments. It features connection persistence through HTTP cookies, load balancing, header addition, modification, deletion both ways. It has request blocking capabilities and provides interface to display server status.Package: haproxyVersion: 2.0.13-2Priority: optionalSection: netOrigin: UbuntuMaintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;Original-Maintainer: Debian HAProxy Maintainers &lt;haproxy@tracker.debian.org&gt;Bugs: https://bugs.launchpad.net/ubuntu/+filebugInstalled-Size: 3,287 kBPre-Depends: dpkg (&gt;= 1.17.14)Depends: libc6 (&gt;= 2.17), libcrypt1 (&gt;= 1:4.1.0), libgcc-s1 (&gt;= 3.0), liblua5.3-0, libpcre2-8-0 (&gt;= 10.22), libssl1.1 (&gt;= 1.1.1), libsystemd0, zlib1g (&gt;= 1:1.1.4), adduser, lsb-base (&gt;= 3.0-6)Suggests: vim-haproxy, haproxy-docHomepage: http://www.haproxy.org/Download-Size: 1,519 kBAPT-Sources: http://archive.ubuntu.com/ubuntu focal/main amd64 PackagesDescription: fast and reliable load balancing reverse proxy HAProxy is a TCP/HTTP reverse proxy which is particularly suited for high availability environments. It features connection persistence through HTTP cookies, load balancing, header addition, modification, deletion both ways. It has request blocking capabilities and provides interface to display server status. 配置 /etc/haproxy/haproxy.cfg 檔案。 123456789101112131415161718192021222324$ vim /etc/haproxy/haproxy.cfg...frontend kube-apiserver-https mode tcp bind :6443 default_backend kube-apiserver-backendbackend kube-apiserver-backend mode tcp server k8s-m1-api 192.168.20.10:5443 check server k8s-m2-api 192.168.20.11:5443 check server k8s-m3-api 192.168.20.12:5443 check frontend etcd-frontend mode tcp bind :2381 default_backend etcd-backend backend etcd-backend mode tcp server k8s-m1-api 192.168.20.10:2379 check server k8s-m2-api 192.168.20.11:2379 check server k8s-m3-api 192.168.20.12:2379 check 將上述配置寫入 /etc/haproxy/haproxy.cfg 檔案最底部。 啟動 HAProxy。 12$ systemctl enable haproxy$ systemctl start haproxy 檢視 HAProxy 狀態。 1234567891011121314151617$ systemctl status haproxy● haproxy.service - HAProxy Load Balancer Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 02:43:44 UTC; 11min ago Docs: man:haproxy(1) file:/usr/share/doc/haproxy/configuration.txt.gz Main PID: 38407 (haproxy) Tasks: 2 (limit: 2281) Memory: 1.8M CGroup: /system.slice/haproxy.service ├─38407 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock └─38408 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sockFeb 04 02:43:44 k8s-m1 systemd[1]: Starting HAProxy Load Balancer...Feb 04 02:43:44 k8s-m1 haproxy[38407]: [NOTICE] 034/024344 (38407) : New worker #1 (38408) forkedFeb 04 02:43:44 k8s-m1 systemd[1]: Started HAProxy Load Balancer. KeepAlivedKeepAlived 主要負責在網卡上提供一個 VIP，讓所有服務皆可以透過 VIP 進行服務存取。 注意：在 k8s-m1, k8s-m2 與 k8s-m3 皆需安裝與啟動服務 安裝 Keepalived。 1$ apt install -y keepalived 安裝完成後，可以檢視當前 Keepalived 安裝版本。 12345678910111213141516171819202122232425262728293031$ apt info keepalived -aPackage: keepalivedVersion: 1:2.0.19-2Priority: optionalSection: adminOrigin: UbuntuMaintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;Original-Maintainer: Alexander Wirt &lt;formorer@debian.org&gt;Bugs: https://bugs.launchpad.net/ubuntu/+filebugInstalled-Size: 1,249 kBPre-Depends: init-system-helpers (&gt;= 1.54~)Depends: iproute2, libc6 (&gt;= 2.28), libglib2.0-0 (&gt;= 2.26.0), libmnl0 (&gt;= 1.0.3-4~), libnftnl11 (&gt;= 1.1.2), libnl-3-200 (&gt;= 3.2.27), libnl-genl-3-200 (&gt;= 3.2.7), libpcre2-8-0 (&gt;= 10.22), libsnmp35 (&gt;= 5.8+dfsg), libssl1.1 (&gt;= 1.1.0)Recommends: ipvsadmHomepage: http://keepalived.orgDownload-Size: 360 kBAPT-Manual-Installed: yesAPT-Sources: http://archive.ubuntu.com/ubuntu focal/main amd64 PackagesDescription: Failover and monitoring daemon for LVS clusters keepalived is used for monitoring real servers within a Linux Virtual Server (LVS) cluster. keepalived can be configured to remove real servers from the cluster pool if it stops responding, as well as send a notification email to make the admin aware of the service failure. . In addition, keepalived implements an independent Virtual Router Redundancy Protocol (VRRPv2; see rfc2338 for additional info) framework for director failover. . You need a kernel &gt;= 2.4.28 or &gt;= 2.6.11 for keepalived. See README.Debian for more information. 在 k8s-m1 配置 /etc/keepalived/keepalived.conf` 檔案。 12345678910111213141516$ vim /etc/keepalived/keepalived.confvrrp_instance V1 { state MASTER interface eth1 virtual_router_id 61 priority 102 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 192.168.20.20 }} 在 k8s-m2 配置 /etc/keepalived/keepalived.conf` 檔案。 12345678910111213141516$ vim /etc/keepalived/keepalived.confvrrp_instance V1 { state BACKUP interface eth1 virtual_router_id 61 priority 101 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 192.168.20.20 }} 在 k8s-m3 配置 /etc/keepalived/keepalived.conf` 檔案。 12345678910111213141516$ vim /etc/keepalived/keepalived.confvrrp_instance V1 { state BACKUP interface eth1 virtual_router_id 61 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 192.168.20.20 }} 在配置前使用者需自行修改 Interface。 啟動 Keepalived。 12$ systemctl enable keepalived$ systemctl start keepalived 檢視 Keepalived 狀態。 12345678910111213141516171819202122$ systemctl status keepalived● keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 03:12:04 UTC; 17s ago Main PID: 39501 (keepalived) Tasks: 2 (limit: 2281) Memory: 2.4M CGroup: /system.slice/keepalived.service ├─39501 /usr/sbin/keepalived --dont-fork └─39512 /usr/sbin/keepalived --dont-forkFeb 04 03:12:04 k8s-m1 Keepalived[39501]: WARNING - keepalived was build for newer Linux 5.4.18, running on Linux 5.4.0-58-generic #64-Ubuntu SMP Wed Dec 9 08:16:25 UTC 2020Feb 04 03:12:04 k8s-m1 Keepalived[39501]: Command line: &apos;/usr/sbin/keepalived&apos; &apos;--dont-fork&apos;Feb 04 03:12:04 k8s-m1 Keepalived[39501]: Opening file &apos;/etc/keepalived/keepalived.conf&apos;.Feb 04 03:12:05 k8s-m1 Keepalived[39501]: Starting VRRP child process, pid=39512Feb 04 03:12:05 k8s-m1 Keepalived_vrrp[39512]: Registering Kernel netlink reflectorFeb 04 03:12:05 k8s-m1 Keepalived_vrrp[39512]: Registering Kernel netlink command channelFeb 04 03:12:05 k8s-m1 Keepalived_vrrp[39512]: Opening file &apos;/etc/keepalived/keepalived.conf&apos;.Feb 04 03:12:05 k8s-m1 Keepalived_vrrp[39512]: Registering gratuitous ARP shared channelFeb 04 03:12:05 k8s-m1 Keepalived_vrrp[39512]: (V1) Entering BACKUP STATE (init)Feb 04 03:12:08 k8s-m1 Keepalived_vrrp[39512]: (V1) Entering MASTER STATE 確認網卡上 VIP 綁定狀態。 12345678910111213141516171819202122$ ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:14:86:db brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0 valid_lft 60637sec preferred_lft 60637sec inet6 fe80::a00:27ff:fe14:86db/64 scope link valid_lft forever preferred_lft forever3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:4d:c8:d9 brd ff:ff:ff:ff:ff:ff inet 192.168.20.10/24 brd 192.168.20.255 scope global eth1 valid_lft forever preferred_lft forever inet 192.168.20.20/32 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fe4d:c8d9/64 scope link valid_lft forever preferred_lft forever 由上述可以看到 VIP 已經被綁到 k8s-m1 的 eth1 Interface。 Etcd Cluster在建立 Kubernetes Cluster 之前，我們需要在 k8s-m1, k8s-m2 與 k8s-m3 上構建成一個 Etcd Cluster，主要目的是儲存所有 Kubernetes 的狀態與元件的資訊，後續 Controller 會針對使用者預期 Kubernetes 上運作的服務與 Etcd 進行比對，來運行建立或刪除的修改行為。 注意：在 k8s-m1, k8s-m2 與 k8s-m3 皆需安裝與啟動服務 下載 Etcd 壓縮檔案。 1$ wget https://github.com/etcd-io/etcd/releases/download/v3.3.8/etcd-v3.3.8-linux-amd64.tar.gz 這邊選擇使用 v3.3.8 版本，若使用較新版本在後續配置上會略有不同，讀者可以斟酌使用。 解壓縮 etcd-v3.3.8-linux-amd64.tar.gz 檔案。1$ tar -xvf etcd-v3.3.8-linux-amd64.tar.gz 進入 etcd-v3.3.8-linux-amd64 目錄，並將 etcd 與 etcdctl 檔案複製到 /usr/local/bin/ 目錄。 1$ cp etcd-v3.3.8-linux-amd64/etcd etcd-v3.3.8-linux-amd64/etcdctl /usr/local/bin/ 建立 /etc/systemd/system/etcd.service 啟動檔案。 1234567891011121314151617$ vim /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerDocumentation=https://github.com/coreos/etcdAfter=network.target[Service]User=rootType=notifyExecStart=etcd --config-file /etc/etcd/config.ymlRestart=on-failureRestartSec=10sLimitNOFILE=40000[Install]WantedBy=multi-user.target 在 k8s-m1 建立 /etc/etcd/config.yml 配置檔案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ vim /etc/etcd/config.ymlname: k8s-m1data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 10000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: &apos;https://0.0.0.0:2380&apos;listen-client-urls: &apos;https://0.0.0.0:2379&apos;max-snapshots: 5max-wals: 5cors:initial-advertise-peer-urls: &apos;https://192.168.20.10:2380&apos;advertise-client-urls: &apos;https://192.168.20.10:2379&apos;discovery:discovery-fallback: &apos;proxy&apos;discovery-proxy:discovery-srv:initial-cluster: &quot;k8s-m1=https://192.168.20.10:2380,k8s-m2=https://192.168.20.11:2380,k8s-m3=https://192.168.20.12:2380&quot;initial-cluster-token: &apos;k8s-etcd-cluster&apos;initial-cluster-state: &apos;new&apos;strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: &apos;off&apos;proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security: ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; cert-file: &apos;/etc/etcd/ssl/etcd.pem&apos; key-file: &apos;/etc/etcd/ssl/etcd-key.pem&apos; client-cert-auth: true trusted-ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; auto-tls: truepeer-transport-security: ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; cert-file: &apos;/etc/etcd/ssl/etcd.pem&apos; key-file: &apos;/etc/etcd/ssl/etcd-key.pem&apos; peer-client-cert-auth: true trusted-ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; auto-tls: truedebug: falselog-package-levels:log-output: defaultforce-new-cluster: false 在 k8s-m2 建立 /etc/etcd/config.yml 配置檔案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ vim /etc/etcd/config.ymlname: k8s-m2data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 10000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: &apos;https://0.0.0.0:2380&apos;listen-client-urls: &apos;https://0.0.0.0:2379&apos;max-snapshots: 5max-wals: 5cors:initial-advertise-peer-urls: &apos;https://192.168.20.11:2380&apos;advertise-client-urls: &apos;https://192.168.20.11:2379&apos;discovery:discovery-fallback: &apos;proxy&apos;discovery-proxy:discovery-srv:initial-cluster: &quot;k8s-m1=https://192.168.20.10:2380,k8s-m2=https://192.168.20.11:2380,k8s-m3=https://192.168.20.12:2380&quot;initial-cluster-token: &apos;k8s-etcd-cluster&apos;initial-cluster-state: &apos;new&apos;strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: &apos;off&apos;proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security: ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; cert-file: &apos;/etc/etcd/ssl/etcd.pem&apos; key-file: &apos;/etc/etcd/ssl/etcd-key.pem&apos; client-cert-auth: true trusted-ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; auto-tls: truepeer-transport-security: ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; cert-file: &apos;/etc/etcd/ssl/etcd.pem&apos; key-file: &apos;/etc/etcd/ssl/etcd-key.pem&apos; peer-client-cert-auth: true trusted-ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; auto-tls: truedebug: falselog-package-levels:log-output: defaultforce-new-cluster: false 在 k8s-m3 建立 /etc/etcd/config.yml 配置檔案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ vim /etc/etcd/config.ymlname: k8s-m3data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 10000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: &apos;https://0.0.0.0:2380&apos;listen-client-urls: &apos;https://0.0.0.0:2379&apos;max-snapshots: 5max-wals: 5cors:initial-advertise-peer-urls: &apos;https://192.168.20.12:2380&apos;advertise-client-urls: &apos;https://192.168.20.12:2379&apos;discovery:discovery-fallback: &apos;proxy&apos;discovery-proxy:discovery-srv:initial-cluster: &quot;k8s-m1=https://192.168.20.10:2380,k8s-m2=https://192.168.20.11:2380,k8s-m3=https://192.168.20.12:2380&quot;initial-cluster-token: &apos;k8s-etcd-cluster&apos;initial-cluster-state: &apos;new&apos;strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: &apos;off&apos;proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security: ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; cert-file: &apos;/etc/etcd/ssl/etcd.pem&apos; key-file: &apos;/etc/etcd/ssl/etcd-key.pem&apos; client-cert-auth: true trusted-ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; auto-tls: truepeer-transport-security: ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; cert-file: &apos;/etc/etcd/ssl/etcd.pem&apos; key-file: &apos;/etc/etcd/ssl/etcd-key.pem&apos; peer-client-cert-auth: true trusted-ca-file: &apos;/etc/etcd/ssl/etcd-ca.pem&apos; auto-tls: truedebug: falselog-package-levels:log-output: defaultforce-new-cluster: false 啟動 Etcd。 12$ systemctl enable etcd$ systemctl start etcd 檢視 Etcd 狀態。 12345678910111213141516171819202122$ systemctl status etcd● etcd.service - Etcd Server Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 03:41:37 UTC; 46s ago Docs: https://github.com/coreos/etcd Main PID: 39586 (etcd) Tasks: 7 (limit: 2281) Memory: 9.4M CGroup: /system.slice/etcd.service └─39586 /usr/local/bin/etcd --config-file /etc/etcd/config.ymlFeb 04 03:41:37 k8s-m1 etcd[39586]: enabled capabilities for version 3.0Feb 04 03:41:38 k8s-m1 etcd[39586]: health check for peer 77fc17e6efe1ad72 could not connect: dial tcp 192.168.20.12:2380: getsockopt: connection refusedFeb 04 03:41:39 k8s-m1 etcd[39586]: peer 77fc17e6efe1ad72 became activeFeb 04 03:41:39 k8s-m1 etcd[39586]: established a TCP streaming connection with peer 77fc17e6efe1ad72 (stream Message reader)Feb 04 03:41:39 k8s-m1 etcd[39586]: established a TCP streaming connection with peer 77fc17e6efe1ad72 (stream Message writer)Feb 04 03:41:39 k8s-m1 etcd[39586]: established a TCP streaming connection with peer 77fc17e6efe1ad72 (stream MsgApp v2 writer)Feb 04 03:41:39 k8s-m1 etcd[39586]: established a TCP streaming connection with peer 77fc17e6efe1ad72 (stream MsgApp v2 reader)Feb 04 03:41:41 k8s-m1 etcd[39586]: updating the cluster version from 3.0 to 3.3Feb 04 03:41:41 k8s-m1 etcd[39586]: updated the cluster version from 3.0 to 3.3Feb 04 03:41:41 k8s-m1 etcd[39586]: enabled capabilities for version 3.3 檢查 Etcd Cluster 每個節點的狀態。 12345678$ etcdctl --endpoints https://127.0.0.1:2379 --ca-file=/etc/etcd/ssl/etcd-ca.pem --cert-file=/etc/etcd/ssl/etcd.pem --key-file=/etc/etcd/ssl/etcd-key.pem --debug cluster-healthCluster-Endpoints: https://127.0.0.1:2379cURL Command: curl -X GET https://127.0.0.1:2379/v2/membersmember 77fc17e6efe1ad72 is healthy: got healthy result from https://192.168.20.12:2379member 962eacc32c073e45 is healthy: got healthy result from https://192.168.20.11:2379member be410bb048009f30 is healthy: got healthy result from https://192.168.20.10:2379cluster is healthy APIServerAPIServer 負責提供各元件之間的溝通，像是 Scheduler 進行 Pod 的 filering 與 scoring，或使用者透過 Kubectl 指令對 Kubernetes 操作等等。 注意：在 k8s-m1, k8s-m2 與 k8s-m3 皆需安裝與啟動服務 在 k8s-m1 建立 /etc/systemd/system/apiserver.service 檔案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455$ vim /etc/systemd/system/apiserver.service[Unit]Description=k8s API ServerDocumentation=https://github.com/coreos/etcdAfter=network.target[Service]User=rootType=notifyExecStart=kube-apiserver \\ --v=0 \\ --logtostderr=true \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=5443 \\ --insecure-port=0 \\ --advertise-address=192.168.20.20 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.20.20:2381 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=front-proxy-client \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --disable-admission-plugins=PersistentVolumeLabel \\ --enable-admission-plugins=NodeRestriction \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=128 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --audit-policy-file=/etc/kubernetes/audit/policy.yml \\ --encryption-provider-config=/etc/kubernetes/encryption/config.yml \\ --event-ttl=1hRestart=on-failureRestartSec=10sLimitNOFILE=40000[Install]WantedBy=multi-user.target advertise-address 需成 VIP etcd-servers 需更為 2381 Port，讓呼叫 Etcd 過 LoadBalancer 在 k8s-m1 建立 /etc/kubernetes/encryption/config.yml 與 /etc/kubernetes/audit/policy.yml 檔案。 12345678910111213141516171819202122$ mkdir -p /etc/kubernetes/encryption/$ vim /etc/kubernetes/encryption/config.ymlkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 8a22dac954598020c9eeab1c368f7c82 - identity: {} $ mkdir -p /etc/kubernetes/audit$ vim /etc/kubernetes/audit/policy.ymlapiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: Metadata secret 根據讀者需求自行更改。 在 k8s-m1 啟動 Kubernetes APIServer。 12$ systemctl enable apiserver$ systemctl start apiserver 在 k8s-m1 檢查 Kubernetes APIserver 狀態。 12345678910111213141516171819202122$ systemctl status apiserver● apiserver.service - k8s API Server Loaded: loaded (/etc/systemd/system/apiserver.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2021-02-06 01:54:06 UTC; 6s ago Docs: https://github.com/coreos/etcd Main PID: 58191 (kube-apiserver) Tasks: 6 (limit: 2281) Memory: 296.6M CGroup: /system.slice/apiserver.service └─58191 /usr/local/bin/kube-apiserver --v=0 --logtostderr=true --allow-privileged=true --bind-address=0.0.0.0 --secure-port=5443 --insecure-port=0 --advertise-address=192.168.20.20 --service-cluster-ip-range=10.96.0.0/12 --service-node-port&gt;Feb 06 01:54:06 k8s-m1 kube-apiserver[58191]: I0206 01:54:06.838406 58191 crd_finalizer.go:266] Starting CRDFinalizerFeb 06 01:54:06 k8s-m1 kube-apiserver[58191]: E0206 01:54:06.897990 58191 controller.go:152] Unable to remove old endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: /registry/masterleases/192.168.20.20, ResourceVersion: 0, A&gt;Feb 06 01:54:06 k8s-m1 kube-apiserver[58191]: I0206 01:54:06.930700 58191 cache.go:39] Caches are synced for autoregister controllerFeb 06 01:54:06 k8s-m1 kube-apiserver[58191]: I0206 01:54:06.931472 58191 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controllerFeb 06 01:54:06 k8s-m1 kube-apiserver[58191]: I0206 01:54:06.934754 58191 cache.go:39] Caches are synced for APIServiceRegistrationController controllerFeb 06 01:54:06 k8s-m1 kube-apiserver[58191]: I0206 01:54:06.939621 58191 cache.go:39] Caches are synced for AvailableConditionController controllerFeb 06 01:54:06 k8s-m1 kube-apiserver[58191]: I0206 01:54:06.940080 58191 shared_informer.go:247] Caches are synced for crd-autoregisterFeb 06 01:54:07 k8s-m1 kube-apiserver[58191]: I0206 01:54:07.827368 58191 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).Feb 06 01:54:07 k8s-m1 kube-apiserver[58191]: I0206 01:54:07.828477 58191 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).Feb 06 01:54:07 k8s-m1 kube-apiserver[58191]: I0206 01:54:07.843918 58191 storage_scheduling.go:143] all system priority classes are created successfully or already exist. 將 /etc/kubernetes/pki/apiserver.pem 複製到 k8s-m2 與 k8s-m3。 12$ scp ${PKI_DIR}/apiserver.pem ${PKI_DIR}/apiserver-key.pem ${PKI_DIR}/front-proxy-ca.pem ${PKI_DIR}/front-proxy-client.pem ${PKI_DIR}/front-proxy-client-key.pem k8s-m2:${PKI_DIR}/$ scp ${PKI_DIR}/apiserver.pem ${PKI_DIR}/apiserver-key.pem ${PKI_DIR}/front-proxy-ca.pem ${PKI_DIR}/front-proxy-client.pem ${PKI_DIR}/front-proxy-client-key.pem k8s-m3:${PKI_DIR}/ 在 k8s-m2 建立 /etc/systemd/system/apiserver.service 檔案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455$ vim /etc/systemd/system/apiserver.service[Unit]Description=k8s API ServerDocumentation=https://github.com/coreos/etcdAfter=network.target[Service]User=rootType=notifyExecStart=kube-apiserver \\ --v=0 \\ --logtostderr=true \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=5443 \\ --insecure-port=0 \\ --advertise-address=192.168.20.20 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.20.20:2381 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=front-proxy-client \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --disable-admission-plugins=PersistentVolumeLabel \\ --enable-admission-plugins=NodeRestriction \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=128 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --audit-policy-file=/etc/kubernetes/audit/policy.yml \\ --encryption-provider-config=/etc/kubernetes/encryption/config.yml \\ --event-ttl=1hRestart=on-failureRestartSec=10sLimitNOFILE=40000[Install]WantedBy=multi-user.target 在 k8s-m2 建立 /etc/kubernetes/encryption/config.yml 與 /etc/kubernetes/audit/policy.yml 檔案。 12345678910111213141516171819202122$ mkdir -p /etc/kubernetes/encryption/$ vim /etc/kubernetes/encryption/config.ymlkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 8a22dac954598020c9eeab1c368f7c82 - identity: {} $ mkdir -p /etc/kubernetes/audit$ vim /etc/kubernetes/audit/policy.ymlapiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: Metadata 在 k8s-m2 啟動 Kubernetes APIServer。 12$ systemctl enable apiserver$ systemctl start apiserver 在 k8s-m2 檢查 Kubernetes APIserver 狀態。 12345678910111213141516171819202122$ systemctl status apiserver● apiserver.service - k8s API Server Loaded: loaded (/etc/systemd/system/apiserver.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2021-02-06 01:59:28 UTC; 22s ago Docs: https://github.com/coreos/etcd Main PID: 23441 (kube-apiserver) Tasks: 7 (limit: 2281) Memory: 301.2M CGroup: /system.slice/apiserver.service └─23441 /usr/local/bin/kube-apiserver --v=0 --logtostderr=true --allow-privileged=true --bind-address=0.0.0.0 --&gt;Feb 06 01:59:28 k8s-m2 kube-apiserver[23441]: I0206 01:59:28.485105 23441 shared_informer.go:240] Waiting for caches to syn&gt;Feb 06 01:59:28 k8s-m2 kube-apiserver[23441]: E0206 01:59:28.614593 23441 controller.go:152] Unable to remove old endpoints&gt;Feb 06 01:59:28 k8s-m2 kube-apiserver[23441]: I0206 01:59:28.664503 23441 shared_informer.go:247] Caches are synced for clu&gt;Feb 06 01:59:28 k8s-m2 kube-apiserver[23441]: I0206 01:59:28.668346 23441 cache.go:39] Caches are synced for APIServiceRegi&gt;Feb 06 01:59:28 k8s-m2 kube-apiserver[23441]: I0206 01:59:28.675642 23441 cache.go:39] Caches are synced for AvailableCondi&gt;Feb 06 01:59:28 k8s-m2 kube-apiserver[23441]: I0206 01:59:28.676190 23441 cache.go:39] Caches are synced for autoregister c&gt;Feb 06 01:59:28 k8s-m2 kube-apiserver[23441]: I0206 01:59:28.687991 23441 shared_informer.go:247] Caches are synced for crd&gt;Feb 06 01:59:29 k8s-m2 kube-apiserver[23441]: I0206 01:59:29.456591 23441 controller.go:132] OpenAPI AggregationController:&gt;Feb 06 01:59:29 k8s-m2 kube-apiserver[23441]: I0206 01:59:29.456849 23441 controller.go:132] OpenAPI AggregationController:&gt;Feb 06 01:59:29 k8s-m2 kube-apiserver[23441]: I0206 01:59:29.474785 23441 storage_scheduling.go:143] all system priority cl&gt; 在 k8s-m3 建立 /etc/systemd/system/apiserver.service 檔案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455$ vim /etc/systemd/system/apiserver.service[Unit]Description=k8s API ServerDocumentation=https://github.com/coreos/etcdAfter=network.target[Service]User=rootType=notifyExecStart=kube-apiserver \\ --v=0 \\ --logtostderr=true \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=5443 \\ --insecure-port=0 \\ --advertise-address=192.168.20.20 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.20.20:2381 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\ --service-account-key-file=/etc/kubernetes/pki/sa.pub \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=front-proxy-client \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --disable-admission-plugins=PersistentVolumeLabel \\ --enable-admission-plugins=NodeRestriction \\ --authorization-mode=Node,RBAC \\ --enable-bootstrap-token-auth=true \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=128 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --audit-policy-file=/etc/kubernetes/audit/policy.yml \\ --encryption-provider-config=/etc/kubernetes/encryption/config.yml \\ --event-ttl=1hRestart=on-failureRestartSec=10sLimitNOFILE=40000[Install]WantedBy=multi-user.target 在 k8s-m3 建立 /etc/kubernetes/encryption/config.yml 與 /etc/kubernetes/audit/policy.yml 檔案。 12345678910111213141516171819202122$ mkdir -p /etc/kubernetes/encryption/$ vim /etc/kubernetes/encryption/config.ymlkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 8a22dac954598020c9eeab1c368f7c82 - identity: {} $ mkdir -p /etc/kubernetes/audit$ vim /etc/kubernetes/audit/policy.ymlapiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: Metadata 在 k8s-m3 啟動 Kubernetes APIServer。 12$ systemctl enable apiserver$ systemctl start apiserver 在 k8s-m3 檢查 Kubernetes APIserver 狀態。 12345678910111213141516171819202122$ systemctl status apiserver● apiserver.service - k8s API Server Loaded: loaded (/etc/systemd/system/apiserver.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2021-02-06 02:01:07 UTC; 15s ago Docs: https://github.com/coreos/etcd Main PID: 11887 (kube-apiserver) Tasks: 7 (limit: 2281) Memory: 294.7M CGroup: /system.slice/apiserver.service └─11887 /usr/local/bin/kube-apiserver --v=0 --logtostderr=true --allow-privileged=true --bind-address=0.0.0.0 --s&gt;Feb 06 02:01:07 k8s-m3 kube-apiserver[11887]: I0206 02:01:07.599249 11887 shared_informer.go:240] Waiting for caches to sync&gt;Feb 06 02:01:07 k8s-m3 kube-apiserver[11887]: I0206 02:01:07.692457 11887 cache.go:39] Caches are synced for APIServiceRegis&gt;Feb 06 02:01:07 k8s-m3 kube-apiserver[11887]: I0206 02:01:07.698206 11887 cache.go:39] Caches are synced for AvailableCondit&gt;Feb 06 02:01:07 k8s-m3 kube-apiserver[11887]: I0206 02:01:07.698808 11887 shared_informer.go:247] Caches are synced for clus&gt;Feb 06 02:01:07 k8s-m3 kube-apiserver[11887]: I0206 02:01:07.698908 11887 cache.go:39] Caches are synced for autoregister co&gt;Feb 06 02:01:07 k8s-m3 kube-apiserver[11887]: I0206 02:01:07.719845 11887 shared_informer.go:247] Caches are synced for crd-&gt;Feb 06 02:01:07 k8s-m3 kube-apiserver[11887]: E0206 02:01:07.720204 11887 controller.go:152] Unable to remove old endpoints &gt;Feb 06 02:01:08 k8s-m3 kube-apiserver[11887]: I0206 02:01:08.583628 11887 controller.go:132] OpenAPI AggregationController: &gt;Feb 06 02:01:08 k8s-m3 kube-apiserver[11887]: I0206 02:01:08.583926 11887 controller.go:132] OpenAPI AggregationController: &gt;Feb 06 02:01:08 k8s-m3 kube-apiserver[11887]: I0206 02:01:08.611881 11887 storage_scheduling.go:143] all system priority cla&gt; SchedulerScheduler 負責在每次建立 Pod 時，根據 filtering 階段篩選出符合需求的 Worker Node，接著在 scoring 階段針對每個符合的 worker node 進行評分，評分依據為節點當前狀態，若剩餘資源較少的節點則分數可能較低，最後評分較高者作為 Pod 建置的 Worker Node。若分數相同則使用亂數方式從中選擇其一作為最後建置的 Worker Node。 注意：在 k8s-m1, k8s-m2 與 k8s-m3 皆需安裝與啟動服務 在 k8s-m1 建立 /etc/systemd/system/scheduler.service 檔案。 12345678910111213141516171819$ vim /etc/systemd/system/scheduler.service[Unit]Description=k8s Scheduler ServerDocumentation=https://github.com/coreos/etcd[Service]User=rootExecStart=kube-scheduler \\ --v=0 \\ --logtostderr=true \\ --address=127.0.0.1 \\ --leader-elect=true \\ --kubeconfig=/etc/kubernetes/scheduler.confRestart=on-failureRestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m1 啟動 Kubernetes Scheduler。 12$ systemctl enable scheduler$ systemctl start scheduler 在 k8s-m 檢視 Kubernetes Scheduler 狀態。 12345678910111213141516171819202122$ systemctl status scheduler● scheduler.service - k8s Scheduler Server Loaded: loaded (/etc/systemd/system/scheduler.service; disabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 07:55:58 UTC; 6s ago Docs: https://github.com/coreos/etcd Main PID: 42594 (kube-scheduler) Tasks: 6 (limit: 2281) Memory: 12.7M CGroup: /system.slice/scheduler.service └─42594 /usr/local/bin/kube-scheduler --v=0 --logtostderr=true --address=127.0.0.1 --leader-elect=true --kubeconfig=/etc/kubernetes/scheduler.confFeb 04 07:55:59 k8s-m1 kube-scheduler[42594]: W0204 07:55:59.367754 42594 authorization.go:156] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won&apos;t work.Feb 04 07:55:59 k8s-m1 kube-scheduler[42594]: I0204 07:55:59.378086 42594 registry.go:173] Registering SelectorSpread pluginFeb 04 07:55:59 k8s-m1 kube-scheduler[42594]: I0204 07:55:59.378268 42594 registry.go:173] Registering SelectorSpread pluginFeb 04 07:55:59 k8s-m1 kube-scheduler[42594]: W0204 07:55:59.379735 42594 authorization.go:47] Authorization is disabledFeb 04 07:55:59 k8s-m1 kube-scheduler[42594]: W0204 07:55:59.379837 42594 authentication.go:40] Authentication is disabledFeb 04 07:55:59 k8s-m1 kube-scheduler[42594]: I0204 07:55:59.379924 42594 deprecated_insecure_serving.go:51] Serving healthz insecurely on 127.0.0.1:10251Feb 04 07:55:59 k8s-m1 kube-scheduler[42594]: I0204 07:55:59.381294 42594 secure_serving.go:197] Serving securely on [::]:10259Feb 04 07:55:59 k8s-m1 kube-scheduler[42594]: I0204 07:55:59.382191 42594 tlsconfig.go:240] Starting DynamicServingCertificateControllerFeb 04 07:55:59 k8s-m1 kube-scheduler[42594]: I0204 07:55:59.482170 42594 leaderelection.go:243] attempting to acquire leader lease kube-system/kube-scheduler...Feb 04 07:55:59 k8s-m1 kube-scheduler[42594]: I0204 07:55:59.502458 42594 leaderelection.go:253] successfully acquired lease kube-system/kube-scheduler 在 k8s-m2 建立 /etc/systemd/system/scheduler.service 檔案。 12345678910111213141516171819$ vim /etc/systemd/system/scheduler.service[Unit]Description=k8s Scheduler ServerDocumentation=https://github.com/coreos/etcd[Service]User=rootExecStart=kube-scheduler \\ --v=0 \\ --logtostderr=true \\ --address=127.0.0.1 \\ --leader-elect=true \\ --kubeconfig=/etc/kubernetes/scheduler.confRestart=on-failureRestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m2 啟動 Kubernetes Scheduler 12$ systemctl enable scheduler$ systemctl start scheduler 在 k8s-m2 檢視 Kubernetes Scheduler 狀態。 12345678910111213141516171819202122$ systemctl status scheduler● scheduler.service - k8s Scheduler Server Loaded: loaded (/etc/systemd/system/scheduler.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 07:56:40 UTC; 1min 31s ago Docs: https://github.com/coreos/etcd Main PID: 7418 (kube-scheduler) Tasks: 6 (limit: 2281) Memory: 12.1M CGroup: /system.slice/scheduler.service └─7418 /usr/local/bin/kube-scheduler --v=0 --logtostderr=true --address=127.0.0.1 --leader-elect=true --kubeconfig&gt;Feb 04 07:56:41 k8s-m2 kube-scheduler[7418]: W0204 07:56:41.731915 7418 authentication.go:289] No authentication-kubeconfig &gt;Feb 04 07:56:41 k8s-m2 kube-scheduler[7418]: W0204 07:56:41.732028 7418 authorization.go:156] No authorization-kubeconfig pr&gt;Feb 04 07:56:41 k8s-m2 kube-scheduler[7418]: I0204 07:56:41.744388 7418 registry.go:173] Registering SelectorSpread pluginFeb 04 07:56:41 k8s-m2 kube-scheduler[7418]: I0204 07:56:41.744586 7418 registry.go:173] Registering SelectorSpread pluginFeb 04 07:56:41 k8s-m2 kube-scheduler[7418]: W0204 07:56:41.746459 7418 authorization.go:47] Authorization is disabledFeb 04 07:56:41 k8s-m2 kube-scheduler[7418]: W0204 07:56:41.746668 7418 authentication.go:40] Authentication is disabledFeb 04 07:56:41 k8s-m2 kube-scheduler[7418]: I0204 07:56:41.746766 7418 deprecated_insecure_serving.go:51] Serving healthz i&gt;Feb 04 07:56:41 k8s-m2 kube-scheduler[7418]: I0204 07:56:41.749890 7418 secure_serving.go:197] Serving securely on [::]:10259Feb 04 07:56:41 k8s-m2 kube-scheduler[7418]: I0204 07:56:41.750208 7418 tlsconfig.go:240] Starting DynamicServingCertificate&gt;Feb 04 07:56:41 k8s-m2 kube-scheduler[7418]: I0204 07:56:41.850345 7418 leaderelection.go:243] attempting to acquire leader 在 k8s-m3 建立 /etc/systemd/system/scheduler.service 檔案。 12345678910111213141516171819$ vim /etc/systemd/system/scheduler.service[Unit]Description=k8s Scheduler ServerDocumentation=https://github.com/coreos/etcd[Service]User=rootExecStart=kube-scheduler \\ --v=0 \\ --logtostderr=true \\ --address=127.0.0.1 \\ --leader-elect=true \\ --kubeconfig=/etc/kubernetes/scheduler.confRestart=on-failureRestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m3 啟動 Kubernetes Scheduler 12$ systemctl enable scheduler$ systemctl start scheduler 在 k8s-m3 檢視 Kubernetes Scheduler 狀態。 12345678910111213141516171819202122$ systemctl status scheduler● scheduler.service - k8s Scheduler Server Loaded: loaded (/etc/systemd/system/scheduler.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 07:59:24 UTC; 28s ago Docs: https://github.com/coreos/etcd Main PID: 17623 (kube-scheduler) Tasks: 6 (limit: 2281) Memory: 11.5M CGroup: /system.slice/scheduler.service └─17623 /usr/local/bin/kube-scheduler --v=0 --logtostderr=true --address=127.0.0.1 --leader-elect=true --kubeconfig=/etc/kubernet&gt;Feb 04 07:59:24 k8s-m3 kube-scheduler[17623]: W0204 07:59:24.487163 17623 authentication.go:289] No authentication-kubeconfig provided in or&gt;Feb 04 07:59:24 k8s-m3 kube-scheduler[17623]: W0204 07:59:24.487313 17623 authorization.go:156] No authorization-kubeconfig provided, so Sub&gt;Feb 04 07:59:24 k8s-m3 kube-scheduler[17623]: I0204 07:59:24.500454 17623 registry.go:173] Registering SelectorSpread pluginFeb 04 07:59:24 k8s-m3 kube-scheduler[17623]: I0204 07:59:24.500674 17623 registry.go:173] Registering SelectorSpread pluginFeb 04 07:59:24 k8s-m3 kube-scheduler[17623]: W0204 07:59:24.502747 17623 authorization.go:47] Authorization is disabledFeb 04 07:59:24 k8s-m3 kube-scheduler[17623]: W0204 07:59:24.502931 17623 authentication.go:40] Authentication is disabledFeb 04 07:59:24 k8s-m3 kube-scheduler[17623]: I0204 07:59:24.503030 17623 deprecated_insecure_serving.go:51] Serving healthz insecurely on 1&gt;Feb 04 07:59:24 k8s-m3 kube-scheduler[17623]: I0204 07:59:24.503870 17623 secure_serving.go:197] Serving securely on [::]:10259Feb 04 07:59:24 k8s-m3 kube-scheduler[17623]: I0204 07:59:24.504056 17623 tlsconfig.go:240] Starting DynamicServingCertificateControllerFeb 04 07:59:24 k8s-m3 kube-scheduler[17623]: I0204 07:59:24.604511 17623 leaderelection.go:243] attempting to acquire leader lease kube-sy&gt; Controller ManagerController Manager 負責將使用者預期的服務狀態與當前狀態盡可能達到一致。像是一個無窮迴圈不斷的將當前 Pod…等等的狀態與 Etcd 進行比較，若 Pod 數少於使用者配置，則新增到使用者預期的數量，反之也如此。 注意：在 k8s-m1, k8s-m2 與 k8s-m3 皆需安裝與啟動服務 在 k8s-m1 建立 /etc/systemd/system/controller.service 檔案。 12345678910111213141516171819202122232425262728293031$ vim /etc/systemd/system/controller.service[Unit]Description=k8s Controller ServerDocumentation=https://github.com/coreos/etcd[Service]User=rootExecStart=kube-controller-manager \\ --v=0 \\ --logtostderr=true \\ --address=127.0.0.1 \\ --root-ca-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --kubeconfig=/etc/kubernetes/controller-manager.conf \\ --leader-elect=true \\ --use-service-account-credentials=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=2m0s \\ --controllers=*,bootstrapsigner,tokencleaner \\ --allocate-node-cidrs=true \\ --cluster-cidr=10.244.0.0/16 \\ --node-cidr-mask-size=24Restart=alwaysRestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m1 啟動 Kubernetes Controller Manager。 12$ systemctl enable controller$ systemctl start controller 在 k8s-m1 檢視 Kubernetes Controller Manager 狀態。 12345678910111213141516171819202122$ systemctl status controller● controller.service - k8s Controller Server Loaded: loaded (/etc/systemd/system/controller.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 08:05:58 UTC; 49s ago Docs: https://github.com/coreos/etcd Main PID: 42654 (kube-controller) Tasks: 4 (limit: 2281) Memory: 33.3M CGroup: /system.slice/controller.service └─42654 /usr/local/bin/kube-controller-manager --v=0 --logtostderr=true --address=127.0.0.1 --root-ca-file=/etc/kubernetes/pki/ca.pem --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem --service-a&gt;Feb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.085006 42654 disruption.go:339] Sending events to api server.Feb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.086305 42654 shared_informer.go:247] Caches are synced for deploymentFeb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.143749 42654 shared_informer.go:247] Caches are synced for resource quotaFeb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.148362 42654 shared_informer.go:247] Caches are synced for resource quotaFeb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.185093 42654 shared_informer.go:247] Caches are synced for endpointFeb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.187570 42654 shared_informer.go:247] Caches are synced for endpoint_slice_mirroringFeb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.196972 42654 shared_informer.go:240] Waiting for caches to sync for garbage collectorFeb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.497394 42654 shared_informer.go:247] Caches are synced for garbage collectorFeb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.534939 42654 shared_informer.go:247] Caches are synced for garbage collectorFeb 04 08:06:17 k8s-m1 kube-controller-manager[42654]: I0204 08:06:17.534985 42654 garbagecollector.go:137] Garbage collector: all resource monitors have synced. Proceeding to collect garbage 在 k8s-m2 建立 /etc/systemd/system/controller.service 檔案。 12345678910111213141516171819202122232425262728293031$ vim /etc/systemd/system/controller.service[Unit]Description=k8s Controller ServerDocumentation=https://github.com/coreos/etcd[Service]User=rootExecStart=kube-controller-manager \\ --v=0 \\ --logtostderr=true \\ --address=127.0.0.1 \\ --root-ca-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --kubeconfig=/etc/kubernetes/controller-manager.conf \\ --leader-elect=true \\ --use-service-account-credentials=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=2m0s \\ --controllers=*,bootstrapsigner,tokencleaner \\ --allocate-node-cidrs=true \\ --cluster-cidr=10.244.0.0/16 \\ --node-cidr-mask-size=24Restart=alwaysRestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m2 啟動 Kubernetes Controller Manager。 12$ systemctl enable controller$ systemctl start controller 在 k8s-m2 檢視 Kubernetes Controller Manager 狀態。 12345678910111213141516171819202122$ systemctl status controller● controller.service - k8s Controller Server Loaded: loaded (/etc/systemd/system/controller.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 08:22:15 UTC; 20s ago Docs: https://github.com/coreos/etcd Main PID: 7552 (kube-controller) Tasks: 4 (limit: 2281) Memory: 18.5M CGroup: /system.slice/controller.service └─7552 /usr/local/bin/kube-controller-manager --v=0 --logtostderr=true --address=127.0.0.1 --root-ca-file=/etc/kub&gt;Feb 04 08:22:15 k8s-m2 kube-controller-manager[7552]: Flag --address has been deprecated, see --bind-address instead.Feb 04 08:22:15 k8s-m2 kube-controller-manager[7552]: I0204 08:22:15.661447 7552 serving.go:331] Generated self-signed cert &gt;Feb 04 08:22:16 k8s-m2 kube-controller-manager[7552]: W0204 08:22:16.017686 7552 authentication.go:265] No authentication-ku&gt;Feb 04 08:22:16 k8s-m2 kube-controller-manager[7552]: W0204 08:22:16.017920 7552 authentication.go:289] No authentication-ku&gt;Feb 04 08:22:16 k8s-m2 kube-controller-manager[7552]: W0204 08:22:16.018021 7552 authorization.go:156] No authorization-kube&gt;Feb 04 08:22:16 k8s-m2 kube-controller-manager[7552]: I0204 08:22:16.018115 7552 controllermanager.go:175] Version: v1.19.7Feb 04 08:22:16 k8s-m2 kube-controller-manager[7552]: I0204 08:22:16.018860 7552 secure_serving.go:197] Serving securely on &gt;Feb 04 08:22:16 k8s-m2 kube-controller-manager[7552]: I0204 08:22:16.019302 7552 deprecated_insecure_serving.go:53] Serving &gt;Feb 04 08:22:16 k8s-m2 kube-controller-manager[7552]: I0204 08:22:16.019418 7552 leaderelection.go:243] attempting to acquir&gt;Feb 04 08:22:16 k8s-m2 kube-controller-manager[7552]: I0204 08:22:16.019671 7552 tlsconfig.go:240] Starting DynamicServingCe&gt; 在 k8s-m3 建立 /etc/systemd/system/controller.service 檔案。 12345678910111213141516171819202122232425262728293031$ vim /etc/systemd/system/controller.service[Unit]Description=k8s Controller ServerDocumentation=https://github.com/coreos/etcd[Service]User=rootExecStart=kube-controller-manager \\ --v=0 \\ --logtostderr=true \\ --address=127.0.0.1 \\ --root-ca-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --kubeconfig=/etc/kubernetes/controller-manager.conf \\ --leader-elect=true \\ --use-service-account-credentials=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=2m0s \\ --controllers=*,bootstrapsigner,tokencleaner \\ --allocate-node-cidrs=true \\ --cluster-cidr=10.244.0.0/16 \\ --node-cidr-mask-size=24Restart=alwaysRestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m3 啟動 Kubernetes Controller Manager。 12$ systemctl enable controller$ systemctl start controller 在 k8s-m3 檢視 Kubernetes Controller Manager 狀態。 12345678910111213141516171819202122$ systemctl status controller● controller.service - k8s Controller Server Loaded: loaded (/etc/systemd/system/controller.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-02-04 08:23:44 UTC; 13s ago Docs: https://github.com/coreos/etcd Main PID: 17748 (kube-controller) Tasks: 4 (limit: 2281) Memory: 19.0M CGroup: /system.slice/controller.service └─17748 /usr/local/bin/kube-controller-manager --v=0 --logtostderr=true --address=127.0.0.1 --root-ca-file=/etc/kubernetes/pki/ca&gt;Feb 04 08:23:44 k8s-m3 kube-controller-manager[17748]: Flag --address has been deprecated, see --bind-address instead.Feb 04 08:23:44 k8s-m3 kube-controller-manager[17748]: I0204 08:23:44.665536 17748 serving.go:331] Generated self-signed cert in-memoryFeb 04 08:23:45 k8s-m3 kube-controller-manager[17748]: W0204 08:23:45.034568 17748 authentication.go:265] No authentication-kubeconfig provi&gt;Feb 04 08:23:45 k8s-m3 kube-controller-manager[17748]: W0204 08:23:45.034851 17748 authentication.go:289] No authentication-kubeconfig provi&gt;Feb 04 08:23:45 k8s-m3 kube-controller-manager[17748]: W0204 08:23:45.034956 17748 authorization.go:156] No authorization-kubeconfig provide&gt;Feb 04 08:23:45 k8s-m3 kube-controller-manager[17748]: I0204 08:23:45.035051 17748 controllermanager.go:175] Version: v1.19.7Feb 04 08:23:45 k8s-m3 kube-controller-manager[17748]: I0204 08:23:45.035789 17748 secure_serving.go:197] Serving securely on [::]:10257Feb 04 08:23:45 k8s-m3 kube-controller-manager[17748]: I0204 08:23:45.036330 17748 deprecated_insecure_serving.go:53] Serving insecurely on &gt;Feb 04 08:23:45 k8s-m3 kube-controller-manager[17748]: I0204 08:23:45.036458 17748 leaderelection.go:243] attempting to acquire leader lease&gt;Feb 04 08:23:45 k8s-m3 kube-controller-manager[17748]: I0204 08:23:45.036703 17748 tlsconfig.go:240] Starting DynamicServingCertificateContr&gt; Master KubeletKubernetes 中 Master 與 worker Node 上皆會運行 Kubelet，主要負責將 Scheduler 所安排的 Pod 在 Kebelet 所在的節點上根據 CRI 將服務運行起來，並且配置完成。 注意：在 k8s-m1, k8s-m2 與 k8s-m3 皆需安裝與啟動服務 在 k8s-m1 建立 /etc/systemd/system/kubelet.service 檔案。 1234567891011121314$ vim /etc/systemd/system/kubelet.service[Unit]Description=kubelet: The Kubernetes Node AgentDocumentation=http://kubernetes.io/docs/[Service]ExecStart=/usr/local/bin/kubeletRestart=alwaysStartLimitInterval=0RestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m1 建立 /etc/systemd/system/kubelet.service.d/10-kubelet.conf 檔案。 12345678910$ mkdir -p /etc/systemd/system/kubelet.service.d/$ vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf[Service]Environment=&quot;KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf&quot;Environment=&quot;KUBELET_SYSTEM_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot;Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yml&quot;Environment=&quot;KUBELET_EXTRA_ARGS=--node-labels=kubelet.kubernetes.io/master=master&quot;ExecStart=ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS 在 k8s-m1 建立 /var/lib/kubelet/config.yml 檔案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273$ mkdir -p /var/lib/kubelet/$ vim /var/lib/kubelet/config.ymlapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0port: 10250readOnlyPort: 10255authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30scgroupDriver: cgroupfscgroupsPerQOS: trueclusterDNS:- 10.96.0.10clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000maxPods: 110nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: truestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0s 在 k8s-m1 關閉 Swap。 123$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0vm.swappiness = 0 在 k8s-m1 建立 /etc/kubernetes/manifests 與 /etc/cni/net.d 目錄。 12$ mkdir -p /etc/kubernetes/manifests$ mkdir -p /etc/cni/net.d 因為本篇介紹的方式採用 binary 啟動 kubernetes component，因此 /etc/kubernetes/manifests 目錄為空 在 k8s-m1 啟動 Kubelet。 12$ systemctl enable kubelet$ systemctl start kubelet 在 k8s-m1 檢視 kubelet 狀態。 123456789101112131415161718192021222324$ systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; disabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since Thu 2021-02-04 09:10:16 UTC; 2min 46s ago Docs: http://kubernetes.io/docs/ Main PID: 52581 (kubelet) Tasks: 11 (limit: 2281) Memory: 27.2M CGroup: /system.slice/kubelet.service └─52581 /usr/local/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yml --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --node-labels=kubelet.kubernetes.io/master=masterFeb 04 09:12:38 k8s-m1 kubelet[52581]: E0204 09:12:38.576036 52581 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 04 09:12:41 k8s-m1 kubelet[52581]: W0204 09:12:41.877295 52581 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 04 09:12:43 k8s-m1 kubelet[52581]: E0204 09:12:43.584125 52581 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 04 09:12:46 k8s-m1 kubelet[52581]: W0204 09:12:46.877580 52581 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 04 09:12:48 k8s-m1 kubelet[52581]: E0204 09:12:48.592595 52581 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 04 09:12:51 k8s-m1 kubelet[52581]: W0204 09:12:51.878295 52581 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 04 09:12:53 k8s-m1 kubelet[52581]: E0204 09:12:53.601709 52581 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 04 09:12:56 k8s-m1 kubelet[52581]: W0204 09:12:56.878503 52581 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 04 09:12:58 k8s-m1 kubelet[52581]: E0204 09:12:58.610208 52581 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 04 09:13:01 k8s-m1 kubelet[52581]: W0204 09:13:01.878733 52581 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d 從 log 中發現 Container runtime network not ready... 的錯誤訊息是正常的，因為尚未安裝 CNI 在 k8s-m2 建立 /etc/systemd/system/kubelet.service 檔案。 1234567891011121314$ vim /etc/systemd/system/kubelet.service[Unit]Description=kubelet: The Kubernetes Node AgentDocumentation=http://kubernetes.io/docs/[Service]ExecStart=/usr/local/bin/kubeletRestart=alwaysStartLimitInterval=0RestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m2 建立 /etc/systemd/system/kubelet.service.d/10-kubelet.conf 檔案。 12345678910$ mkdir -p /etc/systemd/system/kubelet.service.d/$ vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf[Service]Environment=&quot;KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf&quot;Environment=&quot;KUBELET_SYSTEM_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot;Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yml&quot;Environment=&quot;KUBELET_EXTRA_ARGS=--node-labels=kubelet.kubernetes.io/master=master&quot;ExecStart=ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS 在 k8s-m2 建立 /var/lib/kubelet/config.yml 檔案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273$ mkdir -p /var/lib/kubelet/$ vim /var/lib/kubelet/config.ymlapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0port: 10250readOnlyPort: 10255authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30scgroupDriver: cgroupfscgroupsPerQOS: trueclusterDNS:- 10.96.0.10clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000maxPods: 110nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: truestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0s 在 k8s-m2 關閉 Swap。 123$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0vm.swappiness = 0 在 k8s-m2 建立 /etc/kubernetes/manifests 與 /etc/cni/net.d 目錄。 12$ mkdir -p /etc/kubernetes/manifests$ mkdir -p /etc/cni/net.d 在 k8s-m2 在 k8s-m2 啟動 Kubelet。 12$ systemctl enable kubelet$ systemctl start kubelet 在 k8s-m2 檢視 kubelet 狀態。 123456789101112131415161718192021222324$ systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since Thu 2021-02-04 09:17:25 UTC; 12s ago Docs: http://kubernetes.io/docs/ Main PID: 15174 (kubelet) Tasks: 12 (limit: 2281) Memory: 59.3M CGroup: /system.slice/kubelet.service └─15174 /usr/local/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yml --ne&gt;Feb 04 09:17:32 k8s-m2 kubelet[15174]: W0204 09:17:32.855035 15174 manager.go:596] Failed to retrieve checkpoint for &quot;kubelet&gt;Feb 04 09:17:32 k8s-m2 kubelet[15174]: I0204 09:17:32.855374 15174 plugin_manager.go:114] Starting Kubelet Plugin ManagerFeb 04 09:17:32 k8s-m2 kubelet[15174]: I0204 09:17:32.940663 15174 kuberuntime_manager.go:992] updating runtime config throug&gt;Feb 04 09:17:32 k8s-m2 kubelet[15174]: I0204 09:17:32.941087 15174 docker_service.go:357] docker cri received runtime config &gt;Feb 04 09:17:32 k8s-m2 kubelet[15174]: I0204 09:17:32.941285 15174 kubelet_network.go:77] Setting Pod CIDR: -&gt; 10.244.1.0/24Feb 04 09:17:32 k8s-m2 kubelet[15174]: E0204 09:17:32.948136 15174 kubelet.go:2106] Container runtime network not ready: Netw&gt;Feb 04 09:17:33 k8s-m2 kubelet[15174]: I0204 09:17:33.131375 15174 reconciler.go:157] Reconciler: start to sync stateFeb 04 09:17:36 k8s-m2 kubelet[15174]: I0204 09:17:36.174186 15174 transport.go:132] certificate rotation detected, shutting &gt;Feb 04 09:17:36 k8s-m2 kubelet[15174]: W0204 09:17:36.282619 15174 cni.go:239] Unable to update cni config: no networks found&gt;Feb 04 09:17:37 k8s-m2 kubelet[15174]: E0204 09:17:37.863015 15174 kubelet.go:2106] Container runtime network not ready: Netw&gt; 在 k8s-m3 建立 /etc/systemd/system/kubelet.service 檔案。 1234567891011121314$ vim /etc/systemd/system/kubelet.service[Unit]Description=kubelet: The Kubernetes Node AgentDocumentation=http://kubernetes.io/docs/[Service]ExecStart=/usr/local/bin/kubeletRestart=alwaysStartLimitInterval=0RestartSec=10[Install]WantedBy=multi-user.target 在 k8s-m3 建立 /etc/systemd/system/kubelet.service.d/10-kubelet.conf 檔案。 12345678910$ mkdir -p /etc/systemd/system/kubelet.service.d/$ vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf[Service]Environment=&quot;KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf&quot;Environment=&quot;KUBELET_SYSTEM_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot;Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yml&quot;Environment=&quot;KUBELET_EXTRA_ARGS=--node-labels=kubelet.kubernetes.io/master=master&quot;ExecStart=ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS 在 k8s-m3 建立 /var/lib/kubelet/config.yml 檔案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273$ mkdir -p /var/lib/kubelet/$ vim /var/lib/kubelet/config.ymlapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0port: 10250readOnlyPort: 10255authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30scgroupDriver: cgroupfscgroupsPerQOS: trueclusterDNS:- 10.96.0.10clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000maxPods: 110nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: truestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0s 在 k8s-m3 在 k8s-m3 關閉 Swap。 123$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0vm.swappiness = 0 在 k8s-m3 建立 /etc/kubernetes/manifests 與 /etc/cni/net.d 目錄。 12$ mkdir -p /etc/kubernetes/manifests$ mkdir -p /etc/cni/net.d 在 k8s-m3 啟動 Kubelet。 12$ systemctl enable kubelet$ systemctl start kubelet 在 k8s-m3 檢視 kubelet 狀態。 123456789101112131415161718192021222324$ systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since Thu 2021-02-04 09:19:46 UTC; 1min 48s ago Docs: http://kubernetes.io/docs/ Main PID: 25362 (kubelet) Tasks: 12 (limit: 2281) Memory: 60.8M CGroup: /system.slice/kubelet.service └─25362 /usr/local/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yml --network-plugin=cn&gt;Feb 04 09:21:12 k8s-m3 kubelet[25362]: W0204 09:21:12.505872 25362 cni.go:239] Unable to update cni config: no networks found in /etc/cni/ne&gt;Feb 04 09:21:14 k8s-m3 kubelet[25362]: E0204 09:21:14.206218 25362 kubelet.go:2106] Container runtime network not ready: NetworkReady=false &gt;Feb 04 09:21:17 k8s-m3 kubelet[25362]: W0204 09:21:17.506075 25362 cni.go:239] Unable to update cni config: no networks found in /etc/cni/ne&gt;Feb 04 09:21:19 k8s-m3 kubelet[25362]: E0204 09:21:19.213025 25362 kubelet.go:2106] Container runtime network not ready: NetworkReady=false &gt;Feb 04 09:21:22 k8s-m3 kubelet[25362]: W0204 09:21:22.506338 25362 cni.go:239] Unable to update cni config: no networks found in /etc/cni/ne&gt;Feb 04 09:21:24 k8s-m3 kubelet[25362]: E0204 09:21:24.221402 25362 kubelet.go:2106] Container runtime network not ready: NetworkReady=false &gt;Feb 04 09:21:27 k8s-m3 kubelet[25362]: W0204 09:21:27.506638 25362 cni.go:239] Unable to update cni config: no networks found in /etc/cni/ne&gt;Feb 04 09:21:29 k8s-m3 kubelet[25362]: E0204 09:21:29.229734 25362 kubelet.go:2106] Container runtime network not ready: NetworkReady=false &gt;Feb 04 09:21:32 k8s-m3 kubelet[25362]: W0204 09:21:32.507019 25362 cni.go:239] Unable to update cni config: no networks found in /etc/cni/ne&gt;Feb 04 09:21:34 k8s-m3 kubelet[25362]: E0204 09:21:34.237281 25362 kubelet.go:2106] Container runtime network not ready: NetworkReady=false &gt; TLS Bootstrapping在 Kubernetes Cluster 中，運行在 Worker Nodes 上的服務像是 kubelet, kube-proxy 等等必須與 Master Nodes 元件進行通訊，特別是 kube-apiserver。因此官方建議 TLS 方式以提升整體通訊安全性。 注意：在 k8s-m1, k8s-m2 與 k8s-m3 皆需安裝與啟動服務 在 k8s-m1 利用 kubectl 產生 Bootstrap 的 kubeconfig 檔案。 12345678910111213141516171819202122232425$ kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf Cluster &quot;kubernetes&quot; set.$ kubectl config set-credentials tls-bootstrap-token-user \\ --token=${BOOTSTRAP_TOKEN} \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf User &quot;tls-bootstrap-token-user&quot; set.$ kubectl config set-context tls-bootstrap-token-user@kubernetes \\ --cluster=kubernetes \\ --user=tls-bootstrap-token-user \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf Context &quot;tls-bootstrap-token-user@kubernetes&quot; created.$ kubectl config use-context tls-bootstrap-token-user@kubernetes \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf Switched to context &quot;tls-bootstrap-token-user@kubernetes&quot;. 在 k8s-m1 建立 TLS Bootstrap Secret 來提供自動簽證使用。 1234567891011121314151617$ cp /etc/kubernetes/admin.conf ~/.kube/config$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Secretmetadata: name: bootstrap-token-${TOKEN_ID} namespace: kube-systemtype: bootstrap.kubernetes.io/tokenstringData: token-id: &quot;${TOKEN_ID}&quot; token-secret: &quot;${TOKEN_SECRET}&quot; usage-bootstrap-authentication: &quot;true&quot; usage-bootstrap-signing: &quot;true&quot; auth-extra-groups: system:bootstrappers:default-node-tokenEOF secret/bootstrap-token-8ce95d created 在 k8s-m1 建立 kubelet-bootstrap-rbac.yml 檔案。 12345678910111213141516171819202122232425262728293031323334353637383940$ vim kubelet-bootstrap-rbac.ymlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubelet-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-bootstrappersubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers:default-node-token---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: node-autoapprove-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclientsubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers:default-node-token---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: node-autoapprove-certificate-rotationroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclientsubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodes 在 k8s-m1 建立 TLS Bootstrap Autoapprove RBAC 來提供自動受理 CSR。 12345$ kubectl apply -f kubelet-bootstrap-rbac.ymlclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap createdclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap createdclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created 在 k8s-m1 更改 Role。 123$ kubectl label node k8s-m1 node-role.kubernetes.io/master=master$ kubectl label node k8s-m2 node-role.kubernetes.io/master=master$ kubectl label node k8s-m3 node-role.kubernetes.io/master=master 完成後，在任一台 Master 驗證節點狀態。 1234567891011121314$ kubectl get csWarning: v1 ComponentStatus is deprecated in v1.19+NAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy {&quot;health&quot;:&quot;true&quot;}$ kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-m1 NotReady master 20m v1.19.7k8s-m2 NotReady master 11m v1.19.7k8s-m3 NotReady master 9m35s v1.19.7 這裡看到每個 Master 皆為 NotReady 是正常的。 將 k8s-m1, k8s-m2 與 k8s-m3 設定為 taint，主要是讓 Scheduler 不會將 Pods 建立在 Master 上。 12345$ kubectl taint nodes node-role.kubernetes.io/master=&quot;&quot;:NoSchedule --allnode/k8s-m1 taintednode/k8s-m2 taintednode/k8s-m3 tainted Worker Kubelet注意：在 k8s-n1 與 k8s-n2 皆需安裝與啟動服務 在 k8s-n1 與 k8s-n2 建立相關目錄。 12$ mkdir -p /etc/kubernetes/pki/$ mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests 將 ca.pem, ca-key.pem 與 bootstrap-kubelet.conf 檔案由 k8s-m1 複製到 k8s-n1 與 k8s-n2。 1234$ scp ${PKI_DIR}/ca.pem ${PKI_DIR}/ca-key.pem k8s-n1:${PKI_DIR}/$ scp ${K8S_DIR}/bootstrap-kubelet.conf k8s-n1:${K8S_DIR}$ scp ${PKI_DIR}/ca.pem ${PKI_DIR}/ca-key.pem k8s-n2:${PKI_DIR}/$ scp ${K8S_DIR}/bootstrap-kubelet.conf k8s-n2:${K8S_DIR} 在 k8s-n1 建立 /var/lib/kubelet/config.yml 檔案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071$ vim /var/lib/kubelet/config.ymlapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0port: 10250readOnlyPort: 10255authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30scgroupDriver: cgroupfscgroupsPerQOS: trueclusterDNS:- 10.96.0.10clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000maxPods: 110nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: truestreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0s 在 k8s-n1 建立 /lib/systemd/system/kubelet.service 檔案。 1234567891011121314$ vim /lib/systemd/system/kubelet.service[Unit]Description=kubelet: The Kubernetes Node AgentDocumentation=http://kubernetes.io/docs/[Service]ExecStart=/usr/local/bin/kubeletRestart=alwaysStartLimitInterval=0RestartSec=10[Install]WantedBy=multi-user.target 在 k8s-n1 建立 /etc/systemd/system/kubelet.service.d/10-kubelet.conf 檔案。 12345678$ vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf[Service]Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;Environment=&quot;KUBELET_SYSTEM_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot;Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yml&quot;ExecStart=ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS 在 k8s-n1 關閉 Swap。 123$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0vm.swappiness = 0 在 k8s-n1 建立 /etc/kubernetes/manifests 與 /etc/cni/net.d 目錄。 12$ mkdir -p /etc/kubernetes/manifests$ mkdir -p /etc/cni/net.d 在 k8s-n1 啟動 kubelet。 12$ systemctl enable kubelet$ systemctl start kubelet 在 k8s-n1 檢視 kubelet 狀態。 123456789101112131415161718192021222324$ systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since Sat 2021-02-06 02:51:50 UTC; 44s ago Docs: http://kubernetes.io/docs/ Main PID: 20996 (kubelet) Tasks: 10 (limit: 2281) Memory: 31.0M CGroup: /system.slice/kubelet.service └─20996 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yml --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/op&gt;Feb 06 02:52:10 k8s-n1 kubelet[20996]: E0206 02:52:10.620873 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:52:11 k8s-n1 kubelet[20996]: W0206 02:52:11.101792 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 06 02:52:15 k8s-n1 kubelet[20996]: E0206 02:52:15.628330 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:52:16 k8s-n1 kubelet[20996]: W0206 02:52:16.102915 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 06 02:52:20 k8s-n1 kubelet[20996]: E0206 02:52:20.634879 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:52:21 k8s-n1 kubelet[20996]: W0206 02:52:21.103425 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 06 02:52:25 k8s-n1 kubelet[20996]: E0206 02:52:25.642455 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:52:26 k8s-n1 kubelet[20996]: W0206 02:52:26.104481 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 06 02:52:30 k8s-n1 kubelet[20996]: E0206 02:52:30.649202 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:52:31 k8s-n1 kubelet[20996]: W0206 02:52:31.105273 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d 在 k8s-n2 建立 /var/lib/kubelet/config.yml 檔案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071$ vim /var/lib/kubelet/config.ymlapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0port: 10250readOnlyPort: 10255authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30scgroupDriver: cgroupfscgroupsPerQOS: trueclusterDNS:- 10.96.0.10clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000maxPods: 110nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: truestreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0s 在 k8s-n2 建立 /lib/systemd/system/kubelet.service 檔案。 1234567891011121314$ vim /lib/systemd/system/kubelet.service[Unit]Description=kubelet: The Kubernetes Node AgentDocumentation=http://kubernetes.io/docs/[Service]ExecStart=/usr/local/bin/kubeletRestart=alwaysStartLimitInterval=0RestartSec=10[Install]WantedBy=multi-user.target 在 k8s-n2 建立 /etc/systemd/system/kubelet.service.d/10-kubelet.conf 檔案。 12345678$ vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf[Service]Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;Environment=&quot;KUBELET_SYSTEM_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&quot;Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yml&quot;ExecStart=ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS 在 k8s-n2 關閉 Swap。 123$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0vm.swappiness = 0 在 k8s-n2 建立 /etc/kubernetes/manifests 與 /etc/cni/net.d 目錄。 12$ mkdir -p /etc/kubernetes/manifests$ mkdir -p /etc/cni/net.d 在 k8s-n2 啟動 kubelet。 12$ systemctl enable kubelet$ systemctl start kubelet 在 k8s-n2 檢視 kubelet 狀態。 123456789101112131415161718192021222324$ systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet.conf Active: active (running) since Sat 2021-02-06 02:53:52 UTC; 1min 26s ago Docs: http://kubernetes.io/docs/ Main PID: 20996 (kubelet) Tasks: 11 (limit: 2281) Memory: 32.3M CGroup: /system.slice/kubelet.service └─20996 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yml --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/op&gt;Feb 06 02:54:54 k8s-n2 kubelet[20996]: E0206 02:54:54.114507 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:54:58 k8s-n2 kubelet[20996]: W0206 02:54:58.557437 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 06 02:54:59 k8s-n2 kubelet[20996]: E0206 02:54:59.122977 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:55:03 k8s-n2 kubelet[20996]: W0206 02:55:03.558418 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 06 02:55:04 k8s-n2 kubelet[20996]: E0206 02:55:04.130803 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:55:08 k8s-n2 kubelet[20996]: W0206 02:55:08.559195 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 06 02:55:09 k8s-n2 kubelet[20996]: E0206 02:55:09.137331 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:55:13 k8s-n2 kubelet[20996]: W0206 02:55:13.561277 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.dFeb 06 02:55:14 k8s-n2 kubelet[20996]: E0206 02:55:14.145674 20996 kubelet.go:2106] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedFeb 06 02:55:18 k8s-n2 kubelet[20996]: W0206 02:55:18.562345 20996 cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d 在 k8s-m1 更改 Role。 12$ kubectl label node k8s-n1 node-role.kubernetes.io/worker=worker$ kubectl label node k8s-n2 node-role.kubernetes.io/worker=worker 在 k8s-m1 確認當前節點狀態。 1234567$ kubectl get nodeNAME STATUS AGEdefault Active 172mkube-node-lease Active 172mkube-public Active 172mkube-system Active 172m CNI完成 Master 與 Worker 節點部署後，需要選擇一套容器間溝通的網路，常見的有 Flannel, Weave, Calico 等等。我們選擇 Calico 作為容器網路。 以下步驟皆在 k8s-m1 節點上操作。 1234567891011121314151617181920212223242526272829$ kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yamlcustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io createdcustomresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io creatednamespace/tigera-operator createdpodsecuritypolicy.policy/tigera-operator createdserviceaccount/tigera-operator createdclusterrole.rbac.authorization.k8s.io/tigera-operator createdclusterrolebinding.rbac.authorization.k8s.io/tigera-operator createddeployment.apps/tigera-operator created$ kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yamlinstallation.operator.tigera.io/default created kube proxy在 k8s-m1 建立 kube-proxy/kube-proxy-cm.yml 檔案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667$ mkdir kube-proxy/$ vim kube-proxy/kube-proxy-cm.ymlapiVersion: v1data: config.conf: |- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: &quot;&quot; burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /var/lib/kube-proxy/kubeconfig.conf qps: 5 clusterCIDR: 10.244.0.0/16 configSyncPeriod: 15m0s conntrack: maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: &quot;&quot; iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: minSyncPeriod: 0s scheduler: rr syncPeriod: 30s metricsBindAddress: 127.0.0.1:10249 mode: ipvs featureGates: SupportIPVSProxyMode: true oomScoreAdj: -999 portRange: &quot;&quot; resourceContainer: /kube-proxy udpIdleTimeout: 250ms kubeconfig.conf: |- apiVersion: v1 kind: Config clusters: - cluster: certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt server: https://192.168.20.20:6443 name: default contexts: - context: cluster: default namespace: default user: default name: default current-context: default users: - name: default user: tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/tokenkind: ConfigMapmetadata: labels: app: kube-proxy name: kube-proxy namespace: kube-system https://192.168.20.20:6443 為 VIP，讀者須依照需求自行修改。 在 k8s-m1 建立 kube-proxy/kube-proxy.yml 檔案。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081$ vim kube-proxy/kube-proxy.ymlapiVersion: v1kind: ServiceAccountmetadata: name: kube-proxy namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:kube-proxy labels: addonmanager.kubernetes.io/mode: Reconcilesubjects: - kind: ServiceAccount name: kube-proxy namespace: kube-systemroleRef: kind: ClusterRole name: system:node-proxier apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: DaemonSetmetadata: labels: k8s-app: kube-proxy name: kube-proxy namespace: kube-systemspec: selector: matchLabels: k8s-app: kube-proxy template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot; labels: k8s-app: kube-proxy spec: serviceAccount: kube-proxy serviceAccountName: kube-proxy priorityClassName: system-node-critical tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master hostNetwork: true containers: - name: kube-proxy image: k8s.gcr.io/kube-proxy-amd64:v1.11.0 command: - /usr/local/bin/kube-proxy - --config=/var/lib/kube-proxy/config.conf securityContext: privileged: true volumeMounts: - mountPath: /var/lib/kube-proxy name: kube-proxy - mountPath: /run/xtables.lock name: xtables-lock - mountPath: /lib/modules name: lib-modules readOnly: true volumes: - configMap: defaultMode: 420 name: kube-proxy name: kube-proxy - hostPath: path: /run/xtables.lock type: FileOrCreate name: xtables-lock - hostPath: path: /lib/modules type: &quot;&quot; name: lib-modules 在 k8s-m1 建立 kube-proxy 服務。 123456$ kubectl create -f kube-proxy/configmap/kube-proxy createdserviceaccount/kube-proxy createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-proxy createddaemonset.apps/kube-proxy created CoreDNS在 k8s-m1 從 coredns/deployment 下載 coredns-1.14.0.tar.gz 並解壓縮。 12$ wget https://github.com/coredns/deployment/archive/coredns-1.14.0.tar.gz$ tar -xvf coredns-1.14.0.tar.gz 解壓縮完後，利用 deployment-coredns-1.14.0/kubernetes/deploy.sh 檔案產生 coredns.yml 檔案。 1$ ./deployment-coredns-1.14.0/kubernetes/deploy.sh -i 10.96.0.10 &gt; coredns.yml 修改 coredns.yml 檔案，將 forward . /etc/resolv.conf 更改為 forward 8.8.8.8:53。 12345678910111213141516171819202122232425$ vim coredns.yml...data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 8.8.8.8:53 # forward . /etc/resolv.conf { # max_concurrent 1000 #} cache 30 loop reload loadbalance }... 建立 coredns 服務。 1234567kubectl create -f coredns.ymlclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.apps/coredns createdservice/kube-dns created 安裝完成後，檢視當前 Kubernetes Cluster 節點狀態。 12345678$ kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-m1 Ready master 6h8m v1.19.7k8s-m2 Ready master 6h v1.19.7k8s-m3 Ready master 5h58m v1.19.7k8s-n1 Ready worker 5h45m v1.19.7k8s-n2 Ready worker 5h43m v1.19.7 到這裡一個 HA Kubernetes Cluster 大致上建置完成，後續在根據讀者的需求自行安裝 Dashboard 等等的功能。 參考 https://k2r2bai.com/2018/07/17/kubernetes/deploy/manual-install 若針對本篇教學有任何疑問或有敘述錯誤的地方，歡迎在底下留言討論唷～","link":"/2021/02/08/從-Source-Codes-開始打造-HA-Kubernetes-Cluster/"}],"tags":[{"name":"Ceph","slug":"Ceph","link":"/tags/Ceph/"},{"name":"Object Storage","slug":"Object-Storage","link":"/tags/Object-Storage/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Cloud Native","slug":"Cloud-Native","link":"/tags/Cloud-Native/"},{"name":"OpenStack","slug":"OpenStack","link":"/tags/OpenStack/"},{"name":"Keystone","slug":"Keystone","link":"/tags/Keystone/"},{"name":"RGW","slug":"RGW","link":"/tags/RGW/"},{"name":"load balancer","slug":"load-balancer","link":"/tags/load-balancer/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"CoreDNS","slug":"CoreDNS","link":"/tags/CoreDNS/"},{"name":"TLS","slug":"TLS","link":"/tags/TLS/"},{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"Vitess","slug":"Vitess","link":"/tags/Vitess/"}],"categories":[{"name":"Ceph","slug":"Ceph","link":"/categories/Ceph/"},{"name":"OpenStack","slug":"OpenStack","link":"/categories/OpenStack/"},{"name":"load balancer","slug":"load-balancer","link":"/categories/load-balancer/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"},{"name":"TLS","slug":"TLS","link":"/categories/TLS/"},{"name":"Keystone","slug":"OpenStack/Keystone","link":"/categories/OpenStack/Keystone/"},{"name":"Database","slug":"Database","link":"/categories/Database/"},{"name":"Vitess","slug":"Database/Vitess","link":"/categories/Database/Vitess/"},{"name":"Ceph","slug":"OpenStack/Keystone/Ceph","link":"/categories/OpenStack/Keystone/Ceph/"},{"name":"RGW","slug":"OpenStack/Keystone/Ceph/RGW","link":"/categories/OpenStack/Keystone/Ceph/RGW/"}]}